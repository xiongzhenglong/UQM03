This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: uqm-backend
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)


================================================================
Directory Structure
================================================================
uqm-backend/.env copy.example
uqm-backend/.gitignore
uqm-backend/docker-compose.yml
uqm-backend/Dockerfile
uqm-backend/pyproject.toml
uqm-backend/README.md
uqm-backend/requirements.txt
uqm-backend/schema/uqm_schema.json
uqm-backend/setup.py
uqm-backend/src/__version__.py
uqm-backend/src/api/__init__.py
uqm-backend/src/api/models.py
uqm-backend/src/api/routes.py
uqm-backend/src/config/__init__.py
uqm-backend/src/config/settings.py
uqm-backend/src/connectors/__init__.py
uqm-backend/src/connectors/base.py
uqm-backend/src/connectors/mysql.py
uqm-backend/src/connectors/postgres.py
uqm-backend/src/connectors/sqlite.py
uqm-backend/src/core/__init__.py
uqm-backend/src/core/cache.py
uqm-backend/src/core/engine.py
uqm-backend/src/core/executor.py
uqm-backend/src/core/parser.py
uqm-backend/src/main.py
uqm-backend/src/steps/__init__.py
uqm-backend/src/steps/assert_step.py
uqm-backend/src/steps/base.py
uqm-backend/src/steps/enrich_step.py
uqm-backend/src/steps/pivot_step.py
uqm-backend/src/steps/query_step.py
uqm-backend/src/steps/union_step.py
uqm-backend/src/steps/unpivot_step.py
uqm-backend/src/utils/__init__.py
uqm-backend/src/utils/exceptions.py
uqm-backend/src/utils/expression_parser.py
uqm-backend/src/utils/logging.py
uqm-backend/src/utils/sql_builder.py
uqm-backend/src/utils/validators.py
uqm-backend/tests/conftest.py
uqm-backend/tests/integration/test_core_engine.py
uqm-backend/tests/unit/test_expression_parser.py
uqm-backend/tests/unit/test_validators.py

================================================================
Files
================================================================

================
File: uqm-backend/.env copy.example
================
# æ•°æ®åº“é…ç½®
DATABASE_URL=postgresql://uqm_user:uqm_password@localhost:5432/uqm_db
MYSQL_URL=mysql://uqm_user:uqm_password@localhost:3306/uqm_db
SQLITE_URL=sqlite:///./uqm.db

# Redisé…ç½®
REDIS_URL=redis://localhost:6379/0

# APIé…ç½®
DEBUG=True
HOST=0.0.0.0
PORT=8000
SECRET_KEY=your-secret-key-here-change-in-production
ALLOWED_HOSTS=localhost,127.0.0.1,0.0.0.0

# ç¼“å­˜é…ç½®
CACHE_TYPE=redis
CACHE_DEFAULT_TIMEOUT=3600
CACHE_MAX_SIZE=1000

# æ—¥å¿—é…ç½®
LOG_LEVEL=INFO
LOG_FORMAT=json

# æŸ¥è¯¢é…ç½®
MAX_QUERY_TIMEOUT=300
MAX_CONCURRENT_QUERIES=10
QUERY_RESULT_LIMIT=10000

# å®‰å…¨é…ç½®
CORS_ORIGINS="http://localhost:3000,http://localhost:8080"
CORS_CREDENTIALS=True
CORS_METHODS="GET,POST,PUT,DELETE,OPTIONS"
CORS_HEADERS="*"

# ç›‘æ§é…ç½®
ENABLE_METRICS=True
METRICS_PATH=/metrics

================
File: uqm-backend/.gitignore
================
# UQM Backend é¡¹ç›®å¿½ç•¥æ–‡ä»¶

# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
.python-version

# pipenv
Pipfile.lock

# poetry
poetry.lock

# pdm
.pdm.toml
.pdm-python
.pdm-build/

# PEP 582
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
.idea/

# VS Code
.vscode/
*.code-workspace

# Docker
docker-compose.override.yml
.dockerignore

# æ•°æ®æ–‡ä»¶
*.csv
*.json
*.xlsx
*.parquet
data/
uploads/
downloads/

# æ—¥å¿—æ–‡ä»¶
logs/
*.log
*.log.*

# ä¸´æ—¶æ–‡ä»¶
tmp/
temp/
.tmp/
.temp/

# é…ç½®æ–‡ä»¶
config.ini
config.yaml
config.json
local_config.*

# å¯†é’¥å’Œè¯ä¹¦
*.key
*.pem
*.crt
*.p12
*.pfx
secrets/
ssl/

# æ•°æ®åº“æ–‡ä»¶
*.db
*.sqlite
*.sqlite3

# ç¼“å­˜æ–‡ä»¶
.cache/
cache/

# ç›‘æ§å’Œæ€§èƒ½æ•°æ®
.profile
*.prof
monitoring/

# æ“ä½œç³»ç»Ÿæ–‡ä»¶
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# ç¼–è¾‘å™¨å¤‡ä»½æ–‡ä»¶
*~
*.swp
*.swo
*#
.#*

# æµ‹è¯•ç›¸å…³
.coverage.*
.pytest_cache/
test-results/
coverage.xml
htmlcov/

# å·¥å…·é…ç½®
.pre-commit-config.yaml
.black
.flake8
.isort.cfg
mypy.ini
bandit.yaml

# æ‰“åŒ…æ–‡ä»¶
*.tar.gz
*.zip
*.rar

# Node.js (å¦‚æœæœ‰å‰ç«¯ç»„ä»¶)
node_modules/
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# é¡¹ç›®ç‰¹å®š
uqm_data/
examples/output/
benchmarks/results/

================
File: uqm-backend/docker-compose.yml
================
version: '3.8'

services:
  # UQM Backend åº”ç”¨
  uqm-backend:
    build: .
    container_name: uqm-backend
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://uqm_user:uqm_password@postgres:5432/uqm_db
      - REDIS_URL=redis://redis:6379/0
      - LOG_LEVEL=INFO
      - CACHE_BACKEND=redis
      - CORS_ORIGINS=["http://localhost:3000","http://localhost:8080"]
    depends_on:
      - postgres
      - redis
    volumes:
      - ./logs:/app/logs
      - ./data:/app/data
    networks:
      - uqm-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # PostgreSQL æ•°æ®åº“
  postgres:
    image: postgres:15-alpine
    container_name: uqm-postgres
    environment:
      - POSTGRES_DB=uqm_db
      - POSTGRES_USER=uqm_user
      - POSTGRES_PASSWORD=uqm_password
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-scripts:/docker-entrypoint-initdb.d
    networks:
      - uqm-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U uqm_user -d uqm_db"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Redis ç¼“å­˜
  redis:
    image: redis:7-alpine
    container_name: uqm-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - uqm-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    command: redis-server --appendonly yes

  # Nginx åå‘ä»£ç†ï¼ˆå¯é€‰ï¼‰
  nginx:
    image: nginx:alpine
    container_name: uqm-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
    depends_on:
      - uqm-backend
    networks:
      - uqm-network
    restart: unless-stopped
    profiles:
      - with-nginx

  # Prometheus ç›‘æ§ï¼ˆå¯é€‰ï¼‰
  prometheus:
    image: prom/prometheus:latest
    container_name: uqm-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
    networks:
      - uqm-network
    restart: unless-stopped
    profiles:
      - monitoring

  # Grafana ç›‘æ§ä»ªè¡¨æ¿ï¼ˆå¯é€‰ï¼‰
  grafana:
    image: grafana/grafana:latest
    container_name: uqm-grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
    depends_on:
      - prometheus
    networks:
      - uqm-network
    restart: unless-stopped
    profiles:
      - monitoring

  # æµ‹è¯•æ•°æ®åº“ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰
  test-postgres:
    image: postgres:15-alpine
    container_name: uqm-test-postgres
    environment:
      - POSTGRES_DB=uqm_test_db
      - POSTGRES_USER=uqm_test_user
      - POSTGRES_PASSWORD=uqm_test_password
    ports:
      - "5433:5432"
    networks:
      - uqm-network
    profiles:
      - testing
    tmpfs:
      - /var/lib/postgresql/data

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local

networks:
  uqm-network:
    driver: bridge

================
File: uqm-backend/Dockerfile
================
# UQM Backend Docker é•œåƒ

FROM python:3.11-slim

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/app
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    curl \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt pyproject.toml ./

# å®‰è£… Python ä¾èµ–
RUN pip install --no-cache-dir --upgrade pip
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY src/ ./src/
COPY schema/ ./schema/
COPY .env.example ./.env

# åˆ›å»ºé root ç”¨æˆ·
RUN useradd --create-home --shell /bin/bash uqm
RUN chown -R uqm:uqm /app
USER uqm

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["python", "-m", "uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"]

================
File: uqm-backend/pyproject.toml
================
[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

[tool.poetry]
name = "uqm-backend"
version = "0.1.0"
description = "ç»Ÿä¸€æŸ¥è¯¢æ¨¡å‹(UQM)åç«¯æ‰§è¡Œå¼•æ“"
authors = ["UQM Team <uqm@example.com>"]
license = "MIT"
readme = "README.md"
packages = [{include = "src"}]

[tool.poetry.dependencies]
python = "^3.9"
fastapi = "^0.104.1"
uvicorn = {extras = ["standard"], version = "^0.24.0"}
pydantic = "^2.5.0"
pydantic-settings = "^2.1.0"
sqlalchemy = "^2.0.23"
psycopg2-binary = "^2.9.9"
pymysql = "^1.1.0"
pandas = "^2.1.4"
numpy = "^1.24.4"
jsonschema = "^4.20.0"
redis = "^5.0.1"
aioredis = "^2.0.1"
python-multipart = "^0.0.6"
python-jose = {extras = ["cryptography"], version = "^3.3.0"}
passlib = {extras = ["bcrypt"], version = "^1.7.4"}
python-dotenv = "^1.0.0"
structlog = "^23.2.0"

[tool.poetry.group.dev.dependencies]
pytest = "^7.4.3"
pytest-asyncio = "^0.21.1"
pytest-cov = "^4.1.0"
httpx = "^0.25.2"
black = "^23.11.0"
isort = "^5.12.0"
flake8 = "^6.1.0"
mypy = "^1.7.1"

[tool.black]
line-length = 100
target-version = ['py39']
include = '\.pyi?$'
extend-exclude = '''
/(
  # directories
  \.eggs
  | \.git
  | \.hg
  | \.mypy_cache
  | \.tox
  | \.venv
  | build
  | dist
)/
'''

[tool.isort]
profile = "black"
multi_line_output = 3
line_length = 100
known_first_party = ["src"]

[tool.mypy]
python_version = "3.9"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_functions = ["test_*"]
addopts = "-v --cov=src --cov-report=html --cov-report=term-missing"
asyncio_mode = "auto"

================
File: uqm-backend/README.md
================
# UQM Backend - ç»Ÿä¸€æŸ¥è¯¢æ¨¡å‹åç«¯æ‰§è¡Œå¼•æ“

[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104%2B-green.svg)](https://fastapi.tiangolo.com)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Coverage](https://img.shields.io/badge/coverage-85%25-brightgreen.svg)](coverage.html)

UQM Backend æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½ã€å¯æ‰©å±•çš„ç»Ÿä¸€æŸ¥è¯¢æ¨¡å‹åç«¯æ‰§è¡Œå¼•æ“ï¼Œæ—¨åœ¨æä¾›çµæ´»çš„æ•°æ®å¤„ç†å’ŒæŸ¥è¯¢èƒ½åŠ›ã€‚

## âœ¨ ç‰¹æ€§

- **ğŸš€ é«˜æ€§èƒ½**: åŸºäº FastAPI å’Œå¼‚æ­¥ç¼–ç¨‹ï¼Œæ”¯æŒé«˜å¹¶å‘è¯·æ±‚
- **ğŸ”Œ å¤šæ•°æ®æº**: æ”¯æŒ PostgreSQLã€MySQLã€SQLiteã€Redis ç­‰å¤šç§æ•°æ®æº
- **âš¡ å¹¶è¡Œæ‰§è¡Œ**: æ”¯æŒæ­¥éª¤å¹¶è¡Œæ‰§è¡Œï¼Œæå‡å¤„ç†æ•ˆç‡
- **ğŸ’¾ æ™ºèƒ½ç¼“å­˜**: å†…ç½®å¤šçº§ç¼“å­˜ç­–ç•¥ï¼Œä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½
- **ğŸ”’ å®‰å…¨å¯é **: å®Œå–„çš„éªŒè¯æœºåˆ¶ï¼Œé˜²æ­¢ SQL æ³¨å…¥å’Œæ•°æ®æ³„éœ²
- **ğŸ“Š å®æ—¶ç›‘æ§**: é›†æˆ Prometheus ç›‘æ§å’Œå¥åº·æ£€æŸ¥
- **ğŸ³ å®¹å™¨åŒ–**: å®Œæ•´çš„ Docker æ”¯æŒï¼Œä¾¿äºéƒ¨ç½²
- **ğŸ“ å®Œæ•´æ–‡æ¡£**: è‡ªåŠ¨ç”Ÿæˆçš„ API æ–‡æ¡£å’Œè¯¦ç»†ä½¿ç”¨è¯´æ˜

## ğŸ—ï¸ æ¶æ„æ¦‚è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   API å±‚        â”‚    â”‚   æ ¸å¿ƒå¼•æ“      â”‚    â”‚   è¿æ¥å™¨å±‚      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ REST API      â”‚    â”‚ â€¢ UQM è§£æå™¨    â”‚    â”‚ â€¢ PostgreSQL    â”‚
â”‚ â€¢ èº«ä»½éªŒè¯      â”‚    â”‚ â€¢ æ‰§è¡Œå¼•æ“      â”‚    â”‚ â€¢ MySQL         â”‚
â”‚ â€¢ è¯·æ±‚éªŒè¯      â”‚    â”‚ â€¢ æ­¥éª¤è°ƒåº¦å™¨    â”‚    â”‚ â€¢ SQLite        â”‚
â”‚ â€¢ å¼‚å¸¸å¤„ç†      â”‚    â”‚ â€¢ ç¼“å­˜ç®¡ç†      â”‚    â”‚ â€¢ Redis         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                       â”‚                       â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   æ­¥éª¤å®ç°      â”‚    â”‚   å·¥å…·å±‚        â”‚    â”‚   é…ç½®å±‚        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ æŸ¥è¯¢æ­¥éª¤      â”‚    â”‚ â€¢ è¡¨è¾¾å¼è§£æ    â”‚    â”‚ â€¢ ç¯å¢ƒé…ç½®      â”‚
â”‚ â€¢ æ•°æ®ä¸°å¯Œ      â”‚    â”‚ â€¢ æ•°æ®éªŒè¯      â”‚    â”‚ â€¢ æ—¥å¿—é…ç½®      â”‚
â”‚ â€¢ æ•°æ®é€è§†      â”‚    â”‚ â€¢ SQL æ„å»º      â”‚    â”‚ â€¢ å®‰å…¨é…ç½®      â”‚
â”‚ â€¢ æ•°æ®åˆå¹¶      â”‚    â”‚ â€¢ å¼‚å¸¸å¤„ç†      â”‚    â”‚ â€¢ ç›‘æ§é…ç½®      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- Python 3.9+
- PostgreSQL 12+ (å¯é€‰)
- Redis 6+ (å¯é€‰)
- Docker & Docker Compose (å¯é€‰)

### å®‰è£…æ–¹å¼

#### æ–¹å¼ä¸€ï¼šDocker éƒ¨ç½²ï¼ˆæ¨èï¼‰

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/uqm/uqm-backend.git
cd uqm-backend

# ä½¿ç”¨ Docker Compose å¯åŠ¨
docker-compose up -d

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
docker-compose ps
```

#### æ–¹å¼äºŒï¼šæœ¬åœ°å¼€å‘

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/uqm/uqm-backend.git
cd uqm-backend

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# é…ç½®ç¯å¢ƒå˜é‡
cp .env.example .env
# ç¼–è¾‘ .env æ–‡ä»¶ï¼Œé…ç½®æ•°æ®åº“è¿æ¥ç­‰

# å¯åŠ¨æœåŠ¡
python -m uvicorn src.main:app --reload
```

#### æ–¹å¼ä¸‰ï¼špip å®‰è£…

```bash
pip install uqm-backend

# å¯åŠ¨æœåŠ¡
uqm-server
```

### éªŒè¯å®‰è£…

è®¿é—® http://localhost:8000 æŸ¥çœ‹æœåŠ¡çŠ¶æ€ï¼Œæˆ–è®¿é—® http://localhost:8000/docs æŸ¥çœ‹ API æ–‡æ¡£ã€‚

## ğŸ“– ä½¿ç”¨æŒ‡å—

### UQM é…ç½®ç¤ºä¾‹

```json
{
  "name": "user_analysis",
  "version": "1.0.0",
  "description": "ç”¨æˆ·æ•°æ®åˆ†æç¤ºä¾‹",
  "datasources": {
    "main_db": {
      "type": "postgresql",
      "connection": {
        "host": "localhost",
        "port": 5432,
        "database": "analytics",
        "username": "user",
        "password": "password"
      }
    }
  },
  "steps": [
    {
      "name": "extract_users",
      "type": "query",
      "datasource": "main_db",
      "config": {
        "sql": "SELECT id, name, email, age, department FROM users WHERE active = true"
      }
    },
    {
      "name": "enrich_users",
      "type": "enrich",
      "depends_on": ["extract_users"],
      "config": {
        "enrichments": [
          {
            "column": "age_group",
            "expression": "'Young' if age < 30 else 'Senior'"
          },
          {
            "column": "email_domain",
            "expression": "email.split('@')[1]"
          }
        ]
      }
    },
    {
      "name": "pivot_by_department",
      "type": "pivot",
      "depends_on": ["enrich_users"],
      "config": {
        "index_columns": ["department"],
        "pivot_column": "age_group",
        "value_columns": ["id"],
        "aggregation": "count"
      }
    }
  ],
  "output": {
    "format": "json",
    "file_path": "user_analysis_result.json"
  }
}
```

### API ä½¿ç”¨ç¤ºä¾‹

```python
import requests

# æäº¤ UQM ä»»åŠ¡
response = requests.post(
    "http://localhost:8000/api/v1/execute",
    json=uqm_config
)

execution_id = response.json()["execution_id"]

# æŸ¥è¯¢æ‰§è¡ŒçŠ¶æ€
status_response = requests.get(
    f"http://localhost:8000/api/v1/executions/{execution_id}/status"
)

print(status_response.json())
```

### æ”¯æŒçš„æ­¥éª¤ç±»å‹

| æ­¥éª¤ç±»å‹ | è¯´æ˜ | é…ç½®ç¤ºä¾‹ |
|---------|------|----------|
| `query` | SQL æŸ¥è¯¢ | `{"sql": "SELECT * FROM table"}` |
| `enrich` | æ•°æ®ä¸°å¯ŒåŒ– | `{"enrichments": [{"column": "new_col", "expression": "old_col * 2"}]}` |
| `pivot` | æ•°æ®é€è§† | `{"index_columns": ["col1"], "pivot_column": "col2", "value_columns": ["col3"]}` |
| `unpivot` | é€†é€è§† | `{"id_columns": ["id"], "value_columns": ["val1", "val2"]}` |
| `union` | æ•°æ®åˆå¹¶ | `{"datasets": ["step1", "step2"], "type": "union"}` |
| `filter` | æ•°æ®ç­›é€‰ | `{"condition": "age > 25"}` |
| `assert` | æ•°æ®æ–­è¨€ | `{"assertions": [{"name": "test", "condition": "len(df) > 0"}]}` |

## ğŸ”§ é…ç½®è¯´æ˜

### ç¯å¢ƒå˜é‡

```bash
# æ•°æ®åº“é…ç½®
DATABASE_URL=postgresql://user:password@localhost:5432/dbname

# Redis é…ç½®
REDIS_URL=redis://localhost:6379/0

# åº”ç”¨é…ç½®
HOST=0.0.0.0
PORT=8000
LOG_LEVEL=INFO
WORKERS=4

# å®‰å…¨é…ç½®
SECRET_KEY=your-secret-key
CORS_ORIGINS=["http://localhost:3000"]

# ç¼“å­˜é…ç½®
CACHE_BACKEND=redis
CACHE_TTL=3600

# ç›‘æ§é…ç½®
ENABLE_METRICS=true
METRICS_PORT=9090
```

### é«˜çº§é…ç½®

æŸ¥çœ‹ [é…ç½®æ–‡æ¡£](docs/configuration.md) äº†è§£æ›´å¤šé…ç½®é€‰é¡¹ã€‚

## ğŸ§ª æµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest

# è¿è¡Œå•å…ƒæµ‹è¯•
pytest tests/unit/

# è¿è¡Œé›†æˆæµ‹è¯•
pytest tests/integration/

# ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
pytest --cov=src --cov-report=html
```

## ğŸ“Š ç›‘æ§

UQM Backend å†…ç½®äº†å®Œæ•´çš„ç›‘æ§ç³»ç»Ÿï¼š

- **å¥åº·æ£€æŸ¥**: `/health` ç«¯ç‚¹
- **æŒ‡æ ‡ç›‘æ§**: Prometheus æ ¼å¼çš„æŒ‡æ ‡
- **æ€§èƒ½åˆ†æ**: æ‰§è¡Œæ—¶é—´å’Œèµ„æºä½¿ç”¨ç»Ÿè®¡
- **é”™è¯¯è¿½è¸ª**: è¯¦ç»†çš„é”™è¯¯æ—¥å¿—å’Œå †æ ˆè·Ÿè¸ª

### Grafana ä»ªè¡¨æ¿

ä½¿ç”¨ Docker Compose éƒ¨ç½²æ—¶ï¼ŒGrafana å°†è‡ªåŠ¨é…ç½®ç›‘æ§ä»ªè¡¨æ¿ï¼š

```bash
# å¯åŠ¨ç›‘æ§æœåŠ¡
docker-compose --profile monitoring up -d

# è®¿é—® Grafana
# URL: http://localhost:3000
# ç”¨æˆ·å: admin
# å¯†ç : admin
```

## ğŸ›¡ï¸ å®‰å…¨ç‰¹æ€§

- **SQL æ³¨å…¥é˜²æŠ¤**: è‡ªåŠ¨æ£€æµ‹å’Œé˜»æ­¢æ½œåœ¨çš„ SQL æ³¨å…¥æ”»å‡»
- **è¡¨è¾¾å¼å®‰å…¨**: å®‰å…¨çš„è¡¨è¾¾å¼æ‰§è¡Œç¯å¢ƒï¼Œé˜²æ­¢ä»£ç æ³¨å…¥
- **èº«ä»½éªŒè¯**: æ”¯æŒå¤šç§èº«ä»½éªŒè¯æœºåˆ¶
- **æ•°æ®è„±æ•**: æ•æ„Ÿæ•°æ®è‡ªåŠ¨è„±æ•å¤„ç†
- **å®¡è®¡æ—¥å¿—**: å®Œæ•´çš„æ“ä½œå®¡è®¡è¿½è¸ª

## ğŸš€ æ€§èƒ½ä¼˜åŒ–

- **è¿æ¥æ± **: æ•°æ®åº“è¿æ¥æ± ç®¡ç†
- **æŸ¥è¯¢ç¼“å­˜**: å¤šçº§ç¼“å­˜ç­–ç•¥
- **å¹¶è¡Œæ‰§è¡Œ**: æ­¥éª¤çº§å¹¶è¡Œå¤„ç†
- **å†…å­˜ä¼˜åŒ–**: å¤§æ•°æ®é›†æµå¼å¤„ç†
- **æŸ¥è¯¢ä¼˜åŒ–**: è‡ªåŠ¨ SQL æŸ¥è¯¢ä¼˜åŒ–

## ğŸ“š æ–‡æ¡£

- [API æ–‡æ¡£](http://localhost:8000/docs) - äº¤äº’å¼ API æ–‡æ¡£
- [ç”¨æˆ·æŒ‡å—](docs/user-guide.md) - è¯¦ç»†ä½¿ç”¨è¯´æ˜
- [å¼€å‘æŒ‡å—](docs/development.md) - å¼€å‘å’Œè´¡çŒ®æŒ‡å—
- [éƒ¨ç½²æŒ‡å—](docs/deployment.md) - ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
- [æ•…éšœæ’é™¤](docs/troubleshooting.md) - å¸¸è§é—®é¢˜è§£å†³

## ğŸ¤ è´¡çŒ®

æ¬¢è¿è´¡çŒ®ä»£ç ï¼è¯·æŸ¥çœ‹ [è´¡çŒ®æŒ‡å—](CONTRIBUTING.md) äº†è§£è¯¦ç»†ä¿¡æ¯ã€‚

### å¼€å‘ç¯å¢ƒè®¾ç½®

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/uqm/uqm-backend.git
cd uqm-backend

# å®‰è£…å¼€å‘ä¾èµ–
pip install -e ".[dev]"

# å®‰è£… pre-commit é’©å­
pre-commit install

# è¿è¡Œä»£ç è´¨é‡æ£€æŸ¥
make lint

# è¿è¡Œæµ‹è¯•
make test
```

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäº [MIT è®¸å¯è¯](LICENSE) å¼€æºã€‚

## ğŸ“ æ”¯æŒ

- **æ–‡æ¡£**: [https://uqm-backend.readthedocs.io/](https://uqm-backend.readthedocs.io/)
- **é—®é¢˜æŠ¥å‘Š**: [GitHub Issues](https://github.com/uqm/uqm-backend/issues)
- **è®¨è®º**: [GitHub Discussions](https://github.com/uqm/uqm-backend/discussions)
- **é‚®ä»¶**: team@uqm.com

## ğŸ¯ è·¯çº¿å›¾

### v1.1.0 (è®¡åˆ’ä¸­)
- [ ] æ”¯æŒæ›´å¤šæ•°æ®æº (MongoDB, Elasticsearch)
- [ ] å›¾å½¢åŒ–é…ç½®ç•Œé¢
- [ ] æ•°æ®è¡€ç¼˜è¿½è¸ª
- [ ] é«˜çº§è°ƒåº¦åŠŸèƒ½

### v1.2.0 (è§„åˆ’ä¸­)
- [ ] æœºå™¨å­¦ä¹ æ­¥éª¤æ”¯æŒ
- [ ] æµå¼æ•°æ®å¤„ç†
- [ ] åˆ†å¸ƒå¼æ‰§è¡Œ
- [ ] æ•°æ®è´¨é‡ç›‘æ§

## â­ Star å†å²

[![Stargazers over time](https://starchart.cc/uqm/uqm-backend.svg)](https://starchart.cc/uqm/uqm-backend)

---

<div align="center">
  Made with â¤ï¸ by the UQM Team
</div>

================
File: uqm-backend/requirements.txt
================
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
pydantic-settings==2.1.0
SQLAlchemy==2.0.23
psycopg2-binary==2.9.9
pymysql==1.1.0
pandas==2.1.4
numpy==1.24.4
jsonschema==4.20.0
redis==5.0.1
aioredis==2.0.1
python-multipart==0.0.6
python-jose[cryptography]==3.3.0
passlib[bcrypt]==1.7.4
python-dotenv==1.0.0
structlog==23.2.0
pytest==7.4.3
pytest-asyncio==0.21.1
pytest-cov==4.1.0
httpx==0.25.2
black==23.11.0
isort==5.12.0
flake8==6.1.0
mypy==1.7.1

================
File: uqm-backend/schema/uqm_schema.json
================
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "UQM Configuration Schema",
    "description": "ç»Ÿä¸€æŸ¥è¯¢æ¨¡å‹ï¼ˆUQMï¼‰é…ç½®æ–‡ä»¶çš„ JSON Schema å®šä¹‰",
    "type": "object",
    "required": ["name", "version", "steps"],
    "properties": {
        "name": {
            "type": "string",
            "description": "UQM é…ç½®çš„åç§°",
            "minLength": 1,
            "maxLength": 100,
            "pattern": "^[a-zA-Z][a-zA-Z0-9_-]*$"
        },
        "version": {
            "type": "string",
            "description": "UQM é…ç½®çš„ç‰ˆæœ¬å·",
            "pattern": "^\\d+\\.\\d+(\\.\\d+)?$"
        },
        "description": {
            "type": "string",
            "description": "UQM é…ç½®çš„æè¿°ä¿¡æ¯",
            "maxLength": 500
        },
        "author": {
            "type": "string",
            "description": "é…ç½®æ–‡ä»¶çš„ä½œè€…",
            "maxLength": 100
        },
        "created_at": {
            "type": "string",
            "format": "date-time",
            "description": "é…ç½®åˆ›å»ºæ—¶é—´"
        },
        "updated_at": {
            "type": "string",
            "format": "date-time",
            "description": "é…ç½®æ›´æ–°æ—¶é—´"
        },
        "tags": {
            "type": "array",
            "description": "é…ç½®æ ‡ç­¾",
            "items": {
                "type": "string",
                "minLength": 1,
                "maxLength": 50
            },
            "uniqueItems": true
        },
        "datasources": {
            "type": "object",
            "description": "æ•°æ®æºé…ç½®",
            "patternProperties": {
                "^[a-zA-Z][a-zA-Z0-9_]*$": {
                    "$ref": "#/definitions/datasource"
                }
            },
            "additionalProperties": false
        },
        "variables": {
            "type": "object",
            "description": "å…¨å±€å˜é‡å®šä¹‰",
            "patternProperties": {
                "^[a-zA-Z][a-zA-Z0-9_]*$": {
                    "oneOf": [
                        {"type": "string"},
                        {"type": "number"},
                        {"type": "boolean"},
                        {"type": "null"}
                    ]
                }
            },
            "additionalProperties": false
        },
        "steps": {
            "type": "array",
            "description": "æ‰§è¡Œæ­¥éª¤åˆ—è¡¨",
            "minItems": 1,
            "items": {
                "$ref": "#/definitions/step"
            }
        },
        "output": {
            "$ref": "#/definitions/output",
            "description": "è¾“å‡ºé…ç½®"
        },
        "options": {
            "$ref": "#/definitions/options",
            "description": "æ‰§è¡Œé€‰é¡¹"
        }
    },
    "definitions": {
        "datasource": {
            "type": "object",
            "required": ["type", "connection"],
            "properties": {
                "type": {
                    "type": "string",
                    "description": "æ•°æ®æºç±»å‹",
                    "enum": [
                        "postgres", "mysql", "sqlite", "oracle", "sqlserver",
                        "mongodb", "redis", "elasticsearch", "api", "file",
                        "snowflake", "bigquery", "redshift", "clickhouse"
                    ]
                },
                "connection": {
                    "type": "object",
                    "description": "è¿æ¥é…ç½®",
                    "properties": {
                        "host": {"type": "string"},
                        "port": {"type": "integer", "minimum": 1, "maximum": 65535},
                        "database": {"type": "string"},
                        "username": {"type": "string"},
                        "password": {"type": "string"},
                        "schema": {"type": "string"},
                        "url": {"type": "string"},
                        "file_path": {"type": "string"},
                        "encoding": {"type": "string", "default": "utf-8"},
                        "timeout": {"type": "integer", "minimum": 1, "default": 30},
                        "pool_size": {"type": "integer", "minimum": 1, "default": 5},
                        "ssl": {"type": "boolean", "default": false},
                        "options": {"type": "object"}
                    },
                    "additionalProperties": true
                },
                "description": {
                    "type": "string",
                    "description": "æ•°æ®æºæè¿°"
                },
                "cache": {
                    "type": "object",
                    "description": "ç¼“å­˜é…ç½®",
                    "properties": {
                        "enabled": {"type": "boolean", "default": false},
                        "ttl": {"type": "integer", "minimum": 0, "default": 3600},
                        "key_prefix": {"type": "string"}
                    }
                }
            },
            "additionalProperties": false
        },
        "step": {
            "type": "object",
            "required": ["name", "type"],
            "properties": {
                "name": {
                    "type": "string",
                    "description": "æ­¥éª¤åç§°",
                    "minLength": 1,
                    "maxLength": 100,
                    "pattern": "^[a-zA-Z][a-zA-Z0-9_]*$"
                },
                "type": {
                    "type": "string",
                    "description": "æ­¥éª¤ç±»å‹",
                    "enum": [
                        "query", "enrich", "pivot", "unpivot", "union", "assert",
                        "filter", "sort", "group", "aggregate", "join", "transform",
                        "validate", "export", "import", "script"
                    ]
                },
                "description": {
                    "type": "string",
                    "description": "æ­¥éª¤æè¿°"
                },
                "enabled": {
                    "type": "boolean",
                    "description": "æ˜¯å¦å¯ç”¨è¯¥æ­¥éª¤",
                    "default": true
                },
                "depends_on": {
                    "type": "array",
                    "description": "ä¾èµ–çš„æ­¥éª¤åç§°åˆ—è¡¨",
                    "items": {
                        "type": "string",
                        "pattern": "^[a-zA-Z][a-zA-Z0-9_]*$"
                    },
                    "uniqueItems": true
                },
                "datasource": {
                    "type": "string",
                    "description": "ä½¿ç”¨çš„æ•°æ®æºåç§°",
                    "pattern": "^[a-zA-Z][a-zA-Z0-9_]*$"
                },
                "config": {
                    "type": "object",
                    "description": "æ­¥éª¤é…ç½®",
                    "anyOf": [
                        {"$ref": "#/definitions/query_config"},
                        {"$ref": "#/definitions/enrich_config"},
                        {"$ref": "#/definitions/pivot_config"},
                        {"$ref": "#/definitions/unpivot_config"},
                        {"$ref": "#/definitions/union_config"},
                        {"$ref": "#/definitions/assert_config"},
                        {"$ref": "#/definitions/filter_config"},
                        {"$ref": "#/definitions/sort_config"},
                        {"$ref": "#/definitions/group_config"},
                        {"$ref": "#/definitions/aggregate_config"},
                        {"$ref": "#/definitions/join_config"},
                        {"$ref": "#/definitions/transform_config"}
                    ]
                },
                "cache": {
                    "type": "object",
                    "description": "æ­¥éª¤ç¼“å­˜é…ç½®",
                    "properties": {
                        "enabled": {"type": "boolean", "default": false},
                        "ttl": {"type": "integer", "minimum": 0, "default": 3600},
                        "key": {"type": "string"}
                    }
                },
                "timeout": {
                    "type": "integer",
                    "description": "æ­¥éª¤è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰",
                    "minimum": 1,
                    "default": 300
                },
                "retry": {
                    "type": "object",
                    "description": "é‡è¯•é…ç½®",
                    "properties": {
                        "enabled": {"type": "boolean", "default": false},
                        "max_attempts": {"type": "integer", "minimum": 1, "maximum": 10, "default": 3},
                        "delay": {"type": "integer", "minimum": 0, "default": 1}
                    }
                }
            },
            "additionalProperties": false
        },
        "query_config": {
            "type": "object",
            "required": ["sql"],
            "properties": {
                "sql": {
                    "type": "string",
                    "description": "SQL æŸ¥è¯¢è¯­å¥",
                    "minLength": 1
                },
                "parameters": {
                    "type": "object",
                    "description": "æŸ¥è¯¢å‚æ•°",
                    "additionalProperties": true
                }
            }
        },
        "enrich_config": {
            "type": "object",
            "required": ["enrichments"],
            "properties": {
                "enrichments": {
                    "type": "array",
                    "description": "ä¸°å¯ŒåŒ–è§„åˆ™",
                    "items": {
                        "type": "object",
                        "required": ["column", "expression"],
                        "properties": {
                            "column": {"type": "string"},
                            "expression": {"type": "string"},
                            "condition": {"type": "string"}
                        }
                    }
                }
            }
        },
        "pivot_config": {
            "type": "object",
            "required": ["index_columns", "pivot_column", "value_columns"],
            "properties": {
                "index_columns": {
                    "type": "array",
                    "items": {"type": "string"},
                    "minItems": 1
                },
                "pivot_column": {"type": "string"},
                "value_columns": {
                    "type": "array",
                    "items": {"type": "string"},
                    "minItems": 1
                },
                "aggregation": {
                    "type": "string",
                    "enum": ["sum", "count", "avg", "min", "max", "first", "last"],
                    "default": "sum"
                }
            }
        },
        "unpivot_config": {
            "type": "object",
            "required": ["id_columns", "value_columns"],
            "properties": {
                "id_columns": {
                    "type": "array",
                    "items": {"type": "string"},
                    "minItems": 1
                },
                "value_columns": {
                    "type": "array",
                    "items": {"type": "string"},
                    "minItems": 1
                },
                "variable_name": {"type": "string", "default": "variable"},
                "value_name": {"type": "string", "default": "value"}
            }
        },
        "union_config": {
            "type": "object",
            "required": ["datasets"],
            "properties": {
                "datasets": {
                    "type": "array",
                    "items": {"type": "string"},
                    "minItems": 2
                },
                "type": {
                    "type": "string",
                    "enum": ["union", "union_all"],
                    "default": "union"
                },
                "ignore_index": {"type": "boolean", "default": true}
            }
        },
        "assert_config": {
            "type": "object",
            "required": ["assertions"],
            "properties": {
                "assertions": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "required": ["name", "condition"],
                        "properties": {
                            "name": {"type": "string"},
                            "condition": {"type": "string"},
                            "message": {"type": "string"},
                            "severity": {
                                "type": "string",
                                "enum": ["error", "warning", "info"],
                                "default": "error"
                            }
                        }
                    }
                }
            }
        },
        "filter_config": {
            "type": "object",
            "required": ["condition"],
            "properties": {
                "condition": {"type": "string"}
            }
        },
        "sort_config": {
            "type": "object",
            "required": ["columns"],
            "properties": {
                "columns": {
                    "type": "array",
                    "items": {
                        "oneOf": [
                            {"type": "string"},
                            {
                                "type": "object",
                                "required": ["column"],
                                "properties": {
                                    "column": {"type": "string"},
                                    "ascending": {"type": "boolean", "default": true}
                                }
                            }
                        ]
                    },
                    "minItems": 1
                }
            }
        },
        "group_config": {
            "type": "object",
            "required": ["columns"],
            "properties": {
                "columns": {
                    "type": "array",
                    "items": {"type": "string"},
                    "minItems": 1
                }
            }
        },
        "aggregate_config": {
            "type": "object",
            "properties": {
                "group_by": {
                    "type": "array",
                    "items": {"type": "string"}
                },
                "agg_functions": {
                    "type": "object",
                    "patternProperties": {
                        "^[a-zA-Z][a-zA-Z0-9_]*$": {
                            "oneOf": [
                                {
                                    "type": "string",
                                    "enum": ["sum", "count", "avg", "min", "max", "std", "var", "median", "first", "last"]
                                },
                                {
                                    "type": "array",
                                    "items": {
                                        "type": "string",
                                        "enum": ["sum", "count", "avg", "min", "max", "std", "var", "median", "first", "last"]
                                    }
                                }
                            ]
                        }
                    }
                }
            }
        },
        "join_config": {
            "type": "object",
            "required": ["right_dataset", "left_on", "right_on"],
            "properties": {
                "right_dataset": {"type": "string"},
                "left_on": {
                    "oneOf": [
                        {"type": "string"},
                        {"type": "array", "items": {"type": "string"}}
                    ]
                },
                "right_on": {
                    "oneOf": [
                        {"type": "string"},
                        {"type": "array", "items": {"type": "string"}}
                    ]
                },
                "how": {
                    "type": "string",
                    "enum": ["inner", "left", "right", "outer", "cross"],
                    "default": "inner"
                },
                "suffixes": {
                    "type": "array",
                    "items": {"type": "string"},
                    "minItems": 2,
                    "maxItems": 2,
                    "default": ["_x", "_y"]
                }
            }
        },
        "transform_config": {
            "type": "object",
            "required": ["transformations"],
            "properties": {
                "transformations": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "required": ["type"],
                        "properties": {
                            "type": {
                                "type": "string",
                                "enum": ["select", "drop", "rename", "cast", "compute", "replace"]
                            },
                            "columns": {
                                "oneOf": [
                                    {"type": "string"},
                                    {"type": "array", "items": {"type": "string"}}
                                ]
                            },
                            "mapping": {"type": "object"},
                            "expression": {"type": "string"},
                            "data_type": {"type": "string"},
                            "old_value": {},
                            "new_value": {}
                        }
                    }
                }
            }
        },
        "output": {
            "type": "object",
            "properties": {
                "format": {
                    "type": "string",
                    "enum": ["json", "csv", "excel", "parquet", "sql", "table"],
                    "default": "json"
                },
                "destination": {
                    "type": "string",
                    "description": "è¾“å‡ºç›®æ ‡"
                },
                "file_path": {
                    "type": "string",
                    "description": "è¾“å‡ºæ–‡ä»¶è·¯å¾„"
                },
                "table_name": {
                    "type": "string",
                    "description": "è¾“å‡ºè¡¨å"
                },
                "datasource": {
                    "type": "string",
                    "description": "è¾“å‡ºæ•°æ®æº"
                },
                "options": {
                    "type": "object",
                    "description": "è¾“å‡ºé€‰é¡¹",
                    "properties": {
                        "encoding": {"type": "string", "default": "utf-8"},
                        "delimiter": {"type": "string", "default": ","},
                        "header": {"type": "boolean", "default": true},
                        "index": {"type": "boolean", "default": false},
                        "compression": {"type": "string", "enum": ["gzip", "bz2", "xz", "zip"]},
                        "if_exists": {"type": "string", "enum": ["fail", "replace", "append"], "default": "replace"}
                    }
                }
            }
        },
        "options": {
            "type": "object",
            "properties": {
                "parallel": {
                    "type": "boolean",
                    "description": "æ˜¯å¦å¹¶è¡Œæ‰§è¡Œ",
                    "default": false
                },
                "max_workers": {
                    "type": "integer",
                    "description": "æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°",
                    "minimum": 1,
                    "default": 4
                },
                "memory_limit": {
                    "type": "string",
                    "description": "å†…å­˜é™åˆ¶ï¼ˆå¦‚ '1GB', '500MB'ï¼‰"
                },
                "timeout": {
                    "type": "integer",
                    "description": "æ€»è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰",
                    "minimum": 1,
                    "default": 3600
                },
                "log_level": {
                    "type": "string",
                    "enum": ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
                    "default": "INFO"
                },
                "fail_fast": {
                    "type": "boolean",
                    "description": "é‡åˆ°é”™è¯¯æ—¶æ˜¯å¦ç«‹å³åœæ­¢",
                    "default": true
                },
                "validate_data": {
                    "type": "boolean",
                    "description": "æ˜¯å¦éªŒè¯æ•°æ®è´¨é‡",
                    "default": true
                },
                "cache_enabled": {
                    "type": "boolean",
                    "description": "æ˜¯å¦å¯ç”¨ç¼“å­˜",
                    "default": false
                },
                "cache_backend": {
                    "type": "string",
                    "enum": ["memory", "redis", "file"],
                    "default": "memory"
                },
                "optimize_queries": {
                    "type": "boolean",
                    "description": "æ˜¯å¦ä¼˜åŒ–æŸ¥è¯¢",
                    "default": true
                },
                "batch_size": {
                    "type": "integer",
                    "description": "æ‰¹å¤„ç†å¤§å°",
                    "minimum": 1,
                    "default": 1000
                }
            }
        }
    },
    "examples": [
        {
            "name": "sample_uqm",
            "version": "1.0.0",
            "description": "ç¤ºä¾‹ UQM é…ç½®",
            "datasources": {
                "main_db": {
                    "type": "postgres",
                    "connection": {
                        "host": "localhost",
                        "port": 5432,
                        "database": "testdb",
                        "username": "user",
                        "password": "password"
                    }
                }
            },
            "steps": [
                {
                    "name": "extract_data",
                    "type": "query",
                    "datasource": "main_db",
                    "config": {
                        "sql": "SELECT * FROM users WHERE active = true"
                    }
                },
                {
                    "name": "enrich_data",
                    "type": "enrich",
                    "depends_on": ["extract_data"],
                    "config": {
                        "enrichments": [
                            {
                                "column": "full_name",
                                "expression": "first_name + ' ' + last_name"
                            }
                        ]
                    }
                }
            ],
            "output": {
                "format": "json",
                "file_path": "output.json"
            }
        }
    ]
}

================
File: uqm-backend/setup.py
================
"""
UQM Backend å®‰è£…é…ç½®
"""

from setuptools import setup, find_packages
from pathlib import Path

# è¯»å– README æ–‡ä»¶
this_directory = Path(__file__).parent
long_description = (this_directory / "README.md").read_text(encoding='utf-8')

# è¯»å–ç‰ˆæœ¬ä¿¡æ¯
version_file = this_directory / "src" / "__version__.py"
version_info = {}
if version_file.exists():
    exec(version_file.read_text(), version_info)
    version = version_info.get('__version__', '1.0.0')
else:
    version = '1.0.0'

# è¯»å–ä¾èµ–
requirements_file = this_directory / "requirements.txt"
if requirements_file.exists():
    with open(requirements_file, 'r', encoding='utf-8') as f:
        requirements = [line.strip() for line in f if line.strip() and not line.startswith('#')]
else:
    requirements = [
        'fastapi>=0.104.0',
        'uvicorn[standard]>=0.24.0',
        'pydantic>=2.5.0',
        'pydantic-settings>=2.1.0',
        'sqlalchemy>=2.0.0',
        'alembic>=1.13.0',
        'asyncpg>=0.29.0',
        'aiomysql>=0.2.0',
        'aiosqlite>=0.19.0',
        'redis>=5.0.0',
        'aioredis>=2.0.0',
        'pandas>=2.1.0',
        'numpy>=1.24.0',
        'jsonschema>=4.20.0',
        'jinja2>=3.1.0',
        'python-multipart>=0.0.6',
        'python-jose[cryptography]>=3.3.0',
        'passlib[bcrypt]>=1.7.4',
        'aiofiles>=23.2.0',
        'loguru>=0.7.0',
        'prometheus-client>=0.19.0',
        'structlog>=23.2.0'
    ]

# å¼€å‘ä¾èµ–
dev_requirements = [
    'pytest>=7.4.0',
    'pytest-asyncio>=0.21.0',
    'pytest-cov>=4.1.0',
    'pytest-mock>=3.12.0',
    'black>=23.10.0',
    'isort>=5.12.0',
    'flake8>=6.1.0',
    'mypy>=1.7.0',
    'pre-commit>=3.5.0',
    'bandit>=1.7.5',
    'safety>=2.3.0'
]

# æµ‹è¯•ä¾èµ–
test_requirements = [
    'pytest>=7.4.0',
    'pytest-asyncio>=0.21.0',
    'pytest-cov>=4.1.0',
    'pytest-mock>=3.12.0',
    'httpx>=0.25.0',
    'factory-boy>=3.3.0',
    'faker>=20.1.0'
]

# æ–‡æ¡£ä¾èµ–
docs_requirements = [
    'mkdocs>=1.5.0',
    'mkdocs-material>=9.4.0',
    'mkdocstrings[python]>=0.24.0',
    'mkdocs-mermaid2-plugin>=1.1.0'
]

# ç”Ÿäº§ä¾èµ–
prod_requirements = [
    'gunicorn>=21.2.0',
    'setproctitle>=1.3.0'
]

setup(
    name="uqm-backend",
    version=version,
    author="UQM Team",
    author_email="team@uqm.com",
    description="ç»Ÿä¸€æŸ¥è¯¢æ¨¡å‹ï¼ˆUQMï¼‰åç«¯æ‰§è¡Œå¼•æ“",
    long_description=long_description,
    long_description_content_type="text/markdown",
    url="https://github.com/uqm/uqm-backend",
    project_urls={
        "Bug Tracker": "https://github.com/uqm/uqm-backend/issues",
        "Documentation": "https://uqm-backend.readthedocs.io/",
        "Source Code": "https://github.com/uqm/uqm-backend",
    },
    packages=find_packages(include=['src', 'src.*']),
    package_dir={'': '.'},
    include_package_data=True,
    package_data={
        'src': ['schema/*.json', 'templates/*.html', 'static/*'],
        '': ['*.md', '*.txt', '*.yml', '*.yaml'],
    },
    classifiers=[
        "Development Status :: 4 - Beta",
        "Intended Audience :: Developers",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
        "Programming Language :: Python :: 3.11",
        "Programming Language :: Python :: 3.12",
        "Topic :: Software Development :: Libraries :: Python Modules",
        "Topic :: Database",
        "Topic :: Internet :: WWW/HTTP :: HTTP Servers",
        "Topic :: Scientific/Engineering :: Information Analysis",
        "Framework :: FastAPI",
        "Framework :: Pydantic",
        "Environment :: Web Environment",
    ],
    python_requires=">=3.9",
    install_requires=requirements,
    extras_require={
        'dev': dev_requirements,
        'test': test_requirements,
        'docs': docs_requirements,
        'prod': prod_requirements,
        'all': dev_requirements + test_requirements + docs_requirements + prod_requirements,
    },
    entry_points={
        'console_scripts': [
            'uqm-server=src.main:main',
            'uqm-cli=src.cli:main',
        ],
    },
    keywords=[
        'uqm', 'unified-query-model', 'data-processing', 'etl', 'sql',
        'fastapi', 'async', 'pandas', 'database', 'query-engine'
    ],
    license="MIT",
    zip_safe=False,
    platforms=['any'],
    
    # å…ƒæ•°æ®
    maintainer="UQM Team",
    maintainer_email="team@uqm.com",
    
    # ä¾èµ–çº¦æŸ
    python_requires=">=3.9,<4.0",
    
    # å®‰å…¨é…ç½®
    options={
        'bdist_wheel': {
            'universal': False,
        },
    },
)

# å®‰è£…åè„šæœ¬
def post_install():
    """å®‰è£…åæ‰§è¡Œçš„è„šæœ¬"""
    print("UQM Backend å®‰è£…æˆåŠŸ!")
    print("è¿è¡Œ 'uqm-server --help' æŸ¥çœ‹ä½¿ç”¨è¯´æ˜")
    print("è®¿é—® http://localhost:8000/docs æŸ¥çœ‹ API æ–‡æ¡£")

if __name__ == "__main__":
    import sys
    if 'install' in sys.argv:
        post_install()

================
File: uqm-backend/src/__version__.py
================
"""
ç‰ˆæœ¬ä¿¡æ¯
"""

__version__ = "1.0.0"
__version_info__ = (1, 0, 0)
__author__ = "UQM Team"
__email__ = "team@uqm.com"
__description__ = "ç»Ÿä¸€æŸ¥è¯¢æ¨¡å‹ï¼ˆUQMï¼‰åç«¯æ‰§è¡Œå¼•æ“"

================
File: uqm-backend/src/api/__init__.py
================
"""APIæ¨¡å—åŒ…åˆå§‹åŒ–æ–‡ä»¶"""

================
File: uqm-backend/src/api/models.py
================
"""
Pydanticæ•°æ®æ¨¡å‹å®šä¹‰
å®šä¹‰APIè¯·æ±‚å’Œå“åº”çš„æ•°æ®ç»“æ„
"""

from typing import Any, Dict, List, Optional, Union
from datetime import datetime
from enum import Enum

from pydantic import BaseModel, Field, validator


class StepType(str, Enum):
    """æ­¥éª¤ç±»å‹æšä¸¾"""
    QUERY = "query"
    ENRICH = "enrich"
    PIVOT = "pivot"
    UNPIVOT = "unpivot"
    UNION = "union"
    ASSERT = "assert"


class JobStatus(str, Enum):
    """å¼‚æ­¥ä»»åŠ¡çŠ¶æ€æšä¸¾"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


class Parameter(BaseModel):
    """å‚æ•°å®šä¹‰æ¨¡å‹"""
    name: str = Field(..., description="å‚æ•°åç§°")
    type: str = Field(..., description="å‚æ•°ç±»å‹")
    default: Optional[Any] = Field(None, description="é»˜è®¤å€¼")
    required: bool = Field(True, description="æ˜¯å¦å¿…éœ€")
    description: Optional[str] = Field(None, description="å‚æ•°æè¿°")


class Metadata(BaseModel):
    """å…ƒæ•°æ®æ¨¡å‹"""
    name: str = Field(..., description="æŸ¥è¯¢åç§°")
    description: Optional[str] = Field(None, description="æŸ¥è¯¢æè¿°")
    version: Optional[str] = Field("1.0", description="ç‰ˆæœ¬å·")
    author: Optional[str] = Field(None, description="ä½œè€…")
    created_at: Optional[datetime] = Field(None, description="åˆ›å»ºæ—¶é—´")
    updated_at: Optional[datetime] = Field(None, description="æ›´æ–°æ—¶é—´")
    tags: Optional[List[str]] = Field(default_factory=list, description="æ ‡ç­¾åˆ—è¡¨")


class StepResult(BaseModel):
    """å•ä¸ªæ­¥éª¤æ‰§è¡Œç»“æœæ¨¡å‹"""
    step_name: str = Field(..., description="æ­¥éª¤åç§°")
    step_type: StepType = Field(..., description="æ­¥éª¤ç±»å‹")
    status: str = Field(..., description="æ‰§è¡ŒçŠ¶æ€")
    data: Optional[List[Dict[str, Any]]] = Field(None, description="æ­¥éª¤ç»“æœæ•°æ®")
    row_count: int = Field(0, description="ç»“æœè¡Œæ•°")
    execution_time: float = Field(0.0, description="æ‰§è¡Œæ—¶é—´(ç§’)")
    cache_hit: bool = Field(False, description="æ˜¯å¦å‘½ä¸­ç¼“å­˜")
    error: Optional[str] = Field(None, description="é”™è¯¯ä¿¡æ¯")


class ValidationError(BaseModel):
    """éªŒè¯é”™è¯¯è¯¦æƒ…æ¨¡å‹"""
    field: str = Field(..., description="é”™è¯¯å­—æ®µ")
    message: str = Field(..., description="é”™è¯¯ä¿¡æ¯")
    value: Optional[Any] = Field(None, description="é”™è¯¯å€¼")


class ErrorResponse(BaseModel):
    """é”™è¯¯å“åº”æ¨¡å‹"""
    error: Dict[str, Any] = Field(..., description="é”™è¯¯è¯¦æƒ…")
    
    class Config:
        schema_extra = {
            "example": {
                "error": {
                    "code": "VALIDATION_ERROR",
                    "message": "æ•°æ®éªŒè¯å¤±è´¥",
                    "details": {}
                }
            }
        }


class UQMRequest(BaseModel):
    """UQMè¯·æ±‚æ•°æ®æ¨¡å‹"""
    uqm: Dict[str, Any] = Field(..., description="UQM JSONå®šä¹‰")
    parameters: Optional[Dict[str, Any]] = Field(default_factory=dict, description="æŸ¥è¯¢å‚æ•°")
    options: Optional[Dict[str, Any]] = Field(default_factory=dict, description="æ‰§è¡Œé€‰é¡¹")
    
    @validator('uqm')
    def validate_uqm(cls, v):
        """éªŒè¯UQMç»“æ„"""
        if not isinstance(v, dict):
            raise ValueError("UQMå¿…é¡»æ˜¯ä¸€ä¸ªJSONå¯¹è±¡")
        
        required_fields = ['metadata', 'steps']
        for field in required_fields:
            if field not in v:
                raise ValueError(f"UQMç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
        
        return v
    
    class Config:
        schema_extra = {
            "example": {
                "uqm": {
                    "metadata": {
                        "name": "ç¤ºä¾‹æŸ¥è¯¢",
                        "description": "è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹UQMæŸ¥è¯¢"
                    },
                    "steps": [
                        {
                            "name": "step1",
                            "type": "query",
                            "config": {
                                "data_source": "users",
                                "dimensions": ["id", "name"],
                                "metrics": [],
                                "filters": []
                            }
                        }
                    ],
                    "output": "step1"
                },
                "parameters": {
                    "limit": 100
                },
                "options": {
                    "cache_enabled": True,
                    "timeout": 300
                }
            }
        }


class UQMResponse(BaseModel):
    """UQMå“åº”æ•°æ®æ¨¡å‹"""
    success: bool = Field(..., description="æ‰§è¡Œæ˜¯å¦æˆåŠŸ")
    data: Optional[List[Dict[str, Any]]] = Field(None, description="æŸ¥è¯¢ç»“æœæ•°æ®")
    metadata: Optional[Metadata] = Field(None, description="æŸ¥è¯¢å…ƒæ•°æ®")
    execution_info: Dict[str, Any] = Field(default_factory=dict, description="æ‰§è¡Œä¿¡æ¯")
    step_results: Optional[List[StepResult]] = Field(None, description="æ­¥éª¤æ‰§è¡Œç»“æœ")
    
    class Config:
        schema_extra = {
            "example": {
                "success": True,
                "data": [
                    {"id": 1, "name": "å¼ ä¸‰"},
                    {"id": 2, "name": "æå››"}
                ],
                "metadata": {
                    "name": "ç¤ºä¾‹æŸ¥è¯¢",
                    "description": "è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹UQMæŸ¥è¯¢"
                },
                "execution_info": {
                    "total_time": 1.23,
                    "row_count": 2,
                    "cache_hit": False
                },
                "step_results": [
                    {
                        "step_name": "step1",
                        "step_type": "query",
                        "status": "completed",
                        "row_count": 2,
                        "execution_time": 1.23,
                        "cache_hit": False
                    }
                ]
            }
        }


class ValidationRequest(BaseModel):
    """éªŒè¯è¯·æ±‚æ¨¡å‹"""
    uqm: Dict[str, Any] = Field(..., description="è¦éªŒè¯çš„UQM JSONå®šä¹‰")
    
    class Config:
        schema_extra = {
            "example": {
                "uqm": {
                    "metadata": {
                        "name": "ç¤ºä¾‹æŸ¥è¯¢"
                    },
                    "steps": [
                        {
                            "name": "step1",
                            "type": "query",
                            "config": {}
                        }
                    ],
                    "output": "step1"
                }
            }
        }


class ValidationResponse(BaseModel):
    """éªŒè¯å“åº”æ¨¡å‹"""
    valid: bool = Field(..., description="æ˜¯å¦éªŒè¯é€šè¿‡")
    errors: Optional[List[ValidationError]] = Field(None, description="éªŒè¯é”™è¯¯åˆ—è¡¨")
    warnings: Optional[List[str]] = Field(None, description="è­¦å‘Šä¿¡æ¯åˆ—è¡¨")
    
    class Config:
        schema_extra = {
            "example": {
                "valid": True,
                "errors": None,
                "warnings": ["å»ºè®®æ·»åŠ æŸ¥è¯¢æè¿°"]
            }
        }


class HealthResponse(BaseModel):
    """å¥åº·æ£€æŸ¥å“åº”æ¨¡å‹"""
    status: str = Field(..., description="æœåŠ¡çŠ¶æ€")
    timestamp: datetime = Field(..., description="æ£€æŸ¥æ—¶é—´")
    version: str = Field(..., description="æœåŠ¡ç‰ˆæœ¬")
    uptime: float = Field(..., description="è¿è¡Œæ—¶é—´(ç§’)")
    
    class Config:
        schema_extra = {
            "example": {
                "status": "healthy",
                "timestamp": "2023-12-01T10:00:00Z",
                "version": "0.1.0",
                "uptime": 3600.0
            }
        }


class MetricsResponse(BaseModel):
    """æŒ‡æ ‡å“åº”æ¨¡å‹"""
    total_requests: int = Field(..., description="æ€»è¯·æ±‚æ•°")
    successful_requests: int = Field(..., description="æˆåŠŸè¯·æ±‚æ•°")
    failed_requests: int = Field(..., description="å¤±è´¥è¯·æ±‚æ•°")
    average_response_time: float = Field(..., description="å¹³å‡å“åº”æ—¶é—´(ç§’)")
    active_connections: int = Field(..., description="æ´»è·ƒè¿æ¥æ•°")
    cache_hit_rate: float = Field(..., description="ç¼“å­˜å‘½ä¸­ç‡")
    
    class Config:
        schema_extra = {
            "example": {
                "total_requests": 1000,
                "successful_requests": 950,
                "failed_requests": 50,
                "average_response_time": 1.5,
                "active_connections": 10,
                "cache_hit_rate": 0.75
            }
        }


class AsyncJobRequest(BaseModel):
    """å¼‚æ­¥ä»»åŠ¡è¯·æ±‚æ¨¡å‹"""
    uqm: Dict[str, Any] = Field(..., description="UQM JSONå®šä¹‰")
    parameters: Optional[Dict[str, Any]] = Field(default_factory=dict, description="æŸ¥è¯¢å‚æ•°")
    callback_url: Optional[str] = Field(None, description="ç»“æœå›è°ƒURL")
    
    class Config:
        schema_extra = {
            "example": {
                "uqm": {
                    "metadata": {"name": "å¼‚æ­¥æŸ¥è¯¢"},
                    "steps": [{"name": "step1", "type": "query", "config": {}}],
                    "output": "step1"
                },
                "parameters": {},
                "callback_url": "https://example.com/callback"
            }
        }


class AsyncJobResponse(BaseModel):
    """å¼‚æ­¥ä»»åŠ¡å“åº”æ¨¡å‹"""
    job_id: str = Field(..., description="ä»»åŠ¡ID")
    status: JobStatus = Field(..., description="ä»»åŠ¡çŠ¶æ€")
    created_at: datetime = Field(..., description="åˆ›å»ºæ—¶é—´")
    estimated_completion: Optional[datetime] = Field(None, description="é¢„è®¡å®Œæˆæ—¶é—´")
    
    class Config:
        schema_extra = {
            "example": {
                "job_id": "job_123456",
                "status": "pending",
                "created_at": "2023-12-01T10:00:00Z",
                "estimated_completion": "2023-12-01T10:05:00Z"
            }
        }


class JobStatusResponse(BaseModel):
    """ä»»åŠ¡çŠ¶æ€å“åº”æ¨¡å‹"""
    job_id: str = Field(..., description="ä»»åŠ¡ID")
    status: JobStatus = Field(..., description="ä»»åŠ¡çŠ¶æ€")
    created_at: datetime = Field(..., description="åˆ›å»ºæ—¶é—´")
    started_at: Optional[datetime] = Field(None, description="å¼€å§‹æ—¶é—´")
    completed_at: Optional[datetime] = Field(None, description="å®Œæˆæ—¶é—´")
    progress: Optional[float] = Field(None, description="è¿›åº¦ç™¾åˆ†æ¯”")
    result: Optional[UQMResponse] = Field(None, description="æ‰§è¡Œç»“æœ")
    error: Optional[str] = Field(None, description="é”™è¯¯ä¿¡æ¯")
    
    class Config:
        schema_extra = {
            "example": {
                "job_id": "job_123456",
                "status": "completed",
                "created_at": "2023-12-01T10:00:00Z",
                "started_at": "2023-12-01T10:00:01Z",
                "completed_at": "2023-12-01T10:02:30Z",
                "progress": 100.0,
                "result": {
                    "success": True,
                    "data": [{"id": 1, "name": "å¼ ä¸‰"}]
                }
            }
        }

================
File: uqm-backend/src/api/routes.py
================
"""
REST APIè·¯ç”±å®šä¹‰
å®šä¹‰æ‰€æœ‰APIç«¯ç‚¹å’Œå¤„ç†é€»è¾‘
"""

import time
import uuid
from datetime import datetime, timedelta
from typing import Dict, Any

from fastapi import APIRouter, HTTPException, BackgroundTasks, Depends
from fastapi.responses import JSONResponse

from src.api.models import (
    UQMRequest, UQMResponse, ValidationRequest, ValidationResponse,
    HealthResponse, MetricsResponse, ErrorResponse,
    AsyncJobRequest, AsyncJobResponse, JobStatusResponse,
    JobStatus
)
from src.core.engine import get_uqm_engine
from src.core.cache import get_cache_manager
from src.utils.logging import get_logger
from src.utils.exceptions import (
    ValidationError, ExecutionError, TimeoutError
)
from src.config.settings import get_settings

# åˆ›å»ºè·¯ç”±å®ä¾‹
router = APIRouter()
logger = get_logger(__name__)

# æœåŠ¡å¯åŠ¨æ—¶é—´
START_TIME = time.time()

# å¼‚æ­¥ä»»åŠ¡å­˜å‚¨ï¼ˆç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨Redisæˆ–æ•°æ®åº“ï¼‰
async_jobs: Dict[str, Dict[str, Any]] = {}

# æŒ‡æ ‡ç»Ÿè®¡
metrics = {
    "total_requests": 0,
    "successful_requests": 0,
    "failed_requests": 0,
    "total_response_time": 0.0,
    "active_connections": 0,
    "cache_hits": 0,
    "cache_misses": 0
}


def update_metrics(success: bool, response_time: float, cache_hit: bool = False) -> None:
    """æ›´æ–°æŒ‡æ ‡ç»Ÿè®¡"""
    metrics["total_requests"] += 1
    metrics["total_response_time"] += response_time
    
    if success:
        metrics["successful_requests"] += 1
    else:
        metrics["failed_requests"] += 1
    
    if cache_hit:
        metrics["cache_hits"] += 1
    else:
        metrics["cache_misses"] += 1


@router.post(
    "/execute",
    response_model=UQMResponse,
    summary="æ‰§è¡ŒUQMæŸ¥è¯¢",
    description="æ‰§è¡ŒUQMæŸ¥è¯¢å¹¶è¿”å›ç»“æœ",
    responses={
        400: {"model": ErrorResponse, "description": "è¯·æ±‚å‚æ•°é”™è¯¯"},
        500: {"model": ErrorResponse, "description": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯"}
    }
)
async def execute_uqm(request: UQMRequest) -> UQMResponse:
    """
    æ‰§è¡ŒUQMæŸ¥è¯¢çš„ä¸»è¦ç«¯ç‚¹
    
    Args:
        request: UQMè¯·æ±‚æ•°æ®
        
    Returns:
        UQMæ‰§è¡Œç»“æœ
    """
    start_time = time.time()
    
    try:
        logger.info(
            "å¼€å§‹æ‰§è¡ŒUQMæŸ¥è¯¢",
            uqm_name=request.uqm.get("metadata", {}).get("name", "æœªå‘½å"),
            parameters=request.parameters
        )
        
        # è·å–UQMå¼•æ“å®ä¾‹
        engine = get_uqm_engine()
        
        # æ‰§è¡ŒæŸ¥è¯¢
        result = await engine.process(
            uqm_data=request.uqm,
            parameters=request.parameters,
            options=request.options
        )
        
        response_time = time.time() - start_time
        update_metrics(success=True, response_time=response_time)
        
        logger.info(
            "UQMæŸ¥è¯¢æ‰§è¡Œå®Œæˆ",
            execution_time=response_time,
            row_count=len(result.data) if result.data else 0
        )
        
        return result
        
    except ValidationError as e:
        response_time = time.time() - start_time
        update_metrics(success=False, response_time=response_time)
        
        logger.error(
            "UQMæŸ¥è¯¢éªŒè¯å¤±è´¥",
            error=str(e),
            execution_time=response_time
        )
        
        raise HTTPException(
            status_code=400,
            detail={
                "code": "VALIDATION_ERROR",
                "message": str(e),
                "details": e.details
            }
        )
        
    except ExecutionError as e:
        response_time = time.time() - start_time
        update_metrics(success=False, response_time=response_time)
        
        logger.error(
            "UQMæŸ¥è¯¢æ‰§è¡Œå¤±è´¥",
            error=str(e),
            execution_time=response_time
        )
        
        raise HTTPException(
            status_code=500,
            detail={
                "code": "EXECUTION_ERROR",
                "message": str(e),
                "details": e.details
            }
        )
        
    except TimeoutError as e:
        response_time = time.time() - start_time
        update_metrics(success=False, response_time=response_time)
        
        logger.error(
            "UQMæŸ¥è¯¢æ‰§è¡Œè¶…æ—¶",
            error=str(e),
            execution_time=response_time
        )
        
        raise HTTPException(
            status_code=408,
            detail={
                "code": "TIMEOUT_ERROR",
                "message": str(e),
                "details": e.details
            }
        )
        
    except Exception as e:
        response_time = time.time() - start_time
        update_metrics(success=False, response_time=response_time)
        
        logger.error(
            "UQMæŸ¥è¯¢æ‰§è¡Œå‡ºç°æœªçŸ¥é”™è¯¯",
            error=str(e),
            execution_time=response_time,
            exc_info=True
        )
        
        raise HTTPException(
            status_code=500,
            detail={
                "code": "INTERNAL_ERROR",
                "message": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯",
                "details": {}
            }
        )


@router.post(
    "/validate",
    response_model=ValidationResponse,
    summary="éªŒè¯UQMå®šä¹‰",
    description="éªŒè¯UQMå®šä¹‰çš„æœ‰æ•ˆæ€§",
    responses={
        400: {"model": ErrorResponse, "description": "è¯·æ±‚å‚æ•°é”™è¯¯"}
    }
)
async def validate_uqm(request: ValidationRequest) -> ValidationResponse:
    """
    éªŒè¯UQMå®šä¹‰æœ‰æ•ˆæ€§
    
    Args:
        request: éªŒè¯è¯·æ±‚æ•°æ®
        
    Returns:
        éªŒè¯ç»“æœ
    """
    try:
        logger.info(
            "å¼€å§‹éªŒè¯UQMå®šä¹‰",
            uqm_name=request.uqm.get("metadata", {}).get("name", "æœªå‘½å")
        )
        
        # è·å–UQMå¼•æ“å®ä¾‹
        engine = get_uqm_engine()
        
        # éªŒè¯UQMå®šä¹‰
        validation_result = await engine.validate_query(request.uqm)
        
        logger.info(
            "UQMå®šä¹‰éªŒè¯å®Œæˆ",
            valid=validation_result.valid,
            error_count=len(validation_result.errors) if validation_result.errors else 0
        )
        
        return validation_result
        
    except Exception as e:
        logger.error(
            "UQMå®šä¹‰éªŒè¯å‡ºç°é”™è¯¯",
            error=str(e),
            exc_info=True
        )
        
        raise HTTPException(
            status_code=500,
            detail={
                "code": "VALIDATION_ERROR",
                "message": "éªŒè¯è¿‡ç¨‹å‡ºç°é”™è¯¯",
                "details": {"error": str(e)}
            }
        )


@router.get(
    "/health",
    response_model=HealthResponse,
    summary="å¥åº·æ£€æŸ¥",
    description="æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€"
)
async def health_check() -> HealthResponse:
    """
    å¥åº·æ£€æŸ¥ç«¯ç‚¹
    
    Returns:
        æœåŠ¡å¥åº·çŠ¶æ€
    """
    uptime = time.time() - START_TIME
    
    return HealthResponse(
        status="healthy",
        timestamp=datetime.utcnow(),
        version="0.1.0",
        uptime=uptime
    )


@router.get(
    "/metrics",
    response_model=MetricsResponse,
    summary="è·å–ç³»ç»ŸæŒ‡æ ‡",
    description="è·å–ç³»ç»Ÿè¿è¡ŒæŒ‡æ ‡å’Œç»Ÿè®¡ä¿¡æ¯"
)
async def get_metrics() -> MetricsResponse:
    """
    è·å–ç³»ç»ŸæŒ‡æ ‡
    
    Returns:
        ç³»ç»ŸæŒ‡æ ‡æ•°æ®
    """
    # è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡
    total_cache_requests = metrics["cache_hits"] + metrics["cache_misses"]
    cache_hit_rate = (
        metrics["cache_hits"] / total_cache_requests 
        if total_cache_requests > 0 else 0.0
    )
    
    # è®¡ç®—å¹³å‡å“åº”æ—¶é—´
    avg_response_time = (
        metrics["total_response_time"] / metrics["total_requests"]
        if metrics["total_requests"] > 0 else 0.0
    )
    
    return MetricsResponse(
        total_requests=metrics["total_requests"],
        successful_requests=metrics["successful_requests"],
        failed_requests=metrics["failed_requests"],
        average_response_time=avg_response_time,
        active_connections=metrics["active_connections"],
        cache_hit_rate=cache_hit_rate
    )


async def execute_async_job(job_id: str, uqm_data: Dict[str, Any], 
                          parameters: Dict[str, Any], callback_url: str = None) -> None:
    """
    æ‰§è¡Œå¼‚æ­¥ä»»åŠ¡
    
    Args:
        job_id: ä»»åŠ¡ID
        uqm_data: UQMæ•°æ®
        parameters: æŸ¥è¯¢å‚æ•°
        callback_url: å›è°ƒURL
    """
    try:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿è¡Œä¸­
        async_jobs[job_id].update({
            "status": JobStatus.RUNNING,
            "started_at": datetime.utcnow(),
            "progress": 0.0
        })
        
        logger.info(f"å¼€å§‹æ‰§è¡Œå¼‚æ­¥ä»»åŠ¡: {job_id}")
        
        # è·å–UQMå¼•æ“å®ä¾‹
        engine = get_uqm_engine()
        
        # æ‰§è¡ŒæŸ¥è¯¢
        result = await engine.process(
            uqm_data=uqm_data,
            parameters=parameters
        )
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
        async_jobs[job_id].update({
            "status": JobStatus.COMPLETED,
            "completed_at": datetime.utcnow(),
            "progress": 100.0,
            "result": result
        })
        
        logger.info(f"å¼‚æ­¥ä»»åŠ¡æ‰§è¡Œå®Œæˆ: {job_id}")
        
        # å¦‚æœæœ‰å›è°ƒURLï¼Œå‘é€ç»“æœï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
        if callback_url:
            logger.info(f"å‘é€å›è°ƒé€šçŸ¥: {callback_url}")
        
    except Exception as e:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
        async_jobs[job_id].update({
            "status": JobStatus.FAILED,
            "completed_at": datetime.utcnow(),
            "error": str(e)
        })
        
        logger.error(f"å¼‚æ­¥ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {job_id}", error=str(e), exc_info=True)


@router.post(
    "/execute-async",
    response_model=AsyncJobResponse,
    summary="å¼‚æ­¥æ‰§è¡ŒUQMæŸ¥è¯¢",
    description="å¼‚æ­¥æ‰§è¡ŒUQMæŸ¥è¯¢ï¼Œè¿”å›ä»»åŠ¡ID",
    responses={
        400: {"model": ErrorResponse, "description": "è¯·æ±‚å‚æ•°é”™è¯¯"}
    }
)
async def execute_async(request: AsyncJobRequest, background_tasks: BackgroundTasks) -> AsyncJobResponse:
    """
    å¼‚æ­¥æ‰§è¡ŒUQMæŸ¥è¯¢
    
    Args:
        request: å¼‚æ­¥ä»»åŠ¡è¯·æ±‚
        background_tasks: åå°ä»»åŠ¡
        
    Returns:
        å¼‚æ­¥ä»»åŠ¡å“åº”
    """
    try:
        # ç”Ÿæˆä»»åŠ¡ID
        job_id = str(uuid.uuid4())
        
        # åˆ›å»ºä»»åŠ¡è®°å½•
        created_at = datetime.utcnow()
        async_jobs[job_id] = {
            "job_id": job_id,
            "status": JobStatus.PENDING,
            "created_at": created_at,
            "uqm_data": request.uqm,
            "parameters": request.parameters,
            "callback_url": request.callback_url
        }
        
        # æ·»åŠ åå°ä»»åŠ¡
        background_tasks.add_task(
            execute_async_job,
            job_id=job_id,
            uqm_data=request.uqm,
            parameters=request.parameters,
            callback_url=request.callback_url
        )
        
        logger.info(f"åˆ›å»ºå¼‚æ­¥ä»»åŠ¡: {job_id}")
        
        return AsyncJobResponse(
            job_id=job_id,
            status=JobStatus.PENDING,
            created_at=created_at,
            estimated_completion=created_at + timedelta(minutes=5)
        )
        
    except Exception as e:
        logger.error("åˆ›å»ºå¼‚æ­¥ä»»åŠ¡å¤±è´¥", error=str(e), exc_info=True)
        
        raise HTTPException(
            status_code=500,
            detail={
                "code": "ASYNC_JOB_ERROR",
                "message": "åˆ›å»ºå¼‚æ­¥ä»»åŠ¡å¤±è´¥",
                "details": {"error": str(e)}
            }
        )


@router.get(
    "/jobs/{job_id}",
    response_model=JobStatusResponse,
    summary="è·å–å¼‚æ­¥ä»»åŠ¡çŠ¶æ€",
    description="è·å–æŒ‡å®šä»»åŠ¡çš„æ‰§è¡ŒçŠ¶æ€å’Œç»“æœ",
    responses={
        404: {"model": ErrorResponse, "description": "ä»»åŠ¡ä¸å­˜åœ¨"}
    }
)
async def get_job_status(job_id: str) -> JobStatusResponse:
    """
    è·å–å¼‚æ­¥ä»»åŠ¡çŠ¶æ€
    
    Args:
        job_id: ä»»åŠ¡ID
        
    Returns:
        ä»»åŠ¡çŠ¶æ€ä¿¡æ¯
    """
    if job_id not in async_jobs:
        raise HTTPException(
            status_code=404,
            detail={
                "code": "JOB_NOT_FOUND",
                "message": f"ä»»åŠ¡ä¸å­˜åœ¨: {job_id}",
                "details": {}
            }
        )
    
    job_info = async_jobs[job_id]
    
    return JobStatusResponse(
        job_id=job_id,
        status=job_info["status"],
        created_at=job_info["created_at"],
        started_at=job_info.get("started_at"),
        completed_at=job_info.get("completed_at"),
        progress=job_info.get("progress"),
        result=job_info.get("result"),
        error=job_info.get("error")
    )


@router.delete(
    "/jobs/{job_id}",
    summary="å–æ¶ˆå¼‚æ­¥ä»»åŠ¡",
    description="å–æ¶ˆæŒ‡å®šçš„å¼‚æ­¥ä»»åŠ¡",
    responses={
        404: {"model": ErrorResponse, "description": "ä»»åŠ¡ä¸å­˜åœ¨"}
    }
)
async def cancel_job(job_id: str) -> Dict[str, str]:
    """
    å–æ¶ˆå¼‚æ­¥ä»»åŠ¡
    
    Args:
        job_id: ä»»åŠ¡ID
        
    Returns:
        å–æ¶ˆç»“æœ
    """
    if job_id not in async_jobs:
        raise HTTPException(
            status_code=404,
            detail={
                "code": "JOB_NOT_FOUND",
                "message": f"ä»»åŠ¡ä¸å­˜åœ¨: {job_id}",
                "details": {}
            }
        )
    
    job_info = async_jobs[job_id]
    
    if job_info["status"] in [JobStatus.COMPLETED, JobStatus.FAILED]:
        raise HTTPException(
            status_code=400,
            detail={
                "code": "JOB_ALREADY_FINISHED",
                "message": f"ä»»åŠ¡å·²å®Œæˆï¼Œæ— æ³•å–æ¶ˆ: {job_id}",
                "details": {}
            }
        )
    
    # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå·²å–æ¶ˆ
    async_jobs[job_id].update({
        "status": JobStatus.CANCELLED,
        "completed_at": datetime.utcnow()
    })
    
    logger.info(f"å–æ¶ˆå¼‚æ­¥ä»»åŠ¡: {job_id}")
    
    return {"message": f"ä»»åŠ¡å·²å–æ¶ˆ: {job_id}"}

================
File: uqm-backend/src/config/__init__.py
================
"""é…ç½®æ¨¡å—åŒ…åˆå§‹åŒ–æ–‡ä»¶"""

================
File: uqm-backend/src/config/settings.py
================
"""
åº”ç”¨ç¨‹åºé…ç½®ç®¡ç†æ¨¡å—
è´Ÿè´£åŠ è½½å’Œç®¡ç†æ‰€æœ‰é…ç½®é¡¹
"""

import os
from typing import List, Optional
from functools import lru_cache

from pydantic import Field
from pydantic_settings import BaseSettings, SettingsConfigDict


class Settings(BaseSettings):
    """åº”ç”¨ç¨‹åºé…ç½®ç±»"""
    
    # åŸºç¡€é…ç½®
    DEBUG: bool = Field(default=False, description="è°ƒè¯•æ¨¡å¼å¼€å…³")
    HOST: str = Field(default="0.0.0.0", description="æœåŠ¡å™¨ä¸»æœºåœ°å€")
    PORT: int = Field(default=8000, description="æœåŠ¡å™¨ç«¯å£")
    SECRET_KEY: str = Field(default="change-me-in-production", description="åº”ç”¨ç¨‹åºå¯†é’¥")
    
    # æ•°æ®åº“é…ç½®
    DATABASE_URL: Optional[str] = Field(default=None, description="ä¸»æ•°æ®åº“è¿æ¥URL")
    MYSQL_URL: Optional[str] = Field(default=None, description="MySQLæ•°æ®åº“è¿æ¥URL")
    SQLITE_URL: Optional[str] = Field(default="sqlite:///./uqm.db", description="SQLiteæ•°æ®åº“è¿æ¥URL")
    
    # Redisé…ç½®
    REDIS_URL: str = Field(default="redis://localhost:6379/0", description="Redisè¿æ¥URL")
    
    # ç¼“å­˜é…ç½®
    CACHE_TYPE: str = Field(default="memory", description="ç¼“å­˜ç±»å‹")
    CACHE_DEFAULT_TIMEOUT: int = Field(default=3600, description="é»˜è®¤ç¼“å­˜è¶…æ—¶æ—¶é—´(ç§’)")
    CACHE_MAX_SIZE: int = Field(default=1000, description="å†…å­˜ç¼“å­˜æœ€å¤§æ¡ç›®æ•°")
    
    # æ—¥å¿—é…ç½®
    LOG_LEVEL: str = Field(default="INFO", description="æ—¥å¿—çº§åˆ«")
    LOG_FORMAT: str = Field(default="json", description="æ—¥å¿—æ ¼å¼")
    
    # æŸ¥è¯¢é…ç½®
    MAX_QUERY_TIMEOUT: int = Field(default=300, description="æœ€å¤§æŸ¥è¯¢è¶…æ—¶æ—¶é—´(ç§’)")
    MAX_CONCURRENT_QUERIES: int = Field(default=10, description="æœ€å¤§å¹¶å‘æŸ¥è¯¢æ•°")
    QUERY_RESULT_LIMIT: int = Field(default=10000, description="æŸ¥è¯¢ç»“æœè¡Œæ•°é™åˆ¶")
    
    # å®‰å…¨é…ç½®
    ALLOWED_HOSTS: List[str] = Field(default=["localhost", "127.0.0.1"], description="å…è®¸çš„ä¸»æœºåˆ—è¡¨")
    CORS_ORIGINS: List[str] = Field(default=[], description="CORSå…è®¸çš„æº")
    CORS_CREDENTIALS: bool = Field(default=True, description="CORSæ˜¯å¦å…è®¸å‡­è¯")
    CORS_METHODS: List[str] = Field(default=["GET", "POST", "PUT", "DELETE", "OPTIONS"], description="CORSå…è®¸çš„HTTPæ–¹æ³•")
    CORS_HEADERS: List[str] = Field(default=["*"], description="CORSå…è®¸çš„è¯·æ±‚å¤´")
    
    # ç›‘æ§é…ç½®
    ENABLE_METRICS: bool = Field(default=True, description="æ˜¯å¦å¯ç”¨æŒ‡æ ‡ç›‘æ§")
    METRICS_PATH: str = Field(default="/metrics", description="æŒ‡æ ‡æ¥å£è·¯å¾„")
    
    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        case_sensitive=True,
        extra="ignore"
    )
    
    def get_database_config(self) -> dict:
        """è·å–æ•°æ®åº“é…ç½®ä¿¡æ¯"""
        return {
            "postgresql": self.DATABASE_URL,
            "mysql": self.MYSQL_URL,
            "sqlite": self.SQLITE_URL,
        }
    
    def get_cache_config(self) -> dict:
        """è·å–ç¼“å­˜é…ç½®ä¿¡æ¯"""
        return {
            "type": self.CACHE_TYPE,
            "redis_url": self.REDIS_URL,
            "default_timeout": self.CACHE_DEFAULT_TIMEOUT,
            "max_size": self.CACHE_MAX_SIZE,
        }
    
    def get_logging_config(self) -> dict:
        """è·å–æ—¥å¿—é…ç½®ä¿¡æ¯"""
        return {
            "level": self.LOG_LEVEL,
            "format": self.LOG_FORMAT,
        }
    
    def validate_config(self) -> bool:
        """éªŒè¯é…ç½®æœ‰æ•ˆæ€§"""
        try:
            # éªŒè¯å¿…éœ€çš„é…ç½®é¡¹
            if not self.SECRET_KEY or self.SECRET_KEY == "change-me-in-production":
                if not self.DEBUG:
                    raise ValueError("ç”Ÿäº§ç¯å¢ƒå¿…é¡»è®¾ç½®æœ‰æ•ˆçš„SECRET_KEY")
            
            # éªŒè¯æ•°æ®åº“é…ç½®
            if not any([self.DATABASE_URL, self.MYSQL_URL, self.SQLITE_URL]):
                raise ValueError("è‡³å°‘éœ€è¦é…ç½®ä¸€ä¸ªæ•°æ®åº“è¿æ¥")
            
            # éªŒè¯ç«¯å£èŒƒå›´
            if not (1 <= self.PORT <= 65535):
                raise ValueError("ç«¯å£å·å¿…é¡»åœ¨1-65535èŒƒå›´å†…")
            
            # éªŒè¯è¶…æ—¶é…ç½®
            if self.MAX_QUERY_TIMEOUT <= 0:
                raise ValueError("æŸ¥è¯¢è¶…æ—¶æ—¶é—´å¿…é¡»å¤§äº0")
            
            # éªŒè¯å¹¶å‘é…ç½®
            if self.MAX_CONCURRENT_QUERIES <= 0:
                raise ValueError("æœ€å¤§å¹¶å‘æŸ¥è¯¢æ•°å¿…é¡»å¤§äº0")
            
            return True
            
        except ValueError as e:
            print(f"é…ç½®éªŒè¯å¤±è´¥: {e}")
            return False


@lru_cache()
def get_settings() -> Settings:
    """è·å–é…ç½®å®ä¾‹(å•ä¾‹æ¨¡å¼)"""
    settings = Settings()
    
    # éªŒè¯é…ç½®
    if not settings.validate_config():
        raise RuntimeError("é…ç½®éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")
    
    return settings

================
File: uqm-backend/src/connectors/__init__.py
================
"""è¿æ¥å™¨æ¨¡å—åŒ…åˆå§‹åŒ–æ–‡ä»¶"""

================
File: uqm-backend/src/connectors/base.py
================
"""
æ•°æ®è¿æ¥å™¨åŸºç±»
å®šä¹‰æ‰€æœ‰è¿æ¥å™¨çš„é€šç”¨æ¥å£å’ŒåŸºç¡€åŠŸèƒ½
"""

from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional, Union
from functools import lru_cache

from src.utils.logging import LoggerMixin
from src.utils.exceptions import ConnectionError
from src.config.settings import get_settings


class BaseConnector(ABC, LoggerMixin):
    """æ•°æ®è¿æ¥å™¨åŸºç±»"""
    
    def __init__(self, connection_config: Dict[str, Any]):
        """
        åˆå§‹åŒ–è¿æ¥å™¨
        
        Args:
            connection_config: è¿æ¥é…ç½®
        """
        self.connection_config = connection_config
        self.connection = None
        self.is_connected = False
    
    @abstractmethod
    async def connect(self) -> None:
        """å»ºç«‹æ•°æ®åº“è¿æ¥"""
        pass
    
    @abstractmethod
    async def execute_query(self, query: str, params: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        æ‰§è¡ŒæŸ¥è¯¢
        
        Args:
            query: SQLæŸ¥è¯¢è¯­å¥
            params: æŸ¥è¯¢å‚æ•°
            
        Returns:
            æŸ¥è¯¢ç»“æœ
        """
        pass
    
    @abstractmethod
    async def close(self) -> None:
        """å…³é—­è¿æ¥"""
        pass
    
    async def execute_batch(self, queries: List[str]) -> List[List[Dict[str, Any]]]:
        """
        æ‰¹é‡æ‰§è¡ŒæŸ¥è¯¢
        
        Args:
            queries: æŸ¥è¯¢è¯­å¥åˆ—è¡¨
            
        Returns:
            æ‰¹é‡æŸ¥è¯¢ç»“æœ
        """
        results = []
        for query in queries:
            result = await self.execute_query(query)
            results.append(result)
        return results
    
    async def test_connection(self) -> bool:
        """
        æµ‹è¯•è¿æ¥æœ‰æ•ˆæ€§
        
        Returns:
            è¿æ¥æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            if not self.is_connected:
                await self.connect()
            
            # æ‰§è¡Œç®€å•æŸ¥è¯¢æµ‹è¯•è¿æ¥
            await self.execute_query("SELECT 1")
            return True
            
        except Exception as e:
            self.log_error("è¿æ¥æµ‹è¯•å¤±è´¥", error=str(e))
            return False
    
    async def get_table_schema(self, table_name: str) -> Dict[str, Any]:
        """
        è·å–è¡¨ç»“æ„ä¿¡æ¯
        
        Args:
            table_name: è¡¨å
            
        Returns:
            è¡¨ç»“æ„ä¿¡æ¯
        """
        # é»˜è®¤å®ç°ï¼Œå­ç±»å¯ä»¥é‡å†™
        try:
            query = f"SELECT * FROM {table_name} LIMIT 0"
            await self.execute_query(query)
            return {"table_name": table_name, "columns": []}
        except Exception as e:
            self.log_error(f"è·å–è¡¨ {table_name} ç»“æ„å¤±è´¥", error=str(e))
            raise ConnectionError(f"è·å–è¡¨ç»“æ„å¤±è´¥: {e}")
    
    async def get_available_tables(self) -> List[str]:
        """
        è·å–å¯ç”¨è¡¨åˆ—è¡¨
        
        Returns:
            è¡¨ååˆ—è¡¨
        """
        # é»˜è®¤å®ç°ï¼Œå­ç±»å¯ä»¥é‡å†™
        return []
    
    def _handle_connection_error(self, error: Exception) -> None:
        """
        å¤„ç†è¿æ¥é”™è¯¯
        
        Args:
            error: é”™è¯¯ä¿¡æ¯
        """
        self.log_error("æ•°æ®åº“è¿æ¥é”™è¯¯", error=str(error))
        self.is_connected = False
        raise ConnectionError(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {error}")
    
    def _format_query_result(self, result: Any) -> List[Dict[str, Any]]:
        """
        æ ¼å¼åŒ–æŸ¥è¯¢ç»“æœ
        
        Args:
            result: åŸå§‹æŸ¥è¯¢ç»“æœ
            
        Returns:
            æ ¼å¼åŒ–åçš„ç»“æœ
        """
        # é»˜è®¤å®ç°ï¼Œå­ç±»å¯ä»¥é‡å†™
        if isinstance(result, list):
            return result
        return []


class BaseConnectorManager(LoggerMixin):
    """è¿æ¥å™¨ç®¡ç†å™¨åŸºç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–è¿æ¥å™¨ç®¡ç†å™¨"""
        self.connectors: Dict[str, BaseConnector] = {}
        self.settings = get_settings()
    
    def register_connector(self, name: str, connector: BaseConnector) -> None:
        """
        æ³¨å†Œè¿æ¥å™¨
        
        Args:
            name: è¿æ¥å™¨åç§°
            connector: è¿æ¥å™¨å®ä¾‹
        """
        self.connectors[name] = connector
        self.log_info(f"è¿æ¥å™¨ {name} æ³¨å†ŒæˆåŠŸ")
    
    def get_connector(self, name: str) -> Optional[BaseConnector]:
        """
        è·å–è¿æ¥å™¨
        
        Args:
            name: è¿æ¥å™¨åç§°
            
        Returns:
            è¿æ¥å™¨å®ä¾‹
        """
        return self.connectors.get(name)
    
    async def close_all(self) -> None:
        """å…³é—­æ‰€æœ‰è¿æ¥å™¨"""
        for name, connector in self.connectors.items():
            try:
                await connector.close()
                self.log_info(f"è¿æ¥å™¨ {name} å·²å…³é—­")
            except Exception as e:
                self.log_error(f"å…³é—­è¿æ¥å™¨ {name} å¤±è´¥", error=str(e))


class DefaultConnectorManager(BaseConnectorManager):
    """é»˜è®¤è¿æ¥å™¨ç®¡ç†å™¨å®ç°"""
    
    def __init__(self):
        """åˆå§‹åŒ–é»˜è®¤è¿æ¥å™¨ç®¡ç†å™¨"""
        super().__init__()
        self._initialize_connectors()
    
    def _initialize_connectors(self) -> None:
        """åˆå§‹åŒ–é»˜è®¤è¿æ¥å™¨"""
        try:
            # å¯¼å…¥å…·ä½“çš„è¿æ¥å™¨å®ç°
            from src.connectors.postgres import PostgresConnector
            from src.connectors.mysql import MySQLConnector
            from src.connectors.sqlite import SQLiteConnector
            
            # è·å–æ•°æ®åº“é…ç½®
            db_config = self.settings.get_database_config()
            
            # æ³¨å†ŒPostgreSQLè¿æ¥å™¨
            if db_config.get("postgresql"):
                postgres_connector = PostgresConnector(db_config["postgresql"])
                self.register_connector("postgresql", postgres_connector)
            
            # æ³¨å†ŒMySQLè¿æ¥å™¨
            if db_config.get("mysql"):
                mysql_connector = MySQLConnector(db_config["mysql"])
                self.register_connector("mysql", mysql_connector)
            
            # æ³¨å†ŒSQLiteè¿æ¥å™¨
            if db_config.get("sqlite"):
                sqlite_connector = SQLiteConnector(db_config["sqlite"])
                self.register_connector("sqlite", sqlite_connector)
            
            self.log_info("é»˜è®¤è¿æ¥å™¨åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            self.log_error("åˆå§‹åŒ–é»˜è®¤è¿æ¥å™¨å¤±è´¥", error=str(e))
    
    async def get_default_connector(self) -> BaseConnector:
        """
        è·å–é»˜è®¤è¿æ¥å™¨
        
        Returns:
            é»˜è®¤è¿æ¥å™¨å®ä¾‹
        """
        # ä¼˜å…ˆçº§ï¼šPostgreSQL > MySQL > SQLite
        for connector_name in ["postgresql", "mysql", "sqlite"]:
            connector = self.get_connector(connector_name)
            if connector:
                # æµ‹è¯•è¿æ¥
                if await connector.test_connection():
                    return connector
        
        raise ConnectionError("æ²¡æœ‰å¯ç”¨çš„æ•°æ®åº“è¿æ¥å™¨")


# å…¨å±€è¿æ¥å™¨ç®¡ç†å™¨å®ä¾‹
_connector_manager: Optional[BaseConnectorManager] = None


@lru_cache()
def get_connector_manager() -> BaseConnectorManager:
    """è·å–è¿æ¥å™¨ç®¡ç†å™¨å®ä¾‹(å•ä¾‹æ¨¡å¼)"""
    global _connector_manager
    
    if _connector_manager is None:
        _connector_manager = DefaultConnectorManager()
    
    return _connector_manager

================
File: uqm-backend/src/connectors/mysql.py
================
"""
MySQLæ•°æ®åº“è¿æ¥å™¨
å®ç°MySQLæ•°æ®åº“çš„è¿æ¥å’ŒæŸ¥è¯¢åŠŸèƒ½
"""

import asyncio
from typing import Any, Dict, List, Optional
from urllib.parse import urlparse

import pymysql
from pymysql.connections import Connection

from src.connectors.base import BaseConnector
from src.utils.exceptions import ConnectionError


class MySQLConnector(BaseConnector):
    """MySQLè¿æ¥å™¨å®ç°"""
    
    def __init__(self, connection_url: str):
        """
        åˆå§‹åŒ–MySQLè¿æ¥å™¨
        
        Args:
            connection_url: MySQLè¿æ¥URL
        """
        # è§£æè¿æ¥URL
        parsed_url = urlparse(connection_url)
        
        connection_config = {
            "host": parsed_url.hostname,
            "port": parsed_url.port or 3306,
            "database": parsed_url.path.lstrip('/'),
            "user": parsed_url.username,
            "password": parsed_url.password,
            "connection_url": connection_url
        }
        
        super().__init__(connection_config)
        self.connection: Optional[Connection] = None
    
    async def connect(self) -> None:
        """å»ºç«‹MySQLè¿æ¥"""
        try:
            self.log_info("æ­£åœ¨è¿æ¥MySQLæ•°æ®åº“", host=self.connection_config["host"])
            
            # åˆ›å»ºMySQLè¿æ¥
            self.connection = pymysql.connect(
                host=self.connection_config["host"],
                port=self.connection_config["port"],
                user=self.connection_config["user"],
                password=self.connection_config["password"],
                database=self.connection_config["database"],
                charset='utf8mb4',
                cursorclass=pymysql.cursors.DictCursor,
                autocommit=True
            )
            
            # æµ‹è¯•è¿æ¥
            with self.connection.cursor() as cursor:
                cursor.execute("SELECT VERSION()")
                version = cursor.fetchone()
                self.log_info("MySQLè¿æ¥æˆåŠŸ", version=version["VERSION()"])
            
            self.is_connected = True
            
        except Exception as e:
            self.log_error("MySQLè¿æ¥å¤±è´¥", error=str(e))
            self._handle_connection_error(e)
    
    async def execute_query(self, query: str, params: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        æ‰§è¡ŒMySQLæŸ¥è¯¢
        
        Args:
            query: SQLæŸ¥è¯¢è¯­å¥
            params: æŸ¥è¯¢å‚æ•°
            
        Returns:
            æŸ¥è¯¢ç»“æœ
        """
        if not self.is_connected or not self.connection:
            await self.connect()
        
        try:
            self.log_debug("æ‰§è¡ŒMySQLæŸ¥è¯¢", query=query[:200])
            
            with self.connection.cursor() as cursor:
                # æ‰§è¡ŒæŸ¥è¯¢
                if params:
                    cursor.execute(query, params)
                else:
                    cursor.execute(query)
                
                # è·å–ç»“æœ
                if cursor.description:
                    # æœ‰ç»“æœé›†çš„æŸ¥è¯¢
                    result = cursor.fetchall()
                    if not isinstance(result, list):
                        result = [result] if result else []
                else:
                    # æ²¡æœ‰ç»“æœé›†çš„æŸ¥è¯¢
                    result = []
            
            self.log_debug(
                "MySQLæŸ¥è¯¢æ‰§è¡Œå®Œæˆ",
                row_count=len(result)
            )
            
            return result
            
        except Exception as e:
            self.log_error("MySQLæŸ¥è¯¢æ‰§è¡Œå¤±è´¥", error=str(e), query=query[:200])
            self._handle_mysql_error(e)
            raise ConnectionError(f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}")
    
    async def close(self) -> None:
        """å…³é—­MySQLè¿æ¥"""
        try:
            if self.connection:
                self.connection.close()
                self.connection = None
            
            self.is_connected = False
            self.log_info("MySQLè¿æ¥å·²å…³é—­")
            
        except Exception as e:
            self.log_error("å…³é—­MySQLè¿æ¥å¤±è´¥", error=str(e))
    
    async def get_table_schema(self, table_name: str) -> Dict[str, Any]:
        """
        è·å–MySQLè¡¨ç»“æ„ä¿¡æ¯
        
        Args:
            table_name: è¡¨å
            
        Returns:
            è¡¨ç»“æ„ä¿¡æ¯
        """
        try:
            query = """
            SELECT 
                COLUMN_NAME as column_name,
                DATA_TYPE as data_type,
                IS_NULLABLE as is_nullable,
                COLUMN_DEFAULT as column_default,
                COLUMN_KEY as column_key,
                EXTRA as extra
            FROM INFORMATION_SCHEMA.COLUMNS 
            WHERE TABLE_SCHEMA = DATABASE() 
            AND TABLE_NAME = %s
            ORDER BY ORDINAL_POSITION
            """
            
            result = await self.execute_query(query, [table_name])
            
            columns = []
            for row in result:
                columns.append({
                    "name": row["column_name"],
                    "type": row["data_type"],
                    "nullable": row["is_nullable"] == "YES",
                    "default": row["column_default"],
                    "key": row["column_key"],
                    "extra": row["extra"]
                })
            
            return {
                "table_name": table_name,
                "columns": columns
            }
            
        except Exception as e:
            self.log_error(f"è·å–MySQLè¡¨ {table_name} ç»“æ„å¤±è´¥", error=str(e))
            raise ConnectionError(f"è·å–è¡¨ç»“æ„å¤±è´¥: {e}")
    
    async def get_available_tables(self) -> List[str]:
        """
        è·å–MySQLå¯ç”¨è¡¨åˆ—è¡¨
        
        Returns:
            è¡¨ååˆ—è¡¨
        """
        try:
            query = """
            SELECT TABLE_NAME as table_name
            FROM INFORMATION_SCHEMA.TABLES 
            WHERE TABLE_SCHEMA = DATABASE() 
            AND TABLE_TYPE = 'BASE TABLE'
            ORDER BY TABLE_NAME
            """
            
            result = await self.execute_query(query)
            tables = [row["table_name"] for row in result]
            
            self.log_info(f"è·å–åˆ° {len(tables)} ä¸ªMySQLè¡¨")
            return tables
            
        except Exception as e:
            self.log_error("è·å–MySQLè¡¨åˆ—è¡¨å¤±è´¥", error=str(e))
            raise ConnectionError(f"è·å–è¡¨åˆ—è¡¨å¤±è´¥: {e}")
    
    def _optimize_query_for_mysql(self, query: str) -> str:
        """
        ä¸ºMySQLä¼˜åŒ–æŸ¥è¯¢
        
        Args:
            query: åŸå§‹æŸ¥è¯¢
            
        Returns:
            ä¼˜åŒ–åçš„æŸ¥è¯¢
        """
        # MySQLç‰¹å®šçš„æŸ¥è¯¢ä¼˜åŒ–
        # æ¯”å¦‚ï¼š
        # - ä½¿ç”¨FORCE INDEXæç¤º
        # - ä¼˜åŒ–LIMITå’ŒOFFSET
        # - ä½¿ç”¨MySQLç‰¹å®šçš„å‡½æ•°
        
        return query
    
    def _handle_mysql_error(self, error: Exception) -> None:
        """
        å¤„ç†MySQLç‰¹å®šé”™è¯¯
        
        Args:
            error: é”™è¯¯ä¿¡æ¯
        """
        error_msg = str(error)
        
        # è¿æ¥é”™è¯¯
        if "lost connection" in error_msg.lower() or "gone away" in error_msg.lower():
            self.is_connected = False
            self.log_error("MySQLè¿æ¥æ–­å¼€")
        
        # è¯­æ³•é”™è¯¯
        elif "syntax" in error_msg.lower():
            self.log_error("MySQL SQLè¯­æ³•é”™è¯¯", error=error_msg)
        
        # è¡¨ä¸å­˜åœ¨
        elif "doesn't exist" in error_msg.lower():
            self.log_error("MySQLè¡¨ä¸å­˜åœ¨", error=error_msg)
        
        # æƒé™é”™è¯¯
        elif "access denied" in error_msg.lower():
            self.log_error("MySQLæƒé™ä¸è¶³", error=error_msg)
        
        # å…¶ä»–é”™è¯¯
        else:
            self.log_error("MySQLæœªçŸ¥é”™è¯¯", error=error_msg)
    
    def _create_connection_string(self) -> str:
        """
        åˆ›å»ºMySQLè¿æ¥å­—ç¬¦ä¸²
        
        Returns:
            è¿æ¥å­—ç¬¦ä¸²
        """
        config = self.connection_config
        return (
            f"mysql://{config['user']}:{config['password']}"
            f"@{config['host']}:{config['port']}/{config['database']}"
        )

================
File: uqm-backend/src/connectors/postgres.py
================
"""
PostgreSQLæ•°æ®åº“è¿æ¥å™¨
å®ç°PostgreSQLæ•°æ®åº“çš„è¿æ¥å’ŒæŸ¥è¯¢åŠŸèƒ½
"""

import asyncio
from typing import Any, Dict, List, Optional
from urllib.parse import urlparse

import psycopg2
import psycopg2.extras
from psycopg2.pool import SimpleConnectionPool

from src.connectors.base import BaseConnector
from src.utils.exceptions import ConnectionError


class PostgresConnector(BaseConnector):
    """PostgreSQLè¿æ¥å™¨å®ç°"""
    
    def __init__(self, connection_url: str):
        """
        åˆå§‹åŒ–PostgreSQLè¿æ¥å™¨
        
        Args:
            connection_url: PostgreSQLè¿æ¥URL
        """
        # è§£æè¿æ¥URL
        parsed_url = urlparse(connection_url)
        
        connection_config = {
            "host": parsed_url.hostname,
            "port": parsed_url.port or 5432,
            "database": parsed_url.path.lstrip('/'),
            "user": parsed_url.username,
            "password": parsed_url.password,
            "connection_url": connection_url
        }
        
        super().__init__(connection_config)
        self.connection_pool: Optional[SimpleConnectionPool] = None
    
    async def connect(self) -> None:
        """å»ºç«‹PostgreSQLè¿æ¥"""
        try:
            self.log_info("æ­£åœ¨è¿æ¥PostgreSQLæ•°æ®åº“", host=self.connection_config["host"])
            
            # åˆ›å»ºè¿æ¥æ± 
            self.connection_pool = SimpleConnectionPool(
                minconn=1,
                maxconn=10,
                host=self.connection_config["host"],
                port=self.connection_config["port"],
                database=self.connection_config["database"],
                user=self.connection_config["user"],
                password=self.connection_config["password"],
                cursor_factory=psycopg2.extras.RealDictCursor
            )
            
            # æµ‹è¯•è¿æ¥
            conn = self.connection_pool.getconn()
            try:
                with conn.cursor() as cursor:
                    cursor.execute("SELECT version()")
                    version = cursor.fetchone()
                self.log_info("PostgreSQLè¿æ¥æˆåŠŸ", version=version["version"])
            finally:
                self.connection_pool.putconn(conn)
            
            self.is_connected = True
            
        except Exception as e:
            self.log_error("PostgreSQLè¿æ¥å¤±è´¥", error=str(e))
            self._handle_connection_error(e)
    
    async def execute_query(self, query: str, params: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        æ‰§è¡ŒPostgreSQLæŸ¥è¯¢
        
        Args:
            query: SQLæŸ¥è¯¢è¯­å¥
            params: æŸ¥è¯¢å‚æ•°
            
        Returns:
            æŸ¥è¯¢ç»“æœ
        """
        if not self.is_connected or not self.connection_pool:
            await self.connect()
        
        conn = None
        try:
            self.log_debug("æ‰§è¡ŒPostgreSQLæŸ¥è¯¢", query=query[:200])
            
            # ä»è¿æ¥æ± è·å–è¿æ¥
            conn = self.connection_pool.getconn()
            
            with conn.cursor() as cursor:
                # æ‰§è¡ŒæŸ¥è¯¢
                if params:
                    cursor.execute(query, params)
                else:
                    cursor.execute(query)
                
                # è·å–ç»“æœ
                if cursor.description:
                    # æœ‰ç»“æœé›†çš„æŸ¥è¯¢
                    rows = cursor.fetchall()
                    # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                    result = [dict(row) for row in rows]
                else:
                    # æ²¡æœ‰ç»“æœé›†çš„æŸ¥è¯¢ï¼ˆå¦‚INSERT, UPDATE, DELETEï¼‰
                    result = []
                
            # æäº¤äº‹åŠ¡
            conn.commit()
            
            self.log_debug(
                "PostgreSQLæŸ¥è¯¢æ‰§è¡Œå®Œæˆ",
                row_count=len(result)
            )
            
            return result
            
        except Exception as e:
            if conn:
                conn.rollback()
            
            self.log_error("PostgreSQLæŸ¥è¯¢æ‰§è¡Œå¤±è´¥", error=str(e), query=query[:200])
            raise ConnectionError(f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}")
            
        finally:
            if conn and self.connection_pool:
                self.connection_pool.putconn(conn)
    
    async def close(self) -> None:
        """å…³é—­PostgreSQLè¿æ¥"""
        try:
            if self.connection_pool:
                self.connection_pool.closeall()
                self.connection_pool = None
            
            self.is_connected = False
            self.log_info("PostgreSQLè¿æ¥å·²å…³é—­")
            
        except Exception as e:
            self.log_error("å…³é—­PostgreSQLè¿æ¥å¤±è´¥", error=str(e))
    
    async def get_table_schema(self, table_name: str) -> Dict[str, Any]:
        """
        è·å–PostgreSQLè¡¨ç»“æ„ä¿¡æ¯
        
        Args:
            table_name: è¡¨å
            
        Returns:
            è¡¨ç»“æ„ä¿¡æ¯
        """
        try:
            query = """
            SELECT 
                column_name,
                data_type,
                is_nullable,
                column_default
            FROM information_schema.columns 
            WHERE table_name = %s
            ORDER BY ordinal_position
            """
            
            result = await self.execute_query(query, {"table_name": table_name})
            
            columns = []
            for row in result:
                columns.append({
                    "name": row["column_name"],
                    "type": row["data_type"],
                    "nullable": row["is_nullable"] == "YES",
                    "default": row["column_default"]
                })
            
            return {
                "table_name": table_name,
                "columns": columns
            }
            
        except Exception as e:
            self.log_error(f"è·å–PostgreSQLè¡¨ {table_name} ç»“æ„å¤±è´¥", error=str(e))
            raise ConnectionError(f"è·å–è¡¨ç»“æ„å¤±è´¥: {e}")
    
    async def get_available_tables(self) -> List[str]:
        """
        è·å–PostgreSQLå¯ç”¨è¡¨åˆ—è¡¨
        
        Returns:
            è¡¨ååˆ—è¡¨
        """
        try:
            query = """
            SELECT table_name 
            FROM information_schema.tables 
            WHERE table_schema = 'public' 
            AND table_type = 'BASE TABLE'
            ORDER BY table_name
            """
            
            result = await self.execute_query(query)
            tables = [row["table_name"] for row in result]
            
            self.log_info(f"è·å–åˆ° {len(tables)} ä¸ªPostgreSQLè¡¨")
            return tables
            
        except Exception as e:
            self.log_error("è·å–PostgreSQLè¡¨åˆ—è¡¨å¤±è´¥", error=str(e))
            raise ConnectionError(f"è·å–è¡¨åˆ—è¡¨å¤±è´¥: {e}")
    
    def _optimize_query_for_postgres(self, query: str) -> str:
        """
        ä¸ºPostgreSQLä¼˜åŒ–æŸ¥è¯¢
        
        Args:
            query: åŸå§‹æŸ¥è¯¢
            
        Returns:
            ä¼˜åŒ–åçš„æŸ¥è¯¢
        """
        # è¿™é‡Œå¯ä»¥æ·»åŠ PostgreSQLç‰¹å®šçš„æŸ¥è¯¢ä¼˜åŒ–é€»è¾‘
        # æ¯”å¦‚ï¼š
        # - ä½¿ç”¨EXPLAINåˆ†ææŸ¥è¯¢è®¡åˆ’
        # - æ·»åŠ åˆé€‚çš„ç´¢å¼•æç¤º
        # - ä¼˜åŒ–JOINé¡ºåº
        
        return query
    
    def _handle_postgres_error(self, error: Exception) -> None:
        """
        å¤„ç†PostgreSQLç‰¹å®šé”™è¯¯
        
        Args:
            error: é”™è¯¯ä¿¡æ¯
        """
        error_msg = str(error)
        
        # è¿æ¥é”™è¯¯
        if "connection" in error_msg.lower():
            self.is_connected = False
            self.log_error("PostgreSQLè¿æ¥æ–­å¼€")
        
        # è¯­æ³•é”™è¯¯
        elif "syntax error" in error_msg.lower():
            self.log_error("PostgreSQL SQLè¯­æ³•é”™è¯¯", error=error_msg)
        
        # æƒé™é”™è¯¯
        elif "permission denied" in error_msg.lower():
            self.log_error("PostgreSQLæƒé™ä¸è¶³", error=error_msg)
        
        # å…¶ä»–é”™è¯¯
        else:
            self.log_error("PostgreSQLæœªçŸ¥é”™è¯¯", error=error_msg)
    
    def _create_connection_string(self) -> str:
        """
        åˆ›å»ºPostgreSQLè¿æ¥å­—ç¬¦ä¸²
        
        Returns:
            è¿æ¥å­—ç¬¦ä¸²
        """
        config = self.connection_config
        return (
            f"host={config['host']} "
            f"port={config['port']} "
            f"dbname={config['database']} "
            f"user={config['user']} "
            f"password={config['password']}"
        )

================
File: uqm-backend/src/connectors/sqlite.py
================
"""
SQLiteæ•°æ®åº“è¿æ¥å™¨
å®ç°SQLiteæ•°æ®åº“çš„è¿æ¥å’ŒæŸ¥è¯¢åŠŸèƒ½
"""

import sqlite3
import asyncio
from pathlib import Path
from typing import Any, Dict, List, Optional
from urllib.parse import urlparse

from src.connectors.base import BaseConnector
from src.utils.exceptions import ConnectionError


class SQLiteConnector(BaseConnector):
    """SQLiteè¿æ¥å™¨å®ç°"""
    
    def __init__(self, connection_url: str):
        """
        åˆå§‹åŒ–SQLiteè¿æ¥å™¨
        
        Args:
            connection_url: SQLiteè¿æ¥URL (å¦‚: sqlite:///path/to/db.sqlite)
        """
        # è§£æè¿æ¥URL
        parsed_url = urlparse(connection_url)
        database_path = parsed_url.path
        
        # å¤„ç†ç›¸å¯¹è·¯å¾„
        if database_path.startswith('/'):
            database_path = database_path[1:]  # ç§»é™¤å¼€å¤´çš„ /
        
        connection_config = {
            "database_path": database_path,
            "connection_url": connection_url
        }
        
        super().__init__(connection_config)
        self.connection: Optional[sqlite3.Connection] = None
    
    async def connect(self) -> None:
        """å»ºç«‹SQLiteè¿æ¥"""
        try:
            database_path = self.connection_config["database_path"]
            self.log_info("æ­£åœ¨è¿æ¥SQLiteæ•°æ®åº“", path=database_path)
            
            # ç¡®ä¿æ•°æ®åº“ç›®å½•å­˜åœ¨
            db_file = Path(database_path)
            db_file.parent.mkdir(parents=True, exist_ok=True)
            
            # åˆ›å»ºSQLiteè¿æ¥
            self.connection = sqlite3.connect(
                database_path,
                check_same_thread=False,
                timeout=30.0
            )
            
            # è®¾ç½®è¡Œå·¥å‚ï¼Œè¿”å›å­—å…¸æ ¼å¼
            self.connection.row_factory = sqlite3.Row
            
            # å¯ç”¨å¤–é”®çº¦æŸ
            self.connection.execute("PRAGMA foreign_keys = ON")
            
            # è®¾ç½®WALæ¨¡å¼ä»¥æé«˜å¹¶å‘æ€§èƒ½
            self.connection.execute("PRAGMA journal_mode = WAL")
            
            # æµ‹è¯•è¿æ¥
            cursor = self.connection.cursor()
            cursor.execute("SELECT sqlite_version()")
            version = cursor.fetchone()
            self.log_info("SQLiteè¿æ¥æˆåŠŸ", version=version[0])
            
            self.is_connected = True
            
        except Exception as e:
            self.log_error("SQLiteè¿æ¥å¤±è´¥", error=str(e))
            self._handle_connection_error(e)
    
    async def execute_query(self, query: str, params: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        æ‰§è¡ŒSQLiteæŸ¥è¯¢
        
        Args:
            query: SQLæŸ¥è¯¢è¯­å¥
            params: æŸ¥è¯¢å‚æ•°
            
        Returns:
            æŸ¥è¯¢ç»“æœ
        """
        if not self.is_connected or not self.connection:
            await self.connect()
        
        try:
            self.log_debug("æ‰§è¡ŒSQLiteæŸ¥è¯¢", query=query[:200])
            
            cursor = self.connection.cursor()
            
            # æ‰§è¡ŒæŸ¥è¯¢
            if params:
                # å¤„ç†å‚æ•°æ ¼å¼
                if isinstance(params, dict):
                    # å‘½åå‚æ•°
                    cursor.execute(query, params)
                else:
                    # ä½ç½®å‚æ•°
                    cursor.execute(query, params)
            else:
                cursor.execute(query)
            
            # è·å–ç»“æœ
            if cursor.description:
                # æœ‰ç»“æœé›†çš„æŸ¥è¯¢
                rows = cursor.fetchall()
                # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                result = [dict(row) for row in rows]
            else:
                # æ²¡æœ‰ç»“æœé›†çš„æŸ¥è¯¢
                result = []
            
            # æäº¤äº‹åŠ¡
            self.connection.commit()
            
            self.log_debug(
                "SQLiteæŸ¥è¯¢æ‰§è¡Œå®Œæˆ",
                row_count=len(result)
            )
            
            return result
            
        except Exception as e:
            if self.connection:
                self.connection.rollback()
            
            self.log_error("SQLiteæŸ¥è¯¢æ‰§è¡Œå¤±è´¥", error=str(e), query=query[:200])
            self._handle_sqlite_error(e)
            raise ConnectionError(f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}")
    
    async def close(self) -> None:
        """å…³é—­SQLiteè¿æ¥"""
        try:
            if self.connection:
                self.connection.close()
                self.connection = None
            
            self.is_connected = False
            self.log_info("SQLiteè¿æ¥å·²å…³é—­")
            
        except Exception as e:
            self.log_error("å…³é—­SQLiteè¿æ¥å¤±è´¥", error=str(e))
    
    async def get_table_schema(self, table_name: str) -> Dict[str, Any]:
        """
        è·å–SQLiteè¡¨ç»“æ„ä¿¡æ¯
        
        Args:
            table_name: è¡¨å
            
        Returns:
            è¡¨ç»“æ„ä¿¡æ¯
        """
        try:
            query = f"PRAGMA table_info({table_name})"
            result = await self.execute_query(query)
            
            columns = []
            for row in result:
                columns.append({
                    "name": row["name"],
                    "type": row["type"],
                    "nullable": row["notnull"] == 0,
                    "default": row["dflt_value"],
                    "primary_key": row["pk"] == 1
                })
            
            return {
                "table_name": table_name,
                "columns": columns
            }
            
        except Exception as e:
            self.log_error(f"è·å–SQLiteè¡¨ {table_name} ç»“æ„å¤±è´¥", error=str(e))
            raise ConnectionError(f"è·å–è¡¨ç»“æ„å¤±è´¥: {e}")
    
    async def get_available_tables(self) -> List[str]:
        """
        è·å–SQLiteå¯ç”¨è¡¨åˆ—è¡¨
        
        Returns:
            è¡¨ååˆ—è¡¨
        """
        try:
            query = """
            SELECT name 
            FROM sqlite_master 
            WHERE type='table' 
            AND name NOT LIKE 'sqlite_%'
            ORDER BY name
            """
            
            result = await self.execute_query(query)
            tables = [row["name"] for row in result]
            
            self.log_info(f"è·å–åˆ° {len(tables)} ä¸ªSQLiteè¡¨")
            return tables
            
        except Exception as e:
            self.log_error("è·å–SQLiteè¡¨åˆ—è¡¨å¤±è´¥", error=str(e))
            raise ConnectionError(f"è·å–è¡¨åˆ—è¡¨å¤±è´¥: {e}")
    
    async def get_indexes(self, table_name: str) -> List[Dict[str, Any]]:
        """
        è·å–è¡¨çš„ç´¢å¼•ä¿¡æ¯
        
        Args:
            table_name: è¡¨å
            
        Returns:
            ç´¢å¼•ä¿¡æ¯åˆ—è¡¨
        """
        try:
            query = f"PRAGMA index_list({table_name})"
            result = await self.execute_query(query)
            
            indexes = []
            for row in result:
                # è·å–ç´¢å¼•è¯¦ç»†ä¿¡æ¯
                index_name = row["name"]
                index_query = f"PRAGMA index_info({index_name})"
                index_info = await self.execute_query(index_query)
                
                columns = [info["name"] for info in index_info]
                
                indexes.append({
                    "name": index_name,
                    "unique": row["unique"] == 1,
                    "columns": columns
                })
            
            return indexes
            
        except Exception as e:
            self.log_error(f"è·å–SQLiteè¡¨ {table_name} ç´¢å¼•å¤±è´¥", error=str(e))
            return []
    
    def _optimize_query_for_sqlite(self, query: str) -> str:
        """
        ä¸ºSQLiteä¼˜åŒ–æŸ¥è¯¢
        
        Args:
            query: åŸå§‹æŸ¥è¯¢
            
        Returns:
            ä¼˜åŒ–åçš„æŸ¥è¯¢
        """
        # SQLiteç‰¹å®šçš„æŸ¥è¯¢ä¼˜åŒ–
        # æ¯”å¦‚ï¼š
        # - ä½¿ç”¨INDEXED BYæç¤º
        # - ä¼˜åŒ–å¤æ‚çš„JOINæŸ¥è¯¢
        # - ä½¿ç”¨SQLiteç‰¹å®šçš„å‡½æ•°
        
        return query
    
    def _handle_sqlite_error(self, error: Exception) -> None:
        """
        å¤„ç†SQLiteç‰¹å®šé”™è¯¯
        
        Args:
            error: é”™è¯¯ä¿¡æ¯
        """
        error_msg = str(error)
        
        # æ•°æ®åº“é”å®š
        if "database is locked" in error_msg.lower():
            self.log_error("SQLiteæ•°æ®åº“è¢«é”å®š")
        
        # è¯­æ³•é”™è¯¯
        elif "syntax error" in error_msg.lower():
            self.log_error("SQLite SQLè¯­æ³•é”™è¯¯", error=error_msg)
        
        # è¡¨ä¸å­˜åœ¨
        elif "no such table" in error_msg.lower():
            self.log_error("SQLiteè¡¨ä¸å­˜åœ¨", error=error_msg)
        
        # ç£ç›˜ç©ºé—´ä¸è¶³
        elif "disk" in error_msg.lower():
            self.log_error("SQLiteç£ç›˜ç©ºé—´ä¸è¶³", error=error_msg)
        
        # å…¶ä»–é”™è¯¯
        else:
            self.log_error("SQLiteæœªçŸ¥é”™è¯¯", error=error_msg)
    
    async def vacuum(self) -> None:
        """
        æ‰§è¡ŒVACUUMæ“ä½œä»¥ä¼˜åŒ–æ•°æ®åº“
        """
        try:
            self.log_info("å¼€å§‹æ‰§è¡ŒSQLite VACUUMæ“ä½œ")
            await self.execute_query("VACUUM")
            self.log_info("SQLite VACUUMæ“ä½œå®Œæˆ")
            
        except Exception as e:
            self.log_error("SQLite VACUUMæ“ä½œå¤±è´¥", error=str(e))
    
    async def analyze(self) -> None:
        """
        æ‰§è¡ŒANALYZEæ“ä½œä»¥æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            self.log_info("å¼€å§‹æ‰§è¡ŒSQLite ANALYZEæ“ä½œ")
            await self.execute_query("ANALYZE")
            self.log_info("SQLite ANALYZEæ“ä½œå®Œæˆ")
            
        except Exception as e:
            self.log_error("SQLite ANALYZEæ“ä½œå¤±è´¥", error=str(e))

================
File: uqm-backend/src/core/__init__.py
================
"""æ ¸å¿ƒæ¨¡å—åŒ…åˆå§‹åŒ–æ–‡ä»¶"""

================
File: uqm-backend/src/core/cache.py
================
"""
æŸ¥è¯¢ç»“æœç¼“å­˜ç®¡ç†æ¨¡å—
æ”¯æŒå†…å­˜ç¼“å­˜å’ŒRedisç¼“å­˜
"""

import json
import pickle
import time
from abc import ABC, abstractmethod
from typing import Any, Dict, Optional, Union
from datetime import datetime, timedelta
from functools import lru_cache

import redis
from src.config.settings import get_settings
from src.utils.logging import LoggerMixin
from src.utils.exceptions import CacheError


class BaseCacheManager(ABC, LoggerMixin):
    """ç¼“å­˜ç®¡ç†å™¨åŸºç±»"""
    
    @abstractmethod
    async def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜æ•°æ®"""
        pass
    
    @abstractmethod
    async def set(self, key: str, value: Any, ttl: int = None) -> bool:
        """è®¾ç½®ç¼“å­˜æ•°æ®"""
        pass
    
    @abstractmethod
    async def delete(self, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜æ•°æ®"""
        pass
    
    @abstractmethod
    async def exists(self, key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨"""
        pass
    
    @abstractmethod
    async def clear(self) -> bool:
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        pass
    
    @abstractmethod
    async def stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        pass


class MemoryCacheManager(BaseCacheManager):
    """å†…å­˜ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, max_size: int = 1000, default_ttl: int = 3600):
        """
        åˆå§‹åŒ–å†…å­˜ç¼“å­˜ç®¡ç†å™¨
        
        Args:
            max_size: æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
            default_ttl: é»˜è®¤TTL(ç§’)
        """
        self.max_size = max_size
        self.default_ttl = default_ttl
        self.cache: Dict[str, Dict[str, Any]] = {}
        self.access_times: Dict[str, float] = {}
        self.stats_data = {
            "hits": 0,
            "misses": 0,
            "sets": 0,
            "deletes": 0,
            "evictions": 0
        }
    
    async def initialize(self) -> None:
        """åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨"""
        self.log_info("å†…å­˜ç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ", max_size=self.max_size)
    
    async def close(self) -> None:
        """å…³é—­ç¼“å­˜ç®¡ç†å™¨"""
        await self.clear()
        self.log_info("å†…å­˜ç¼“å­˜ç®¡ç†å™¨å·²å…³é—­")
    
    async def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜æ•°æ®"""
        try:
            if key not in self.cache:
                self.stats_data["misses"] += 1
                return None
            
            cache_item = self.cache[key]
            
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            if self._is_expired(cache_item):
                await self.delete(key)
                self.stats_data["misses"] += 1
                return None
            
            # æ›´æ–°è®¿é—®æ—¶é—´
            self.access_times[key] = time.time()
            self.stats_data["hits"] += 1
            
            # ååºåˆ—åŒ–æ•°æ®
            return self._deserialize_data(cache_item["data"])
            
        except Exception as e:
            self.log_error("è·å–ç¼“å­˜æ•°æ®å¤±è´¥", key=key, error=str(e))
            raise CacheError(f"è·å–ç¼“å­˜æ•°æ®å¤±è´¥: {e}")
    
    async def set(self, key: str, value: Any, ttl: int = None) -> bool:
        """è®¾ç½®ç¼“å­˜æ•°æ®"""
        try:
            # å¦‚æœç¼“å­˜å·²æ»¡ï¼Œæ¸…ç†è¿‡æœŸé¡¹æˆ–ç§»é™¤æœ€ä¹…æœªè®¿é—®çš„é¡¹
            if len(self.cache) >= self.max_size and key not in self.cache:
                await self._cleanup_expired()
                
                if len(self.cache) >= self.max_size:
                    await self._remove_oldest()
            
            # åºåˆ—åŒ–æ•°æ®
            serialized_data = self._serialize_data(value)
            
            # è®¾ç½®ç¼“å­˜é¡¹
            expire_time = time.time() + (ttl or self.default_ttl)
            self.cache[key] = {
                "data": serialized_data,
                "expire_time": expire_time,
                "created_time": time.time()
            }
            
            self.access_times[key] = time.time()
            self.stats_data["sets"] += 1
            
            return True
            
        except Exception as e:
            self.log_error("è®¾ç½®ç¼“å­˜æ•°æ®å¤±è´¥", key=key, error=str(e))
            raise CacheError(f"è®¾ç½®ç¼“å­˜æ•°æ®å¤±è´¥: {e}")
    
    async def delete(self, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜æ•°æ®"""
        try:
            if key in self.cache:
                del self.cache[key]
                if key in self.access_times:
                    del self.access_times[key]
                self.stats_data["deletes"] += 1
                return True
            return False
            
        except Exception as e:
            self.log_error("åˆ é™¤ç¼“å­˜æ•°æ®å¤±è´¥", key=key, error=str(e))
            raise CacheError(f"åˆ é™¤ç¼“å­˜æ•°æ®å¤±è´¥: {e}")
    
    async def exists(self, key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨"""
        if key not in self.cache:
            return False
        
        cache_item = self.cache[key]
        if self._is_expired(cache_item):
            await self.delete(key)
            return False
        
        return True
    
    async def clear(self) -> bool:
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        try:
            self.cache.clear()
            self.access_times.clear()
            self.log_info("å†…å­˜ç¼“å­˜å·²æ¸…ç©º")
            return True
            
        except Exception as e:
            self.log_error("æ¸…ç©ºç¼“å­˜å¤±è´¥", error=str(e))
            raise CacheError(f"æ¸…ç©ºç¼“å­˜å¤±è´¥: {e}")
    
    async def stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_requests = self.stats_data["hits"] + self.stats_data["misses"]
        hit_rate = self.stats_data["hits"] / total_requests if total_requests > 0 else 0
        
        return {
            "type": "memory",
            "total_items": len(self.cache),
            "max_size": self.max_size,
            "hit_rate": hit_rate,
            **self.stats_data
        }
    
    def _is_expired(self, cache_item: Dict[str, Any]) -> bool:
        """æ£€æŸ¥ç¼“å­˜é¡¹æ˜¯å¦è¿‡æœŸ"""
        return time.time() > cache_item["expire_time"]
    
    async def _cleanup_expired(self) -> None:
        """æ¸…ç†è¿‡æœŸçš„ç¼“å­˜é¡¹"""
        expired_keys = []
        current_time = time.time()
        
        for key, cache_item in self.cache.items():
            if current_time > cache_item["expire_time"]:
                expired_keys.append(key)
        
        for key in expired_keys:
            await self.delete(key)
        
        if expired_keys:
            self.log_info("æ¸…ç†è¿‡æœŸç¼“å­˜é¡¹", count=len(expired_keys))
    
    async def _remove_oldest(self) -> None:
        """ç§»é™¤æœ€ä¹…æœªè®¿é—®çš„ç¼“å­˜é¡¹"""
        if not self.access_times:
            return
        
        # æ‰¾åˆ°æœ€ä¹…æœªè®¿é—®çš„é”®
        oldest_key = min(self.access_times.keys(), key=lambda k: self.access_times[k])
        await self.delete(oldest_key)
        self.stats_data["evictions"] += 1
        
        self.log_info("ç§»é™¤æœ€ä¹…æœªè®¿é—®çš„ç¼“å­˜é¡¹", key=oldest_key)
    
    def _serialize_data(self, data: Any) -> bytes:
        """åºåˆ—åŒ–ç¼“å­˜æ•°æ®"""
        try:
            return pickle.dumps(data)
        except Exception as e:
            self.log_error("åºåˆ—åŒ–æ•°æ®å¤±è´¥", error=str(e))
            raise CacheError(f"åºåˆ—åŒ–æ•°æ®å¤±è´¥: {e}")
    
    def _deserialize_data(self, data: bytes) -> Any:
        """ååºåˆ—åŒ–ç¼“å­˜æ•°æ®"""
        try:
            return pickle.loads(data)
        except Exception as e:
            self.log_error("ååºåˆ—åŒ–æ•°æ®å¤±è´¥", error=str(e))
            raise CacheError(f"ååºåˆ—åŒ–æ•°æ®å¤±è´¥: {e}")


class RedisCacheManager(BaseCacheManager):
    """Redisç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, redis_url: str, default_ttl: int = 3600):
        """
        åˆå§‹åŒ–Redisç¼“å­˜ç®¡ç†å™¨
        
        Args:
            redis_url: Redisè¿æ¥URL
            default_ttl: é»˜è®¤TTL(ç§’)
        """
        self.redis_url = redis_url
        self.default_ttl = default_ttl
        self.redis_client: Optional[redis.Redis] = None
        self.stats_data = {
            "hits": 0,
            "misses": 0,
            "sets": 0,
            "deletes": 0
        }
    
    async def initialize(self) -> None:
        """åˆå§‹åŒ–Redisè¿æ¥"""
        try:
            self.redis_client = redis.from_url(
                self.redis_url,
                decode_responses=False,
                socket_timeout=5,
                socket_connect_timeout=5,
                retry_on_timeout=True
            )
            
            # æµ‹è¯•è¿æ¥
            await self._ping()
            
            self.log_info("Redisç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ", redis_url=self.redis_url)
            
        except Exception as e:
            self.log_error("Redisç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥", error=str(e))
            raise CacheError(f"Redisç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    
    async def close(self) -> None:
        """å…³é—­Redisè¿æ¥"""
        if self.redis_client:
            await self.redis_client.close()
            self.log_info("Redisç¼“å­˜ç®¡ç†å™¨å·²å…³é—­")
    
    async def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜æ•°æ®"""
        try:
            if not self.redis_client:
                raise CacheError("Rediså®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            
            data = self.redis_client.get(key)
            
            if data is None:
                self.stats_data["misses"] += 1
                return None
            
            self.stats_data["hits"] += 1
            return self._deserialize_data(data)
            
        except Exception as e:
            self.log_error("è·å–Redisç¼“å­˜æ•°æ®å¤±è´¥", key=key, error=str(e))
            raise CacheError(f"è·å–Redisç¼“å­˜æ•°æ®å¤±è´¥: {e}")
    
    async def set(self, key: str, value: Any, ttl: int = None) -> bool:
        """è®¾ç½®ç¼“å­˜æ•°æ®"""
        try:
            if not self.redis_client:
                raise CacheError("Rediså®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            
            serialized_data = self._serialize_data(value)
            
            success = self.redis_client.setex(
                key, 
                ttl or self.default_ttl, 
                serialized_data
            )
            
            if success:
                self.stats_data["sets"] += 1
            
            return bool(success)
            
        except Exception as e:
            self.log_error("è®¾ç½®Redisç¼“å­˜æ•°æ®å¤±è´¥", key=key, error=str(e))
            raise CacheError(f"è®¾ç½®Redisç¼“å­˜æ•°æ®å¤±è´¥: {e}")
    
    async def delete(self, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜æ•°æ®"""
        try:
            if not self.redis_client:
                raise CacheError("Rediså®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            
            result = self.redis_client.delete(key)
            
            if result > 0:
                self.stats_data["deletes"] += 1
                return True
            
            return False
            
        except Exception as e:
            self.log_error("åˆ é™¤Redisç¼“å­˜æ•°æ®å¤±è´¥", key=key, error=str(e))
            raise CacheError(f"åˆ é™¤Redisç¼“å­˜æ•°æ®å¤±è´¥: {e}")
    
    async def exists(self, key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨"""
        try:
            if not self.redis_client:
                raise CacheError("Rediså®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            
            return bool(self.redis_client.exists(key))
            
        except Exception as e:
            self.log_error("æ£€æŸ¥Redisç¼“å­˜å­˜åœ¨æ€§å¤±è´¥", key=key, error=str(e))
            raise CacheError(f"æ£€æŸ¥Redisç¼“å­˜å­˜åœ¨æ€§å¤±è´¥: {e}")
    
    async def clear(self) -> bool:
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        try:
            if not self.redis_client:
                raise CacheError("Rediså®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            
            # æ³¨æ„ï¼šè¿™å°†æ¸…ç©ºæ•´ä¸ªRedisæ•°æ®åº“ï¼Œç”Ÿäº§ç¯å¢ƒéœ€è¦è°¨æ…ä½¿ç”¨
            self.redis_client.flushdb()
            self.log_info("Redisç¼“å­˜å·²æ¸…ç©º")
            return True
            
        except Exception as e:
            self.log_error("æ¸…ç©ºRedisç¼“å­˜å¤±è´¥", error=str(e))
            raise CacheError(f"æ¸…ç©ºRedisç¼“å­˜å¤±è´¥: {e}")
    
    async def stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        try:
            if not self.redis_client:
                raise CacheError("Rediså®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            
            info = self.redis_client.info()
            total_requests = self.stats_data["hits"] + self.stats_data["misses"]
            hit_rate = self.stats_data["hits"] / total_requests if total_requests > 0 else 0
            
            return {
                "type": "redis",
                "connected_clients": info.get("connected_clients", 0),
                "used_memory": info.get("used_memory", 0),
                "used_memory_human": info.get("used_memory_human", "0B"),
                "hit_rate": hit_rate,
                **self.stats_data
            }
            
        except Exception as e:
            self.log_error("è·å–Redisç»Ÿè®¡ä¿¡æ¯å¤±è´¥", error=str(e))
            return {
                "type": "redis",
                "error": str(e),
                **self.stats_data
            }
    
    async def _ping(self) -> None:
        """æµ‹è¯•Redisè¿æ¥"""
        if not self.redis_client:
            raise CacheError("Rediså®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
        
        result = self.redis_client.ping()
        if not result:
            raise CacheError("Redisè¿æ¥æµ‹è¯•å¤±è´¥")
    
    def _serialize_data(self, data: Any) -> bytes:
        """åºåˆ—åŒ–ç¼“å­˜æ•°æ®"""
        try:
            return pickle.dumps(data)
        except Exception as e:
            self.log_error("åºåˆ—åŒ–æ•°æ®å¤±è´¥", error=str(e))
            raise CacheError(f"åºåˆ—åŒ–æ•°æ®å¤±è´¥: {e}")
    
    def _deserialize_data(self, data: bytes) -> Any:
        """ååºåˆ—åŒ–ç¼“å­˜æ•°æ®"""
        try:
            return pickle.loads(data)
        except Exception as e:
            self.log_error("ååºåˆ—åŒ–æ•°æ®å¤±è´¥", error=str(e))
            raise CacheError(f"ååºåˆ—åŒ–æ•°æ®å¤±è´¥: {e}")


# å…¨å±€ç¼“å­˜ç®¡ç†å™¨å®ä¾‹
_cache_manager: Optional[BaseCacheManager] = None


@lru_cache()
def get_cache_manager() -> BaseCacheManager:
    """è·å–ç¼“å­˜ç®¡ç†å™¨å®ä¾‹(å•ä¾‹æ¨¡å¼)"""
    global _cache_manager
    
    if _cache_manager is None:
        settings = get_settings()
        cache_config = settings.get_cache_config()
        
        if cache_config["type"].lower() == "redis":
            _cache_manager = RedisCacheManager(
                redis_url=cache_config["redis_url"],
                default_ttl=cache_config["default_timeout"]
            )
        else:
            _cache_manager = MemoryCacheManager(
                max_size=cache_config["max_size"],
                default_ttl=cache_config["default_timeout"]
            )
    
    return _cache_manager

================
File: uqm-backend/src/core/engine.py
================
"""
UQMæ‰§è¡Œå¼•æ“ä¸»ç±»
è´Ÿè´£åè°ƒæ•´ä¸ªæŸ¥è¯¢æ‰§è¡Œæµç¨‹
"""

import time
import hashlib
from typing import Any, Dict, List, Optional
from functools import lru_cache

from src.api.models import UQMResponse, StepResult, Metadata, StepType
from src.core.parser import UQMParser
from src.core.executor import Executor
from src.core.cache import get_cache_manager
from src.connectors.base import get_connector_manager
from src.utils.logging import LoggerMixin
from src.utils.exceptions import ValidationError, ExecutionError
from src.config.settings import get_settings


class UQMEngine(LoggerMixin):
    """UQMæ‰§è¡Œå¼•æ“ä¸»ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–UQMæ‰§è¡Œå¼•æ“"""
        self.parser = UQMParser()
        self.cache_manager = get_cache_manager()
        self.connector_manager = get_connector_manager()
        self.settings = get_settings()
    
    async def process(self, uqm_data: Dict[str, Any], 
                     parameters: Optional[Dict[str, Any]] = None,
                     options: Optional[Dict[str, Any]] = None) -> UQMResponse:
        """
        å¤„ç†UQMæŸ¥è¯¢çš„ä¸»å…¥å£æ–¹æ³•
        
        Args:
            uqm_data: UQM JSONæ•°æ®
            parameters: æŸ¥è¯¢å‚æ•°
            options: æ‰§è¡Œé€‰é¡¹
            
        Returns:
            æŸ¥è¯¢æ‰§è¡Œç»“æœ
            
        Raises:
            ValidationError: éªŒè¯å¤±è´¥
            ExecutionError: æ‰§è¡Œå¤±è´¥
        """
        start_time = time.time()
        
        try:
            self.log_info(
                "å¼€å§‹å¤„ç†UQMæŸ¥è¯¢",
                uqm_name=uqm_data.get("metadata", {}).get("name", "æœªå‘½å")
            )
            
            # å‚æ•°é¢„å¤„ç†
            parameters = parameters or {}
            options = options or {}
            
            # è§£æUQMæ•°æ®
            parsed_data = self.parser.parse(uqm_data)
            
            # å‚æ•°æ›¿æ¢
            processed_data = self._substitute_parameters(parsed_data, parameters)
            
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = self._generate_cache_key(processed_data, parameters)
            
            # æ£€æŸ¥ç¼“å­˜
            cached_result = None
            if options.get("cache_enabled", True):
                cached_result = await self.cache_manager.get(cache_key)
                if cached_result:
                    self.log_info("å‘½ä¸­ç¼“å­˜", cache_key=cache_key)
                    return cached_result
            
            # åˆ›å»ºæ‰§è¡Œå™¨å¹¶æ‰§è¡Œ
            executor = Executor(
                steps=processed_data["steps"],
                connector_manager=self.connector_manager,
                cache_manager=self.cache_manager,
                options=options
            )
            
            execution_result = await executor.execute()
            
            # è·å–è¾“å‡ºæ­¥éª¤çš„ç»“æœ
            output_step = processed_data["output"]
            output_data = execution_result.get_step_data(output_step)
            
            # æ„å»ºå“åº”
            execution_time = time.time() - start_time
            response = UQMResponse(
                success=True,
                data=output_data,
                metadata=Metadata(**processed_data["metadata"]),
                execution_info={
                    "total_time": execution_time,
                    "row_count": len(output_data) if output_data else 0,
                    "cache_hit": False,
                    "steps_executed": len(processed_data["steps"])
                },
                step_results=self._build_step_results(execution_result.step_results)
            )
            
            # ç¼“å­˜ç»“æœ
            if options.get("cache_enabled", True):
                cache_ttl = options.get("cache_ttl", self.settings.CACHE_DEFAULT_TIMEOUT)
                await self.cache_manager.set(cache_key, response, cache_ttl)
            
            self.log_info(
                "UQMæŸ¥è¯¢å¤„ç†å®Œæˆ",
                execution_time=execution_time,
                row_count=len(output_data) if output_data else 0
            )
            
            return response
            
        except ValidationError as e:
            self.log_error("UQMæŸ¥è¯¢éªŒè¯å¤±è´¥", error=str(e))
            raise
            
        except ExecutionError as e:
            self.log_error("UQMæŸ¥è¯¢æ‰§è¡Œå¤±è´¥", error=str(e))
            raise
            
        except Exception as e:
            execution_time = time.time() - start_time
            self.log_error(
                "UQMæŸ¥è¯¢å¤„ç†å‡ºç°æœªçŸ¥é”™è¯¯",
                error=str(e),
                execution_time=execution_time,
                exc_info=True
            )
            raise ExecutionError(f"æŸ¥è¯¢å¤„ç†å¤±è´¥: {e}")
    
    async def validate_query(self, uqm_data: Dict[str, Any]) -> Any:
        """
        éªŒè¯UQMæŸ¥è¯¢æœ‰æ•ˆæ€§
        
        Args:
            uqm_data: UQM JSONæ•°æ®
            
        Returns:
            éªŒè¯ç»“æœ
        """
        try:
            self.log_info("å¼€å§‹éªŒè¯UQMæŸ¥è¯¢")
            
            # ä½¿ç”¨è§£æå™¨è¿›è¡ŒéªŒè¯
            validation_result = self.parser.validate_schema(uqm_data)
            
            self.log_info(
                "UQMæŸ¥è¯¢éªŒè¯å®Œæˆ",
                valid=validation_result.valid,
                error_count=len(validation_result.errors) if validation_result.errors else 0
            )
            
            return validation_result
            
        except Exception as e:
            self.log_error("UQMæŸ¥è¯¢éªŒè¯å‡ºç°é”™è¯¯", error=str(e))
            raise ValidationError(f"æŸ¥è¯¢éªŒè¯å¤±è´¥: {e}")
    
    def _substitute_parameters(self, uqm_data: Dict[str, Any], 
                             parameters: Dict[str, Any]) -> Dict[str, Any]:
        """
        å‚æ•°æ›¿æ¢å¤„ç†
        
        Args:
            uqm_data: è§£æåçš„UQMæ•°æ®
            parameters: å‚æ•°å€¼å­—å…¸
            
        Returns:
            å‚æ•°æ›¿æ¢åçš„UQMæ•°æ®
        """
        try:
            self.log_info("å¼€å§‹å‚æ•°æ›¿æ¢", parameter_count=len(parameters))
            
            # æ·±æ‹·è´æ•°æ®ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
            import copy
            processed_data = copy.deepcopy(uqm_data)
            
            # å°†æ•°æ®è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²è¿›è¡Œå‚æ•°æ›¿æ¢
            import json
            data_str = json.dumps(processed_data)
            
            # æ›¿æ¢å‚æ•°
            for param_name, param_value in parameters.items():
                placeholder = f"${param_name}"
                # å¦‚æœå‚æ•°å€¼æ˜¯å­—ç¬¦ä¸²ï¼Œéœ€è¦æ·»åŠ å¼•å·
                if isinstance(param_value, str):
                    replacement = json.dumps(param_value)
                else:
                    replacement = json.dumps(param_value)
                
                data_str = data_str.replace(f'"{placeholder}"', replacement)
                data_str = data_str.replace(placeholder, replacement)
            
            # è½¬æ¢å›å­—å…¸
            processed_data = json.loads(data_str)
            
            self.log_info("å‚æ•°æ›¿æ¢å®Œæˆ")
            return processed_data
            
        except Exception as e:
            self.log_error("å‚æ•°æ›¿æ¢å¤±è´¥", error=str(e))
            raise ValidationError(f"å‚æ•°æ›¿æ¢å¤±è´¥: {e}")
    
    def _generate_cache_key(self, uqm_data: Dict[str, Any], 
                           parameters: Dict[str, Any]) -> str:
        """
        ç”Ÿæˆç¼“å­˜é”®
        
        Args:
            uqm_data: UQMæ•°æ®
            parameters: å‚æ•°
            
        Returns:
            ç¼“å­˜é”®
        """
        try:
            # åˆ›å»ºåŒ…å«UQMæ•°æ®å’Œå‚æ•°çš„å­—å…¸
            cache_data = {
                "uqm": uqm_data,
                "parameters": parameters
            }
            
            # åºåˆ—åŒ–å¹¶ç”Ÿæˆhash
            import json
            data_str = json.dumps(cache_data, sort_keys=True)
            cache_key = hashlib.md5(data_str.encode('utf-8')).hexdigest()
            
            return f"uqm_cache:{cache_key}"
            
        except Exception as e:
            self.log_error("ç”Ÿæˆç¼“å­˜é”®å¤±è´¥", error=str(e))
            # å¦‚æœç”Ÿæˆç¼“å­˜é”®å¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªåŸºäºæ—¶é—´çš„é”®ï¼ˆä¸ä¼šå‘½ä¸­ç¼“å­˜ï¼‰
            return f"uqm_cache:no_cache_{int(time.time())}"
    
    def _build_step_results(self, step_results: Dict[str, Any]) -> List[StepResult]:
        """
        æ„å»ºæ­¥éª¤ç»“æœåˆ—è¡¨
        
        Args:
            step_results: æ‰§è¡Œå™¨è¿”å›çš„æ­¥éª¤ç»“æœ
            
        Returns:
            æ­¥éª¤ç»“æœåˆ—è¡¨
        """
        results = []
        
        for step_name, step_data in step_results.items():
            result = StepResult(
                step_name=step_name,
                step_type=StepType(step_data.get("type", "query")),
                status=step_data.get("status", "completed"),
                data=step_data.get("data"),
                row_count=step_data.get("row_count", 0),
                execution_time=step_data.get("execution_time", 0.0),
                cache_hit=step_data.get("cache_hit", False),
                error=step_data.get("error")
            )
            results.append(result)
        
        return results


# å…¨å±€å¼•æ“å®ä¾‹
_uqm_engine: Optional[UQMEngine] = None


@lru_cache()
def get_uqm_engine() -> UQMEngine:
    """è·å–UQMå¼•æ“å®ä¾‹(å•ä¾‹æ¨¡å¼)"""
    global _uqm_engine
    
    if _uqm_engine is None:
        _uqm_engine = UQMEngine()
    
    return _uqm_engine

================
File: uqm-backend/src/core/executor.py
================
"""
æ­¥éª¤æ‰§è¡Œå™¨æ¨¡å—
è´Ÿè´£æŒ‰é¡ºåºæ‰§è¡ŒUQMå®šä¹‰çš„å„ä¸ªæ­¥éª¤
"""

import time
import hashlib
from typing import Any, Dict, List, Optional, Union
from dataclasses import dataclass

from src.steps.query_step import QueryStep
from src.steps.enrich_step import EnrichStep
from src.steps.pivot_step import PivotStep
from src.steps.unpivot_step import UnpivotStep
from src.steps.union_step import UnionStep
from src.steps.assert_step import AssertStep
from src.core.cache import BaseCacheManager
from src.connectors.base import BaseConnectorManager
from src.utils.logging import LoggerMixin
from src.utils.exceptions import ExecutionError


@dataclass
class ExecutionResult:
    """æ‰§è¡Œç»“æœæ•°æ®ç±»"""
    step_results: Dict[str, Any]
    step_data: Dict[str, List[Dict[str, Any]]]
    
    def get_step_data(self, step_name: str) -> Optional[List[Dict[str, Any]]]:
        """è·å–æŒ‡å®šæ­¥éª¤çš„æ•°æ®"""
        return self.step_data.get(step_name)


class Executor(LoggerMixin):
    """æ­¥éª¤æ‰§è¡Œç®¡ç†å™¨"""
    
    def __init__(self, steps: List[Dict[str, Any]], 
                 connector_manager: BaseConnectorManager,
                 cache_manager: BaseCacheManager,
                 options: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–æ‰§è¡Œå™¨
        
        Args:
            steps: æ­¥éª¤åˆ—è¡¨
            connector_manager: è¿æ¥å™¨ç®¡ç†å™¨
            cache_manager: ç¼“å­˜ç®¡ç†å™¨
            options: æ‰§è¡Œé€‰é¡¹
        """
        self.steps = steps
        self.connector_manager = connector_manager
        self.cache_manager = cache_manager
        self.options = options or {}
        
        # æ­¥éª¤æ‰§è¡Œç»“æœå­˜å‚¨
        self.step_results: Dict[str, Any] = {}
        self.step_data: Dict[str, List[Dict[str, Any]]] = {}
        
        # æ­¥éª¤ç±»å‹æ˜ å°„
        self.step_classes = {
            "query": QueryStep,
            "enrich": EnrichStep,
            "pivot": PivotStep,
            "unpivot": UnpivotStep,
            "union": UnionStep,
            "assert": AssertStep
        }
    
    async def execute(self) -> ExecutionResult:
        """
        æ‰§è¡Œæ‰€æœ‰æ­¥éª¤
        
        Returns:
            æ‰§è¡Œç»“æœ
            
        Raises:
            ExecutionError: æ‰§è¡Œå¤±è´¥
        """
        try:
            self.log_info("å¼€å§‹æ‰§è¡Œæ­¥éª¤", step_count=len(self.steps))
            
            for step_config in self.steps:
                step_name = step_config["name"]
                
                try:
                    # æ‰§è¡Œå•ä¸ªæ­¥éª¤
                    await self._execute_step(step_config)
                    
                    self.log_info(f"æ­¥éª¤ {step_name} æ‰§è¡Œå®Œæˆ")
                    
                except Exception as e:
                    self.log_error(f"æ­¥éª¤ {step_name} æ‰§è¡Œå¤±è´¥", error=str(e))
                    
                    # è®°å½•æ­¥éª¤æ‰§è¡Œå¤±è´¥
                    self.step_results[step_name] = {
                        "type": step_config["type"],
                        "status": "failed",
                        "error": str(e),
                        "execution_time": 0.0,
                        "row_count": 0,
                        "cache_hit": False
                    }
                    
                    # æ ¹æ®é€‰é¡¹å†³å®šæ˜¯å¦ç»§ç»­æ‰§è¡Œ
                    if not self.options.get("continue_on_error", False):
                        raise ExecutionError(f"æ­¥éª¤ {step_name} æ‰§è¡Œå¤±è´¥: {e}")
            
            self.log_info("æ‰€æœ‰æ­¥éª¤æ‰§è¡Œå®Œæˆ")
            
            return ExecutionResult(
                step_results=self.step_results,
                step_data=self.step_data
            )
            
        except ExecutionError:
            raise
        except Exception as e:
            self.log_error("æ­¥éª¤æ‰§è¡Œè¿‡ç¨‹å‡ºç°æœªçŸ¥é”™è¯¯", error=str(e), exc_info=True)
            raise ExecutionError(f"æ­¥éª¤æ‰§è¡Œå¤±è´¥: {e}")
    
    async def _execute_step(self, step_config: Dict[str, Any]) -> None:
        """
        æ‰§è¡Œå•ä¸ªæ­¥éª¤
        
        Args:
            step_config: æ­¥éª¤é…ç½®
        """
        step_name = step_config["name"]
        step_type = step_config["type"]
        config = step_config["config"]
        
        start_time = time.time()
        
        try:
            self.log_info(f"å¼€å§‹æ‰§è¡Œæ­¥éª¤: {step_name} (ç±»å‹: {step_type})")
            
            # æ£€æŸ¥ç¼“å­˜
            cache_key = self._generate_cache_key(step_config)
            cached_data = None
            cache_hit = False
            
            if self.options.get("cache_enabled", True):
                cached_data = await self.cache_manager.get(cache_key)
                if cached_data is not None:
                    cache_hit = True
                    self.log_info(f"æ­¥éª¤ {step_name} å‘½ä¸­ç¼“å­˜")
            
            if cache_hit:
                # ä½¿ç”¨ç¼“å­˜æ•°æ®
                step_data = cached_data
            else:
                # æ‰§è¡Œæ­¥éª¤
                step_data = await self._execute_step_by_type(step_type, config)
                
                # ç¼“å­˜ç»“æœ
                if self.options.get("cache_enabled", True) and step_data:
                    cache_ttl = self._parse_ttl(config.get("cache_ttl", "1h"))
                    await self.cache_manager.set(cache_key, step_data, cache_ttl)
            
            execution_time = time.time() - start_time
            
            # è®°å½•æ­¥éª¤æ‰§è¡Œç»“æœ
            self.step_results[step_name] = {
                "type": step_type,
                "status": "completed",
                "execution_time": execution_time,
                "row_count": len(step_data) if step_data else 0,
                "cache_hit": cache_hit
            }
            
            # å­˜å‚¨æ­¥éª¤æ•°æ®
            self.step_data[step_name] = step_data or []
            
            self.log_info(
                f"æ­¥éª¤ {step_name} æ‰§è¡Œå®Œæˆ",
                execution_time=execution_time,
                row_count=len(step_data) if step_data else 0,
                cache_hit=cache_hit
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            self.log_error(
                f"æ­¥éª¤ {step_name} æ‰§è¡Œå¤±è´¥",
                error=str(e),
                execution_time=execution_time
            )
            raise ExecutionError(f"æ­¥éª¤ {step_name} æ‰§è¡Œå¤±è´¥: {e}")
    
    async def _execute_step_by_type(self, step_type: str, 
                                   config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        æ ¹æ®æ­¥éª¤ç±»å‹æ‰§è¡Œæ­¥éª¤
        
        Args:
            step_type: æ­¥éª¤ç±»å‹
            config: æ­¥éª¤é…ç½®
            
        Returns:
            æ­¥éª¤æ‰§è¡Œç»“æœæ•°æ®
        """
        if step_type not in self.step_classes:
            raise ExecutionError(f"ä¸æ”¯æŒçš„æ­¥éª¤ç±»å‹: {step_type}")
        
        # è·å–æ­¥éª¤ç±»
        step_class = self.step_classes[step_type]
        
        # åˆ›å»ºæ­¥éª¤å®ä¾‹
        step_instance = step_class(config)
        
        # å‡†å¤‡æ‰§è¡Œä¸Šä¸‹æ–‡
        context = self._prepare_execution_context(config)
        
        # æ‰§è¡Œæ­¥éª¤
        result = await step_instance.execute(context)
        
        return result
    
    def _prepare_execution_context(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        å‡†å¤‡æ­¥éª¤æ‰§è¡Œä¸Šä¸‹æ–‡
        
        Args:
            config: æ­¥éª¤é…ç½®
            
        Returns:
            æ‰§è¡Œä¸Šä¸‹æ–‡
        """
        context = {
            "connector_manager": self.connector_manager,
            "cache_manager": self.cache_manager,
            "options": self.options,
            "step_data": self.step_data,
            "get_source_data": self._get_source_data
        }
        
        return context
    
    def _get_source_data(self, source_name: Union[str, List[str]]) -> Union[List[Dict[str, Any]], Dict[str, List[Dict[str, Any]]]]:
        """
        è·å–æºæ­¥éª¤æ•°æ®
        
        Args:
            source_name: æºæ­¥éª¤åç§°æˆ–åç§°åˆ—è¡¨
            
        Returns:
            æºæ­¥éª¤æ•°æ®
        """
        if isinstance(source_name, str):
            if source_name not in self.step_data:
                raise ExecutionError(f"æºæ­¥éª¤æ•°æ®ä¸å­˜åœ¨: {source_name}")
            return self.step_data[source_name]
        
        elif isinstance(source_name, list):
            result = {}
            for name in source_name:
                if name not in self.step_data:
                    raise ExecutionError(f"æºæ­¥éª¤æ•°æ®ä¸å­˜åœ¨: {name}")
                result[name] = self.step_data[name]
            return result
        
        else:
            raise ExecutionError(f"æ— æ•ˆçš„æºæ­¥éª¤åç§°ç±»å‹: {type(source_name)}")
    
    def _generate_cache_key(self, step_config: Dict[str, Any]) -> str:
        """
        ç”Ÿæˆæ­¥éª¤ç¼“å­˜é”®
        
        Args:
            step_config: æ­¥éª¤é…ç½®
            
        Returns:
            ç¼“å­˜é”®
        """
        try:
            # åˆ›å»ºåŒ…å«æ­¥éª¤é…ç½®å’Œä¾èµ–æ•°æ®çš„å­—å…¸
            cache_data = {
                "step_config": step_config,
                "dependency_data": {}
            }
            
            # æ·»åŠ ä¾èµ–æ­¥éª¤çš„æ•°æ®hash
            config = step_config.get("config", {})
            if "source" in config:
                source = config["source"]
                if isinstance(source, str):
                    cache_data["dependency_data"][source] = self._get_data_hash(source)
                elif isinstance(source, list):
                    for src in source:
                        cache_data["dependency_data"][src] = self._get_data_hash(src)
            
            if "sources" in config:
                sources = config["sources"]
                if isinstance(sources, list):
                    for src in sources:
                        cache_data["dependency_data"][src] = self._get_data_hash(src)
            
            # åºåˆ—åŒ–å¹¶ç”Ÿæˆhash
            import json
            data_str = json.dumps(cache_data, sort_keys=True)
            cache_key = hashlib.md5(data_str.encode('utf-8')).hexdigest()
            
            step_name = step_config["name"]
            return f"step_cache:{step_name}:{cache_key}"
            
        except Exception as e:
            self.log_error("ç”Ÿæˆæ­¥éª¤ç¼“å­˜é”®å¤±è´¥", error=str(e))
            # å¦‚æœç”Ÿæˆç¼“å­˜é”®å¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªåŸºäºæ—¶é—´çš„é”®ï¼ˆä¸ä¼šå‘½ä¸­ç¼“å­˜ï¼‰
            step_name = step_config.get("name", "unknown")
            return f"step_cache:{step_name}:no_cache_{int(time.time())}"
    
    def _get_data_hash(self, step_name: str) -> str:
        """
        è·å–æ­¥éª¤æ•°æ®çš„hashå€¼
        
        Args:
            step_name: æ­¥éª¤åç§°
            
        Returns:
            æ•°æ®hashå€¼
        """
        try:
            if step_name not in self.step_data:
                return "no_data"
            
            import json
            data_str = json.dumps(self.step_data[step_name], sort_keys=True)
            return hashlib.md5(data_str.encode('utf-8')).hexdigest()
            
        except Exception:
            return "hash_error"
    
    def _parse_ttl(self, ttl_str: str) -> int:
        """
        è§£æTTLå­—ç¬¦ä¸²
        
        Args:
            ttl_str: TTLå­—ç¬¦ä¸² (å¦‚: "1h", "30m", "3600s")
            
        Returns:
            TTLç§’æ•°
        """
        try:
            if isinstance(ttl_str, int):
                return ttl_str
            
            if not isinstance(ttl_str, str):
                return 3600  # é»˜è®¤1å°æ—¶
            
            ttl_str = ttl_str.lower().strip()
            
            # è§£ææ—¶é—´å•ä½
            if ttl_str.endswith('s'):
                return int(ttl_str[:-1])
            elif ttl_str.endswith('m'):
                return int(ttl_str[:-1]) * 60
            elif ttl_str.endswith('h'):
                return int(ttl_str[:-1]) * 3600
            elif ttl_str.endswith('d'):
                return int(ttl_str[:-1]) * 86400
            else:
                # å‡è®¾æ˜¯ç§’æ•°
                return int(ttl_str)
                
        except (ValueError, TypeError):
            self.log_warning(f"æ— æ³•è§£æTTLå­—ç¬¦ä¸²: {ttl_str}ï¼Œä½¿ç”¨é»˜è®¤å€¼3600ç§’")
            return 3600

================
File: uqm-backend/src/core/parser.py
================
"""
UQM JSONè§£æå™¨æ¨¡å—
è´Ÿè´£è§£æå’ŒéªŒè¯UQM JSONå®šä¹‰
"""

import json
from typing import Any, Dict, List, Optional, Set, Tuple
from pathlib import Path

import jsonschema
from jsonschema import validate, ValidationError as JsonSchemaValidationError

from src.api.models import ValidationError as UQMValidationError, ValidationResponse
from src.utils.logging import LoggerMixin
from src.utils.exceptions import ParseError, ValidationError


class UQMParser(LoggerMixin):
    """UQMæ•°æ®è§£æå™¨"""
    
    def __init__(self, schema_path: Optional[str] = None):
        """
        åˆå§‹åŒ–UQMè§£æå™¨
        
        Args:
            schema_path: UQM Schemaæ–‡ä»¶è·¯å¾„
        """
        self.schema_path = schema_path
        self.schema = None
        
        if schema_path:
            self.schema = self._load_schema(schema_path)
        else:
            # ä½¿ç”¨å†…ç½®çš„åŸºç¡€Schema
            self.schema = self._get_default_schema()
    
    def parse(self, uqm_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        è§£æUQMæ•°æ®
        
        Args:
            uqm_data: UQM JSONæ•°æ®
            
        Returns:
            è§£æåçš„UQMæ•°æ®
            
        Raises:
            ParseError: è§£æå¤±è´¥
        """
        try:
            self.log_info("å¼€å§‹è§£æUQMæ•°æ®")
            
            # éªŒè¯åŸºæœ¬ç»“æ„
            self._validate_basic_structure(uqm_data)
            
            # æå–å„ä¸ªç»„ä»¶
            metadata = self.extract_metadata(uqm_data)
            steps = self.extract_steps(uqm_data)
            parameters = self.extract_parameters(uqm_data)
            output_step = self.get_output_step(uqm_data)
            
            # éªŒè¯æ­¥éª¤ä¾èµ–å…³ç³»
            self._validate_step_dependencies(steps)
            
            # è§£ææ­¥éª¤æ‰§è¡Œé¡ºåº
            execution_order = self._resolve_step_order(steps)
            
            parsed_data = {
                "metadata": metadata,
                "steps": steps,
                "parameters": parameters,
                "output": output_step,
                "execution_order": execution_order
            }
            
            self.log_info(
                "UQMæ•°æ®è§£æå®Œæˆ",
                step_count=len(steps),
                output_step=output_step
            )
            
            return parsed_data
            
        except Exception as e:
            self.log_error("UQMæ•°æ®è§£æå¤±è´¥", error=str(e))
            raise ParseError(f"UQMæ•°æ®è§£æå¤±è´¥: {e}")
    
    def extract_steps(self, uqm_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        æå–æ­¥éª¤åˆ—è¡¨
        
        Args:
            uqm_data: UQMæ•°æ®
            
        Returns:
            æ­¥éª¤åˆ—è¡¨
        """
        steps = uqm_data.get("steps", [])
        
        if not isinstance(steps, list):
            raise ParseError("stepså¿…é¡»æ˜¯ä¸€ä¸ªæ•°ç»„")
        
        if not steps:
            raise ParseError("è‡³å°‘éœ€è¦å®šä¹‰ä¸€ä¸ªæ­¥éª¤")
        
        # éªŒè¯æ¯ä¸ªæ­¥éª¤çš„åŸºæœ¬ç»“æ„
        for i, step in enumerate(steps):
            if not isinstance(step, dict):
                raise ParseError(f"æ­¥éª¤ {i} å¿…é¡»æ˜¯ä¸€ä¸ªå¯¹è±¡")
            
            if "name" not in step:
                raise ParseError(f"æ­¥éª¤ {i} ç¼ºå°‘nameå­—æ®µ")
            
            if "type" not in step:
                raise ParseError(f"æ­¥éª¤ {step['name']} ç¼ºå°‘typeå­—æ®µ")
            
            if "config" not in step:
                raise ParseError(f"æ­¥éª¤ {step['name']} ç¼ºå°‘configå­—æ®µ")
        
        return steps
    
    def get_output_step(self, uqm_data: Dict[str, Any]) -> str:
        """
        è·å–è¾“å‡ºæ­¥éª¤åç§°
        
        Args:
            uqm_data: UQMæ•°æ®
            
        Returns:
            è¾“å‡ºæ­¥éª¤åç§°
        """
        output_step = uqm_data.get("output")
        
        if not output_step:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºæ­¥éª¤ï¼Œä½¿ç”¨æœ€åä¸€ä¸ªæ­¥éª¤
            steps = uqm_data.get("steps", [])
            if steps:
                output_step = steps[-1]["name"]
            else:
                raise ParseError("æ— æ³•ç¡®å®šè¾“å‡ºæ­¥éª¤")
        
        # éªŒè¯è¾“å‡ºæ­¥éª¤æ˜¯å¦å­˜åœ¨
        step_names = {step["name"] for step in uqm_data.get("steps", [])}
        if output_step not in step_names:
            raise ParseError(f"è¾“å‡ºæ­¥éª¤ '{output_step}' ä¸å­˜åœ¨")
        
        return output_step
    
    def extract_metadata(self, uqm_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        æå–å…ƒæ•°æ®
        
        Args:
            uqm_data: UQMæ•°æ®
            
        Returns:
            å…ƒæ•°æ®å­—å…¸
        """
        metadata = uqm_data.get("metadata", {})
        
        if not isinstance(metadata, dict):
            raise ParseError("metadataå¿…é¡»æ˜¯ä¸€ä¸ªå¯¹è±¡")
        
        # è®¾ç½®é»˜è®¤å€¼
        default_metadata = {
            "name": "æœªå‘½åæŸ¥è¯¢",
            "description": "",
            "version": "1.0",
            "author": "",
            "tags": []
        }
        
        # åˆå¹¶é»˜è®¤å€¼å’Œç”¨æˆ·æä¾›çš„å…ƒæ•°æ®
        result = {**default_metadata, **metadata}
        
        return result
    
    def extract_parameters(self, uqm_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        æå–å‚æ•°å®šä¹‰
        
        Args:
            uqm_data: UQMæ•°æ®
            
        Returns:
            å‚æ•°å®šä¹‰åˆ—è¡¨
        """
        parameters = uqm_data.get("parameters", [])
        
        if not isinstance(parameters, list):
            raise ParseError("parameterså¿…é¡»æ˜¯ä¸€ä¸ªæ•°ç»„")
        
        # éªŒè¯æ¯ä¸ªå‚æ•°å®šä¹‰
        for i, param in enumerate(parameters):
            if not isinstance(param, dict):
                raise ParseError(f"å‚æ•° {i} å¿…é¡»æ˜¯ä¸€ä¸ªå¯¹è±¡")
            
            if "name" not in param:
                raise ParseError(f"å‚æ•° {i} ç¼ºå°‘nameå­—æ®µ")
            
            if "type" not in param:
                raise ParseError(f"å‚æ•° {param['name']} ç¼ºå°‘typeå­—æ®µ")
        
        return parameters
    
    def validate_schema(self, uqm_data: Dict[str, Any]) -> ValidationResponse:
        """
        éªŒè¯UQMæ•°æ®æ˜¯å¦ç¬¦åˆSchema
        
        Args:
            uqm_data: UQMæ•°æ®
            
        Returns:
            éªŒè¯ç»“æœ
        """
        errors = []
        warnings = []
        
        try:
            # å¦‚æœæœ‰Schemaï¼Œè¿›è¡ŒSchemaéªŒè¯
            if self.schema:
                try:
                    validate(instance=uqm_data, schema=self.schema)
                except JsonSchemaValidationError as e:
                    errors.append(UQMValidationError(
                        field=".".join(str(x) for x in e.path),
                        message=e.message,
                        value=e.instance
                    ))
            
            # è¿›è¡Œä¸šåŠ¡é€»è¾‘éªŒè¯
            try:
                self.parse(uqm_data)
            except ParseError as e:
                errors.append(UQMValidationError(
                    field="general",
                    message=str(e),
                    value=None
                ))
            
            # æ£€æŸ¥æ½œåœ¨çš„è­¦å‘Š
            warnings.extend(self._check_warnings(uqm_data))
            
            return ValidationResponse(
                valid=len(errors) == 0,
                errors=errors if errors else None,
                warnings=warnings if warnings else None
            )
            
        except Exception as e:
            self.log_error("UQMéªŒè¯è¿‡ç¨‹å‡ºç°é”™è¯¯", error=str(e))
            return ValidationResponse(
                valid=False,
                errors=[UQMValidationError(
                    field="validation",
                    message=f"éªŒè¯è¿‡ç¨‹å‡ºç°é”™è¯¯: {e}",
                    value=None
                )]
            )
    
    def _validate_basic_structure(self, uqm_data: Dict[str, Any]) -> None:
        """éªŒè¯UQMåŸºæœ¬ç»“æ„"""
        if not isinstance(uqm_data, dict):
            raise ParseError("UQMæ•°æ®å¿…é¡»æ˜¯ä¸€ä¸ªJSONå¯¹è±¡")
        
        required_fields = ["steps"]
        for field in required_fields:
            if field not in uqm_data:
                raise ParseError(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
    
    def _validate_step_dependencies(self, steps: List[Dict[str, Any]]) -> None:
        """
        éªŒè¯æ­¥éª¤ä¾èµ–å…³ç³»
        
        Args:
            steps: æ­¥éª¤åˆ—è¡¨
        """
        step_names = {step["name"] for step in steps}
        
        for step in steps:
            step_name = step["name"]
            step_config = step.get("config", {})
            
            # æ£€æŸ¥sourceå¼•ç”¨
            if "source" in step_config:
                source = step_config["source"]
                if isinstance(source, str) and source not in step_names:
                    raise ParseError(f"æ­¥éª¤ '{step_name}' å¼•ç”¨äº†ä¸å­˜åœ¨çš„æºæ­¥éª¤: '{source}'")
                elif isinstance(source, list):
                    for src in source:
                        if src not in step_names:
                            raise ParseError(f"æ­¥éª¤ '{step_name}' å¼•ç”¨äº†ä¸å­˜åœ¨çš„æºæ­¥éª¤: '{src}'")
            
            # æ£€æŸ¥sourceså¼•ç”¨ï¼ˆç”¨äºunionæ­¥éª¤ï¼‰
            if "sources" in step_config:
                sources = step_config["sources"]
                if isinstance(sources, list):
                    for src in sources:
                        if src not in step_names:
                            raise ParseError(f"æ­¥éª¤ '{step_name}' å¼•ç”¨äº†ä¸å­˜åœ¨çš„æºæ­¥éª¤: '{src}'")
    
    def _resolve_step_order(self, steps: List[Dict[str, Any]]) -> List[str]:
        """
        è§£ææ­¥éª¤æ‰§è¡Œé¡ºåºï¼ˆæ‹“æ‰‘æ’åºï¼‰
        
        Args:
            steps: æ­¥éª¤åˆ—è¡¨
            
        Returns:
            æŒ‰æ‰§è¡Œé¡ºåºæ’åˆ—çš„æ­¥éª¤åç§°åˆ—è¡¨
        """
        # æ„å»ºä¾èµ–å›¾
        dependencies = {}
        step_names = set()
        
        for step in steps:
            step_name = step["name"]
            step_names.add(step_name)
            dependencies[step_name] = set()
            
            step_config = step.get("config", {})
            
            # æ·»åŠ sourceä¾èµ–
            if "source" in step_config:
                source = step_config["source"]
                if isinstance(source, str):
                    dependencies[step_name].add(source)
                elif isinstance(source, list):
                    dependencies[step_name].update(source)
            
            # æ·»åŠ sourcesä¾èµ–
            if "sources" in step_config:
                sources = step_config["sources"]
                if isinstance(sources, list):
                    dependencies[step_name].update(sources)
        
        # æ‹“æ‰‘æ’åº
        result = []
        visited = set()
        temp_visited = set()
        
        def visit(node: str):
            if node in temp_visited:
                raise ParseError(f"æ£€æµ‹åˆ°å¾ªç¯ä¾èµ–ï¼Œæ¶‰åŠæ­¥éª¤: {node}")
            
            if node not in visited:
                temp_visited.add(node)
                
                for dep in dependencies.get(node, set()):
                    visit(dep)
                
                temp_visited.remove(node)
                visited.add(node)
                result.append(node)
        
        for step_name in step_names:
            if step_name not in visited:
                visit(step_name)
        
        return result
    
    def _check_warnings(self, uqm_data: Dict[str, Any]) -> List[str]:
        """æ£€æŸ¥æ½œåœ¨çš„è­¦å‘Šé—®é¢˜"""
        warnings = []
        
        # æ£€æŸ¥å…ƒæ•°æ®å®Œæ•´æ€§
        metadata = uqm_data.get("metadata", {})
        if not metadata.get("description"):
            warnings.append("å»ºè®®æ·»åŠ æŸ¥è¯¢æè¿°")
        
        if not metadata.get("author"):
            warnings.append("å»ºè®®æ·»åŠ ä½œè€…ä¿¡æ¯")
        
        # æ£€æŸ¥æ­¥éª¤æ•°é‡
        steps = uqm_data.get("steps", [])
        if len(steps) > 10:
            warnings.append("æ­¥éª¤æ•°é‡è¾ƒå¤šï¼Œå¯èƒ½å½±å“æ€§èƒ½")
        
        # æ£€æŸ¥å‚æ•°ä½¿ç”¨
        parameters = uqm_data.get("parameters", [])
        if parameters:
            param_names = {param["name"] for param in parameters}
            uqm_str = json.dumps(uqm_data)
            
            for param_name in param_names:
                if f"${param_name}" not in uqm_str:
                    warnings.append(f"å‚æ•° '{param_name}' å®šä¹‰äº†ä½†æœªä½¿ç”¨")
        
        return warnings
    
    def _load_schema(self, schema_path: str) -> Dict[str, Any]:
        """
        åŠ è½½UQM Schemaæ–‡ä»¶
        
        Args:
            schema_path: Schemaæ–‡ä»¶è·¯å¾„
            
        Returns:
            Schemaå­—å…¸
        """
        try:
            schema_file = Path(schema_path)
            if not schema_file.exists():
                raise FileNotFoundError(f"Schemaæ–‡ä»¶ä¸å­˜åœ¨: {schema_path}")
            
            with open(schema_file, 'r', encoding='utf-8') as f:
                schema = json.load(f)
            
            self.log_info("UQM SchemaåŠ è½½æˆåŠŸ", schema_path=schema_path)
            return schema
            
        except Exception as e:
            self.log_error("åŠ è½½UQM Schemaå¤±è´¥", error=str(e))
            raise ParseError(f"åŠ è½½UQM Schemaå¤±è´¥: {e}")
    
    def _get_default_schema(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤çš„UQM Schema"""
        return {
            "$schema": "http://json-schema.org/draft-07/schema#",
            "type": "object",
            "required": ["steps"],
            "properties": {
                "metadata": {
                    "type": "object",
                    "properties": {
                        "name": {"type": "string"},
                        "description": {"type": "string"},
                        "version": {"type": "string"},
                        "author": {"type": "string"},
                        "tags": {
                            "type": "array",
                            "items": {"type": "string"}
                        }
                    }
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "required": ["name", "type"],
                        "properties": {
                            "name": {"type": "string"},
                            "type": {"type": "string"},
                            "default": {},
                            "required": {"type": "boolean"},
                            "description": {"type": "string"}
                        }
                    }
                },
                "steps": {
                    "type": "array",
                    "minItems": 1,
                    "items": {
                        "type": "object",
                        "required": ["name", "type", "config"],
                        "properties": {
                            "name": {"type": "string"},
                            "type": {
                                "type": "string",
                                "enum": ["query", "enrich", "pivot", "unpivot", "union", "assert"]
                            },
                            "config": {"type": "object"}
                        }
                    }
                },
                "output": {"type": "string"}
            }
        }

================
File: uqm-backend/src/main.py
================
"""
UQM Backend ä¸»ç¨‹åºå…¥å£
è´Ÿè´£å¯åŠ¨FastAPIåº”ç”¨ç¨‹åºå’Œé…ç½®ç›¸å…³æœåŠ¡
"""

import uvicorn
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.trustedhost import TrustedHostMiddleware
from contextlib import asynccontextmanager

from src.api.routes import router
from src.config.settings import get_settings
from src.core.cache import get_cache_manager
from src.utils.logging import setup_logging
from src.utils.exceptions import setup_exception_handlers


# åº”ç”¨ç¨‹åºç”Ÿå‘½å‘¨æœŸç®¡ç†
@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç¨‹åºå¯åŠ¨å’Œå…³é—­æ—¶çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    settings = get_settings()
    setup_logging(settings.LOG_LEVEL)
    
    # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
    cache_manager = get_cache_manager()
    await cache_manager.initialize()
    
    print("UQM Backend æœåŠ¡å¯åŠ¨å®Œæˆ")
    
    yield
    
    # å…³é—­æ—¶æ¸…ç†èµ„æº
    await cache_manager.close()
    print("UQM Backend æœåŠ¡å·²å…³é—­")


def create_app() -> FastAPI:
    """åˆ›å»ºFastAPIåº”ç”¨ç¨‹åºå®ä¾‹"""
    settings = get_settings()
    
    # åˆ›å»ºFastAPIåº”ç”¨å®ä¾‹
    app = FastAPI(
        title="UQM Backend API",
        description="ç»Ÿä¸€æŸ¥è¯¢æ¨¡å‹(UQM)åç«¯æ‰§è¡Œå¼•æ“",
        version="0.1.0",
        docs_url="/docs" if settings.DEBUG else None,
        redoc_url="/redoc" if settings.DEBUG else None,
        lifespan=lifespan
    )
    
    # é…ç½®CORSä¸­é—´ä»¶
    if settings.CORS_ORIGINS:
        app.add_middleware(
            CORSMiddleware,
            allow_origins=settings.CORS_ORIGINS,
            allow_credentials=settings.CORS_CREDENTIALS,
            allow_methods=settings.CORS_METHODS,
            allow_headers=settings.CORS_HEADERS,
        )
    
    # é…ç½®ä¿¡ä»»ä¸»æœºä¸­é—´ä»¶
    if settings.ALLOWED_HOSTS:
        app.add_middleware(
            TrustedHostMiddleware,
            allowed_hosts=settings.ALLOWED_HOSTS
        )
    
    # æ³¨å†Œè·¯ç”±
    app.include_router(router, prefix="/api/v1")
    
    # è®¾ç½®å¼‚å¸¸å¤„ç†å™¨
    setup_exception_handlers(app)
    
    return app


# åˆ›å»ºåº”ç”¨å®ä¾‹
app = create_app()


def main():
    """ä¸»å‡½æ•° - å¯åŠ¨åº”ç”¨ç¨‹åº"""
    settings = get_settings()
    
    uvicorn.run(
        "src.main:app",
        host=settings.HOST,
        port=settings.PORT,
        reload=settings.DEBUG,
        log_level=settings.LOG_LEVEL.lower(),
        access_log=True,
        workers=1 if settings.DEBUG else 4
    )


if __name__ == "__main__":
    main()

================
File: uqm-backend/src/steps/__init__.py
================
"""æ­¥éª¤æ¨¡å—åŒ…åˆå§‹åŒ–æ–‡ä»¶"""

================
File: uqm-backend/src/steps/assert_step.py
================
"""
æ•°æ®æ–­è¨€æ­¥éª¤å®ç°
ç”¨äºéªŒè¯æ•°æ®è´¨é‡å’Œä¸šåŠ¡è§„åˆ™
"""

from typing import Any, Dict, List, Optional, Union
import pandas as pd
import re

from src.steps.base import BaseStep
from src.utils.exceptions import ValidationError, ExecutionError


class AssertStep(BaseStep):
    """æ–­è¨€æ­¥éª¤æ‰§è¡Œå™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–æ–­è¨€æ­¥éª¤
        
        Args:
            config: æ–­è¨€æ­¥éª¤é…ç½®
        """
        super().__init__(config)
        
        # æ”¯æŒçš„æ–­è¨€ç±»å‹
        self.supported_assertions = {
            'row_count': self._assert_row_count,
            'not_null': self._assert_not_null,
            'unique': self._assert_unique,
            'range': self._assert_range,
            'regex': self._assert_regex,
            'custom': self._assert_custom,
            'column_exists': self._assert_column_exists,
            'data_type': self._assert_data_type,
            'value_in': self._assert_value_in,
            'relationship': self._assert_relationship
        }
    
    def validate(self) -> None:
        """éªŒè¯æ–­è¨€æ­¥éª¤é…ç½®"""
        required_fields = ["source", "assertions"]
        self._validate_required_config(required_fields)
        
        # éªŒè¯assertionså­—æ®µ
        assertions = self.config.get("assertions")
        if not isinstance(assertions, list):
            raise ValidationError("assertionså¿…é¡»æ˜¯æ•°ç»„")
        
        # éªŒè¯æ¯ä¸ªæ–­è¨€
        for i, assertion in enumerate(assertions):
            if not isinstance(assertion, dict):
                raise ValidationError(f"æ–­è¨€ {i} å¿…é¡»æ˜¯å¯¹è±¡")
            
            assertion_type = assertion.get("type")
            if not assertion_type:
                raise ValidationError(f"æ–­è¨€ {i} ç¼ºå°‘typeå­—æ®µ")
            
            if assertion_type not in self.supported_assertions:
                raise ValidationError(f"ä¸æ”¯æŒçš„æ–­è¨€ç±»å‹: {assertion_type}")
    
    async def execute(self, context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œæ–­è¨€æ­¥éª¤
        
        Args:
            context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            æºæ•°æ®ï¼ˆæ–­è¨€é€šè¿‡æ—¶ï¼‰
        """
        try:
            # è·å–æºæ•°æ®
            source_name = self.config["source"]
            source_data = context["get_source_data"](source_name)
            
            # æ‰§è¡Œæ–­è¨€æ£€æŸ¥
            assertion_results = self._perform_assertions(source_data)
            
            # å¤„ç†æ–­è¨€ç»“æœ
            self._handle_assertion_results(assertion_results)
            
            # æ–­è¨€é€šè¿‡ï¼Œè¿”å›åŸå§‹æ•°æ®
            return source_data
            
        except Exception as e:
            self.log_error("æ–­è¨€æ­¥éª¤æ‰§è¡Œå¤±è´¥", error=str(e))
            raise ExecutionError(f"æ–­è¨€æ‰§è¡Œå¤±è´¥: {e}")
    
    def _perform_assertions(self, source_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œæ–­è¨€æ£€æŸ¥
        
        Args:
            source_data: æºæ•°æ®
            
        Returns:
            æ–­è¨€ç»“æœåˆ—è¡¨
        """
        assertions = self.config["assertions"]
        assertion_results = []
        
        for assertion in assertions:
            try:
                assertion_type = assertion["type"]
                assertion_func = self.supported_assertions[assertion_type]
                
                # æ‰§è¡Œæ–­è¨€
                result = assertion_func(source_data, assertion)
                
                assertion_results.append({
                    "type": assertion_type,
                    "passed": result.get("passed", True),
                    "message": result.get("message", ""),
                    "details": result.get("details", {})
                })
                
            except Exception as e:
                assertion_results.append({
                    "type": assertion.get("type", "unknown"),
                    "passed": False,
                    "message": str(e),
                    "details": {"error": str(e)}
                })
        
        return assertion_results
    
    def _handle_assertion_results(self, assertion_results: List[Dict[str, Any]]) -> None:
        """
        å¤„ç†æ–­è¨€ç»“æœ
        
        Args:
            assertion_results: æ–­è¨€ç»“æœåˆ—è¡¨
        """
        failed_assertions = [r for r in assertion_results if not r["passed"]]
        
        if failed_assertions:
            # ç”Ÿæˆæ–­è¨€æŠ¥å‘Š
            report = self._generate_assertion_report(assertion_results)
            
            # æ ¹æ®é…ç½®å†³å®šå¦‚ä½•å¤„ç†å¤±è´¥
            on_failure = self.config.get("on_failure", "error")
            
            if on_failure == "error":
                # æŠ›å‡ºå¼‚å¸¸
                raise ExecutionError(f"æ–­è¨€æ£€æŸ¥å¤±è´¥:\n{report}")
            elif on_failure == "warning":
                # è®°å½•è­¦å‘Š
                self.log_warning("æ–­è¨€æ£€æŸ¥å¤±è´¥", report=report)
            elif on_failure == "ignore":
                # å¿½ç•¥å¤±è´¥
                self.log_info("æ–­è¨€æ£€æŸ¥å¤±è´¥ä½†è¢«å¿½ç•¥", report=report)
        else:
            self.log_info("æ‰€æœ‰æ–­è¨€æ£€æŸ¥é€šè¿‡")
    
    def _assert_row_count(self, data: List[Dict[str, Any]], assertion: Dict[str, Any]) -> Dict[str, Any]:
        """æ–­è¨€è¡Œæ•°"""
        expected_count = assertion.get("expected")
        min_count = assertion.get("min")
        max_count = assertion.get("max")
        
        actual_count = len(data)
        
        if expected_count is not None and actual_count != expected_count:
            return {
                "passed": False,
                "message": f"æœŸæœ›è¡Œæ•° {expected_count}ï¼Œå®é™…è¡Œæ•° {actual_count}",
                "details": {"expected": expected_count, "actual": actual_count}
            }
        
        if min_count is not None and actual_count < min_count:
            return {
                "passed": False,
                "message": f"è¡Œæ•°å°‘äºæœ€å°å€¼ {min_count}ï¼Œå®é™…è¡Œæ•° {actual_count}",
                "details": {"min": min_count, "actual": actual_count}
            }
        
        if max_count is not None and actual_count > max_count:
            return {
                "passed": False,
                "message": f"è¡Œæ•°è¶…è¿‡æœ€å¤§å€¼ {max_count}ï¼Œå®é™…è¡Œæ•° {actual_count}",
                "details": {"max": max_count, "actual": actual_count}
            }
        
        return {"passed": True, "message": f"è¡Œæ•°æ£€æŸ¥é€šè¿‡: {actual_count}"}
    
    def _assert_not_null(self, data: List[Dict[str, Any]], assertion: Dict[str, Any]) -> Dict[str, Any]:
        """æ–­è¨€éç©º"""
        columns = assertion.get("columns", [])
        if isinstance(columns, str):
            columns = [columns]
        
        null_records = []
        
        for i, record in enumerate(data):
            for column in columns:
                if column in record and (record[column] is None or record[column] == ""):
                    null_records.append({"row": i, "column": column, "value": record[column]})
        
        if null_records:
            return {
                "passed": False,
                "message": f"å‘ç° {len(null_records)} ä¸ªç©ºå€¼",
                "details": {"null_records": null_records[:10]}  # åªæ˜¾ç¤ºå‰10ä¸ª
            }
        
        return {"passed": True, "message": f"éç©ºæ£€æŸ¥é€šè¿‡ï¼Œæ£€æŸ¥äº† {len(columns)} åˆ—"}
    
    def _assert_unique(self, data: List[Dict[str, Any]], assertion: Dict[str, Any]) -> Dict[str, Any]:
        """æ–­è¨€å”¯ä¸€æ€§"""
        columns = assertion.get("columns", [])
        if isinstance(columns, str):
            columns = [columns]
        
        seen_values = {}
        duplicate_records = []
        
        for i, record in enumerate(data):
            key_values = tuple(record.get(col) for col in columns)
            
            if key_values in seen_values:
                duplicate_records.append({
                    "row": i,
                    "duplicate_of": seen_values[key_values],
                    "values": dict(zip(columns, key_values))
                })
            else:
                seen_values[key_values] = i
        
        if duplicate_records:
            return {
                "passed": False,
                "message": f"å‘ç° {len(duplicate_records)} ä¸ªé‡å¤å€¼",
                "details": {"duplicate_records": duplicate_records[:10]}
            }
        
        return {"passed": True, "message": f"å”¯ä¸€æ€§æ£€æŸ¥é€šè¿‡ï¼Œæ£€æŸ¥äº† {len(columns)} åˆ—"}
    
    def _assert_range(self, data: List[Dict[str, Any]], assertion: Dict[str, Any]) -> Dict[str, Any]:
        """æ–­è¨€å€¼èŒƒå›´"""
        column = assertion.get("column")
        min_value = assertion.get("min")
        max_value = assertion.get("max")
        
        out_of_range_records = []
        
        for i, record in enumerate(data):
            if column in record:
                value = record[column]
                if isinstance(value, (int, float)):
                    if min_value is not None and value < min_value:
                        out_of_range_records.append({
                            "row": i,
                            "column": column,
                            "value": value,
                            "reason": f"å°äºæœ€å°å€¼ {min_value}"
                        })
                    elif max_value is not None and value > max_value:
                        out_of_range_records.append({
                            "row": i,
                            "column": column,
                            "value": value,
                            "reason": f"å¤§äºæœ€å¤§å€¼ {max_value}"
                        })
        
        if out_of_range_records:
            return {
                "passed": False,
                "message": f"å‘ç° {len(out_of_range_records)} ä¸ªè¶…å‡ºèŒƒå›´çš„å€¼",
                "details": {"out_of_range_records": out_of_range_records[:10]}
            }
        
        return {"passed": True, "message": f"èŒƒå›´æ£€æŸ¥é€šè¿‡ï¼Œåˆ— {column}"}
    
    def _assert_regex(self, data: List[Dict[str, Any]], assertion: Dict[str, Any]) -> Dict[str, Any]:
        """æ–­è¨€æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…"""
        column = assertion.get("column")
        pattern = assertion.get("pattern")
        
        if not pattern:
            return {"passed": False, "message": "ç¼ºå°‘patternå‚æ•°"}
        
        try:
            regex = re.compile(pattern)
        except re.error as e:
            return {"passed": False, "message": f"æ— æ•ˆçš„æ­£åˆ™è¡¨è¾¾å¼: {e}"}
        
        mismatch_records = []
        
        for i, record in enumerate(data):
            if column in record:
                value = str(record[column])
                if not regex.match(value):
                    mismatch_records.append({
                        "row": i,
                        "column": column,
                        "value": value
                    })
        
        if mismatch_records:
            return {
                "passed": False,
                "message": f"å‘ç° {len(mismatch_records)} ä¸ªä¸åŒ¹é…æ­£åˆ™è¡¨è¾¾å¼çš„å€¼",
                "details": {"mismatch_records": mismatch_records[:10]}
            }
        
        return {"passed": True, "message": f"æ­£åˆ™è¡¨è¾¾å¼æ£€æŸ¥é€šè¿‡ï¼Œåˆ— {column}"}
    
    def _assert_custom(self, data: List[Dict[str, Any]], assertion: Dict[str, Any]) -> Dict[str, Any]:
        """è‡ªå®šä¹‰æ–­è¨€"""
        expression = assertion.get("expression")
        
        if not expression:
            return {"passed": False, "message": "ç¼ºå°‘expressionå‚æ•°"}
        
        try:
            # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„è‡ªå®šä¹‰æ–­è¨€é€»è¾‘
            # ä¸ºäº†å®‰å…¨æ€§ï¼Œè¿™é‡Œåªæ˜¯ä¸€ä¸ªç¤ºä¾‹
            failed_rows = []
            
            for i, record in enumerate(data):
                # ç®€å•çš„è¡¨è¾¾å¼è¯„ä¼°ï¼ˆç”Ÿäº§ç¯å¢ƒéœ€è¦æ›´å®‰å…¨çš„å®ç°ï¼‰
                try:
                    # å°†è®°å½•ä½œä¸ºå±€éƒ¨å˜é‡ä¼ å…¥
                    if not eval(expression, {"__builtins__": {}}, record):
                        failed_rows.append(i)
                except Exception:
                    failed_rows.append(i)
            
            if failed_rows:
                return {
                    "passed": False,
                    "message": f"è‡ªå®šä¹‰æ–­è¨€å¤±è´¥ï¼Œ{len(failed_rows)} è¡Œä¸æ»¡è¶³æ¡ä»¶",
                    "details": {"failed_rows": failed_rows[:10]}
                }
            
            return {"passed": True, "message": "è‡ªå®šä¹‰æ–­è¨€é€šè¿‡"}
            
        except Exception as e:
            return {"passed": False, "message": f"è‡ªå®šä¹‰æ–­è¨€æ‰§è¡Œé”™è¯¯: {e}"}
    
    def _assert_column_exists(self, data: List[Dict[str, Any]], assertion: Dict[str, Any]) -> Dict[str, Any]:
        """æ–­è¨€åˆ—å­˜åœ¨"""
        columns = assertion.get("columns", [])
        if isinstance(columns, str):
            columns = [columns]
        
        if not data:
            return {"passed": False, "message": "æ•°æ®ä¸ºç©ºï¼Œæ— æ³•æ£€æŸ¥åˆ—"}
        
        existing_columns = set(data[0].keys())
        missing_columns = [col for col in columns if col not in existing_columns]
        
        if missing_columns:
            return {
                "passed": False,
                "message": f"ç¼ºå°‘åˆ—: {missing_columns}",
                "details": {"missing_columns": missing_columns}
            }
        
        return {"passed": True, "message": f"åˆ—å­˜åœ¨æ£€æŸ¥é€šè¿‡: {columns}"}
    
    def _assert_data_type(self, data: List[Dict[str, Any]], assertion: Dict[str, Any]) -> Dict[str, Any]:
        """æ–­è¨€æ•°æ®ç±»å‹"""
        column = assertion.get("column")
        expected_type = assertion.get("expected_type")
        
        type_mapping = {
            "int": int,
            "float": float,
            "str": str,
            "bool": bool,
            "number": (int, float)
        }
        
        expected_python_type = type_mapping.get(expected_type)
        if not expected_python_type:
            return {"passed": False, "message": f"ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {expected_type}"}
        
        type_errors = []
        
        for i, record in enumerate(data):
            if column in record and record[column] is not None:
                if not isinstance(record[column], expected_python_type):
                    type_errors.append({
                        "row": i,
                        "column": column,
                        "value": record[column],
                        "actual_type": type(record[column]).__name__
                    })
        
        if type_errors:
            return {
                "passed": False,
                "message": f"å‘ç° {len(type_errors)} ä¸ªç±»å‹é”™è¯¯",
                "details": {"type_errors": type_errors[:10]}
            }
        
        return {"passed": True, "message": f"æ•°æ®ç±»å‹æ£€æŸ¥é€šè¿‡ï¼Œåˆ— {column}"}
    
    def _assert_value_in(self, data: List[Dict[str, Any]], assertion: Dict[str, Any]) -> Dict[str, Any]:
        """æ–­è¨€å€¼åœ¨æŒ‡å®šé›†åˆä¸­"""
        column = assertion.get("column")
        allowed_values = assertion.get("allowed_values", [])
        
        invalid_records = []
        
        for i, record in enumerate(data):
            if column in record:
                value = record[column]
                if value not in allowed_values:
                    invalid_records.append({
                        "row": i,
                        "column": column,
                        "value": value
                    })
        
        if invalid_records:
            return {
                "passed": False,
                "message": f"å‘ç° {len(invalid_records)} ä¸ªæ— æ•ˆå€¼",
                "details": {"invalid_records": invalid_records[:10]}
            }
        
        return {"passed": True, "message": f"å€¼åŸŸæ£€æŸ¥é€šè¿‡ï¼Œåˆ— {column}"}
    
    def _assert_relationship(self, data: List[Dict[str, Any]], assertion: Dict[str, Any]) -> Dict[str, Any]:
        """æ–­è¨€å­—æ®µé—´å…³ç³»"""
        # è¿™æ˜¯ä¸€ä¸ªå¤æ‚çš„æ–­è¨€ç±»å‹ï¼Œå¯ä»¥æ£€æŸ¥å­—æ®µé—´çš„å…³ç³»
        # ä¾‹å¦‚ï¼šå¼€å§‹æ—¥æœŸå¿…é¡»å°äºç»“æŸæ—¥æœŸ
        return {"passed": True, "message": "å…³ç³»æ–­è¨€æš‚æœªå®ç°"}
    
    def _generate_assertion_report(self, assertion_results: List[Dict[str, Any]]) -> str:
        """ç”Ÿæˆæ–­è¨€æŠ¥å‘Š"""
        total_assertions = len(assertion_results)
        passed_assertions = len([r for r in assertion_results if r["passed"]])
        failed_assertions = total_assertions - passed_assertions
        
        report_lines = [
            f"æ–­è¨€æ£€æŸ¥æŠ¥å‘Š:",
            f"æ€»è®¡: {total_assertions}, é€šè¿‡: {passed_assertions}, å¤±è´¥: {failed_assertions}",
            ""
        ]
        
        for result in assertion_results:
            status = "âœ“" if result["passed"] else "âœ—"
            report_lines.append(f"{status} {result['type']}: {result['message']}")
        
        return "\n".join(report_lines)

================
File: uqm-backend/src/steps/base.py
================
"""
æ­¥éª¤åŸºç±»å®šä¹‰
å®šä¹‰æ‰€æœ‰æ­¥éª¤çš„æŠ½è±¡åŸºç±»å’Œé€šç”¨åŠŸèƒ½
"""

import time
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional

from src.utils.logging import LoggerMixin
from src.utils.exceptions import ExecutionError, ValidationError


class BaseStep(ABC, LoggerMixin):
    """æ‰€æœ‰æ­¥éª¤çš„æŠ½è±¡åŸºç±»"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–æ­¥éª¤
        
        Args:
            config: æ­¥éª¤é…ç½®
        """
        self.config = config
        self.step_name = config.get("name", self.__class__.__name__)
        
        # éªŒè¯é…ç½®
        self.validate()
    
    @abstractmethod
    async def execute(self, context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œæ­¥éª¤ï¼ˆæŠ½è±¡æ–¹æ³•ï¼‰
        
        Args:
            context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            æ­¥éª¤æ‰§è¡Œç»“æœ
        """
        pass
    
    @abstractmethod
    def validate(self) -> None:
        """éªŒè¯æ­¥éª¤é…ç½®ï¼ˆæŠ½è±¡æ–¹æ³•ï¼‰"""
        pass
    
    def _prepare_context(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        å‡†å¤‡æ‰§è¡Œä¸Šä¸‹æ–‡
        
        Args:
            context: åŸå§‹ä¸Šä¸‹æ–‡
            
        Returns:
            å‡†å¤‡å¥½çš„ä¸Šä¸‹æ–‡
        """
        # æ·»åŠ æ­¥éª¤ç‰¹å®šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        prepared_context = context.copy()
        prepared_context["step_config"] = self.config
        prepared_context["step_name"] = self.step_name
        
        return prepared_context
    
    def _log_execution(self, start_time: float, end_time: float, 
                      result_count: int = 0) -> None:
        """
        è®°å½•æ‰§è¡Œæ—¥å¿—
        
        Args:
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´
            result_count: ç»“æœæ•°é‡
        """
        execution_time = end_time - start_time
        
        self.log_info(
            f"æ­¥éª¤ {self.step_name} æ‰§è¡Œå®Œæˆ",
            execution_time=execution_time,
            result_count=result_count
        )
    
    def _handle_error(self, error: Exception) -> None:
        """
        å¤„ç†æ‰§è¡Œé”™è¯¯
        
        Args:
            error: é”™è¯¯ä¿¡æ¯
        """
        self.log_error(
            f"æ­¥éª¤ {self.step_name} æ‰§è¡Œå¤±è´¥",
            error=str(error),
            exc_info=True
        )
        
        # æ ¹æ®é”™è¯¯ç±»å‹æŠ›å‡ºç›¸åº”çš„å¼‚å¸¸
        if isinstance(error, (ValidationError, ExecutionError)):
            raise error
        else:
            raise ExecutionError(f"æ­¥éª¤ {self.step_name} æ‰§è¡Œå¤±è´¥: {error}")
    
    def _validate_required_config(self, required_fields: List[str]) -> None:
        """
        éªŒè¯å¿…éœ€çš„é…ç½®å­—æ®µ
        
        Args:
            required_fields: å¿…éœ€å­—æ®µåˆ—è¡¨
        """
        for field in required_fields:
            if field not in self.config:
                raise ValidationError(
                    f"æ­¥éª¤ {self.step_name} ç¼ºå°‘å¿…éœ€é…ç½®: {field}"
                )
    
    def _get_config_value(self, key: str, default: Any = None) -> Any:
        """
        è·å–é…ç½®å€¼
        
        Args:
            key: é…ç½®é”®
            default: é»˜è®¤å€¼
            
        Returns:
            é…ç½®å€¼
        """
        return self.config.get(key, default)
    
    async def _execute_with_timing(self, context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        å¸¦è®¡æ—¶çš„æ‰§è¡Œæ–¹æ³•
        
        Args:
            context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            æ‰§è¡Œç»“æœ
        """
        start_time = time.time()
        
        try:
            self.log_info(f"å¼€å§‹æ‰§è¡Œæ­¥éª¤: {self.step_name}")
            
            # å‡†å¤‡ä¸Šä¸‹æ–‡
            prepared_context = self._prepare_context(context)
            
            # æ‰§è¡Œæ­¥éª¤
            result = await self.execute(prepared_context)
            
            end_time = time.time()
            
            # è®°å½•æ‰§è¡Œæ—¥å¿—
            self._log_execution(
                start_time=start_time,
                end_time=end_time,
                result_count=len(result) if result else 0
            )
            
            return result
            
        except Exception as e:
            end_time = time.time()
            self._handle_error(e)
    
    def get_step_type(self) -> str:
        """
        è·å–æ­¥éª¤ç±»å‹
        
        Returns:
            æ­¥éª¤ç±»å‹
        """
        # ä»ç±»åæ¨æ–­æ­¥éª¤ç±»å‹
        class_name = self.__class__.__name__
        if class_name.endswith("Step"):
            step_type = class_name[:-4].lower()
        else:
            step_type = class_name.lower()
        
        return step_type
    
    def get_dependencies(self) -> List[str]:
        """
        è·å–æ­¥éª¤ä¾èµ–
        
        Returns:
            ä¾èµ–æ­¥éª¤åˆ—è¡¨
        """
        dependencies = []
        
        # æ£€æŸ¥sourceé…ç½®
        source = self.config.get("source")
        if source:
            if isinstance(source, str):
                dependencies.append(source)
            elif isinstance(source, list):
                dependencies.extend(source)
        
        # æ£€æŸ¥sourcesé…ç½®
        sources = self.config.get("sources")
        if sources and isinstance(sources, list):
            dependencies.extend(sources)
        
        return dependencies
    
    def __str__(self) -> str:
        """å­—ç¬¦ä¸²è¡¨ç¤º"""
        return f"{self.__class__.__name__}(name={self.step_name})"
    
    def __repr__(self) -> str:
        """å¯¹è±¡è¡¨ç¤º"""
        return f"{self.__class__.__name__}(name={self.step_name}, config={self.config})"

================
File: uqm-backend/src/steps/enrich_step.py
================
"""
æ•°æ®ä¸°å¯ŒåŒ–æ­¥éª¤å®ç°
é€šè¿‡æŸ¥æ‰¾è¡¨æ¥ä¸°å¯Œæºæ•°æ®
"""

from typing import Any, Dict, List, Optional, Union
import pandas as pd

from src.steps.base import BaseStep
from src.utils.exceptions import ValidationError, ExecutionError


class EnrichStep(BaseStep):
    """ä¸°å¯ŒåŒ–æ­¥éª¤æ‰§è¡Œå™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–ä¸°å¯ŒåŒ–æ­¥éª¤
        
        Args:
            config: ä¸°å¯ŒåŒ–æ­¥éª¤é…ç½®
        """
        super().__init__(config)
    
    def validate(self) -> None:
        """éªŒè¯ä¸°å¯ŒåŒ–æ­¥éª¤é…ç½®"""
        required_fields = ["source", "lookup", "on"]
        self._validate_required_config(required_fields)
        
        # éªŒè¯sourceå­—æ®µ
        source = self.config.get("source")
        if not isinstance(source, str):
            raise ValidationError("sourceå¿…é¡»æ˜¯å­—ç¬¦ä¸²")
        
        # éªŒè¯lookupå­—æ®µ
        lookup = self.config.get("lookup")
        if not isinstance(lookup, (str, dict)):
            raise ValidationError("lookupå¿…é¡»æ˜¯å­—ç¬¦ä¸²æˆ–å¯¹è±¡")
        
        # éªŒè¯onå­—æ®µ
        on = self.config.get("on")
        if not isinstance(on, (str, dict, list)):
            raise ValidationError("onå¿…é¡»æ˜¯å­—ç¬¦ä¸²ã€å¯¹è±¡æˆ–æ•°ç»„")
    
    async def execute(self, context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œä¸°å¯ŒåŒ–æ­¥éª¤
        
        Args:
            context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            ä¸°å¯ŒåŒ–åçš„æ•°æ®
        """
        try:
            # è·å–æºæ•°æ®
            source_name = self.config["source"]
            source_data = context["get_source_data"](source_name)
            
            # è·å–æŸ¥æ‰¾è¡¨æ•°æ®
            lookup_data = await self._fetch_lookup_data(context)
            
            # æ‰§è¡Œæ•°æ®ä¸°å¯ŒåŒ–
            enriched_data = self._perform_enrichment(source_data, lookup_data)
            
            return enriched_data
            
        except Exception as e:
            self.log_error("ä¸°å¯ŒåŒ–æ­¥éª¤æ‰§è¡Œå¤±è´¥", error=str(e))
            raise ExecutionError(f"ä¸°å¯ŒåŒ–æ‰§è¡Œå¤±è´¥: {e}")
    
    async def _fetch_lookup_data(self, context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        è·å–æŸ¥æ‰¾è¡¨æ•°æ®
        
        Args:
            context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            æŸ¥æ‰¾è¡¨æ•°æ®
        """
        lookup_config = self.config["lookup"]
        
        if isinstance(lookup_config, str):
            # ä»å…¶ä»–æ­¥éª¤è·å–æ•°æ®
            return context["get_source_data"](lookup_config)
        
        elif isinstance(lookup_config, dict):
            # ä»æ•°æ®åº“è¡¨è·å–æ•°æ®
            table_name = lookup_config.get("table")
            if not table_name:
                raise ValidationError("lookupé…ç½®ä¸­ç¼ºå°‘tableå­—æ®µ")
            
            # æ„å»ºæŸ¥è¯¢
            columns = lookup_config.get("columns", ["*"])
            where_conditions = lookup_config.get("where", [])
            
            # ä½¿ç”¨SQLæ„å»ºå™¨æ„å»ºæŸ¥è¯¢
            from src.utils.sql_builder import SQLBuilder
            sql_builder = SQLBuilder()
            
            query = sql_builder.build_select_query(
                select_fields=columns,
                from_table=table_name,
                where_conditions=where_conditions
            )
            
            # æ‰§è¡ŒæŸ¥è¯¢
            connector_manager = context["connector_manager"]
            connector = await connector_manager.get_default_connector()
            
            return await connector.execute_query(query)
        
        else:
            raise ValidationError("æ— æ•ˆçš„lookupé…ç½®")
    
    def _perform_enrichment(self, source_data: List[Dict[str, Any]], 
                           lookup_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œæ•°æ®ä¸°å¯ŒåŒ–
        
        Args:
            source_data: æºæ•°æ®
            lookup_data: æŸ¥æ‰¾è¡¨æ•°æ®
            
        Returns:
            ä¸°å¯ŒåŒ–åçš„æ•°æ®
        """
        try:
            if not source_data:
                return []
            
            if not lookup_data:
                self.log_warning("æŸ¥æ‰¾è¡¨æ•°æ®ä¸ºç©ºï¼Œè¿”å›åŸå§‹æ•°æ®")
                return source_data
            
            # è½¬æ¢ä¸ºDataFrameè¿›è¡Œå¤„ç†
            source_df = pd.DataFrame(source_data)
            lookup_df = pd.DataFrame(lookup_data)
            
            # è§£æè¿æ¥æ¡ä»¶
            join_config = self._parse_join_config()
            
            # æ‰§è¡Œè¿æ¥
            result_df = self._perform_join(source_df, lookup_df, join_config)
            
            # è½¬æ¢å›å­—å…¸åˆ—è¡¨
            return result_df.to_dict('records')
            
        except Exception as e:
            self.log_error("æ‰§è¡Œæ•°æ®ä¸°å¯ŒåŒ–å¤±è´¥", error=str(e))
            raise ExecutionError(f"æ•°æ®ä¸°å¯ŒåŒ–å¤±è´¥: {e}")
    
    def _parse_join_config(self) -> Dict[str, Any]:
        """
        è§£æè¿æ¥é…ç½®
        
        Returns:
            è¿æ¥é…ç½®å­—å…¸
        """
        on_config = self.config["on"]
        join_type = self.config.get("join_type", "left")
        
        if isinstance(on_config, str):
            # ç®€å•å­—æ®µè¿æ¥
            return {
                "type": join_type,
                "left_on": on_config,
                "right_on": on_config
            }
        
        elif isinstance(on_config, dict):
            # å¤æ‚è¿æ¥é…ç½®
            return {
                "type": join_type,
                "left_on": on_config.get("left"),
                "right_on": on_config.get("right"),
                "condition": on_config.get("condition")
            }
        
        elif isinstance(on_config, list):
            # å¤šå­—æ®µè¿æ¥
            return {
                "type": join_type,
                "left_on": on_config,
                "right_on": on_config
            }
        
        else:
            raise ValidationError("æ— æ•ˆçš„è¿æ¥é…ç½®")
    
    def _perform_join(self, source_df: pd.DataFrame, 
                     lookup_df: pd.DataFrame,
                     join_config: Dict[str, Any]) -> pd.DataFrame:
        """
        æ‰§è¡ŒDataFrameè¿æ¥
        
        Args:
            source_df: æºæ•°æ®DataFrame
            lookup_df: æŸ¥æ‰¾è¡¨DataFrame
            join_config: è¿æ¥é…ç½®
            
        Returns:
            è¿æ¥åçš„DataFrame
        """
        join_type = join_config["type"]
        left_on = join_config["left_on"]
        right_on = join_config["right_on"]
        
        # éªŒè¯è¿æ¥é”®æ˜¯å¦å­˜åœ¨
        self._validate_join_keys(source_df, lookup_df, left_on, right_on)
        
        # æ ¹æ®è¿æ¥ç±»å‹æ‰§è¡Œè¿æ¥
        if join_type.lower() == "left":
            how = "left"
        elif join_type.lower() == "right":
            how = "right"
        elif join_type.lower() == "inner":
            how = "inner"
        elif join_type.lower() == "outer":
            how = "outer"
        else:
            how = "left"  # é»˜è®¤å·¦è¿æ¥
        
        # æ‰§è¡Œè¿æ¥
        result_df = source_df.merge(
            lookup_df,
            left_on=left_on,
            right_on=right_on,
            how=how,
            suffixes=('', '_lookup')
        )
        
        # å¤„ç†åˆ—åå†²çª
        result_df = self._handle_column_conflicts(result_df)
        
        return result_df
    
    def _validate_join_keys(self, source_df: pd.DataFrame, 
                           lookup_df: pd.DataFrame,
                           left_on: Union[str, List[str]],
                           right_on: Union[str, List[str]]) -> None:
        """
        éªŒè¯è¿æ¥é”®æ˜¯å¦å­˜åœ¨
        
        Args:
            source_df: æºæ•°æ®DataFrame
            lookup_df: æŸ¥æ‰¾è¡¨DataFrame
            left_on: å·¦è¿æ¥é”®
            right_on: å³è¿æ¥é”®
        """
        # éªŒè¯å·¦è¿æ¥é”®
        if isinstance(left_on, str):
            left_keys = [left_on]
        else:
            left_keys = left_on
        
        for key in left_keys:
            if key not in source_df.columns:
                raise ValidationError(f"æºæ•°æ®ä¸­ä¸å­˜åœ¨è¿æ¥é”®: {key}")
        
        # éªŒè¯å³è¿æ¥é”®
        if isinstance(right_on, str):
            right_keys = [right_on]
        else:
            right_keys = right_on
        
        for key in right_keys:
            if key not in lookup_df.columns:
                raise ValidationError(f"æŸ¥æ‰¾è¡¨ä¸­ä¸å­˜åœ¨è¿æ¥é”®: {key}")
    
    def _handle_column_conflicts(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        å¤„ç†åˆ—åå†²çª
        
        Args:
            df: åŸå§‹DataFrame
            
        Returns:
            å¤„ç†åçš„DataFrame
        """
        # è·å–éœ€è¦é‡å‘½åçš„åˆ—
        columns_to_rename = {}
        for col in df.columns:
            if col.endswith('_lookup'):
                original_name = col[:-7]  # ç§»é™¤'_lookup'åç¼€
                new_name = self._get_unique_column_name(df, original_name)
                columns_to_rename[col] = new_name
        
        # é‡å‘½ååˆ—
        if columns_to_rename:
            df = df.rename(columns=columns_to_rename)
        
        return df
    
    def _get_unique_column_name(self, df: pd.DataFrame, base_name: str) -> str:
        """
        è·å–å”¯ä¸€çš„åˆ—å
        
        Args:
            df: DataFrame
            base_name: åŸºç¡€åç§°
            
        Returns:
            å”¯ä¸€çš„åˆ—å
        """
        if base_name not in df.columns:
            return base_name
        
        counter = 1
        while True:
            new_name = f"{base_name}_{counter}"
            if new_name not in df.columns:
                return new_name
            counter += 1
    
    def _optimize_lookup_strategy(self, source_size: int, lookup_size: int) -> str:
        """
        ä¼˜åŒ–æŸ¥æ‰¾ç­–ç•¥
        
        Args:
            source_size: æºæ•°æ®å¤§å°
            lookup_size: æŸ¥æ‰¾è¡¨å¤§å°
            
        Returns:
            ä¼˜åŒ–ç­–ç•¥
        """
        # æ ¹æ®æ•°æ®å¤§å°é€‰æ‹©ä¸åŒçš„ç­–ç•¥
        if source_size < 1000 and lookup_size < 1000:
            return "memory_join"
        elif lookup_size < 10000:
            return "hash_join"
        else:
            return "merge_join"
    
    def _handle_missing_keys(self, result_df: pd.DataFrame) -> pd.DataFrame:
        """
        å¤„ç†ç¼ºå¤±é”®å€¼
        
        Args:
            result_df: ç»“æœDataFrame
            
        Returns:
            å¤„ç†åçš„DataFrame
        """
        missing_strategy = self.config.get("missing_strategy", "keep")
        
        if missing_strategy == "drop":
            # åˆ é™¤åŒ…å«ç¼ºå¤±å€¼çš„è¡Œ
            result_df = result_df.dropna()
        elif missing_strategy == "fill":
            # ç”¨é»˜è®¤å€¼å¡«å……ç¼ºå¤±å€¼
            fill_value = self.config.get("fill_value", "")
            result_df = result_df.fillna(fill_value)
        # "keep"ç­–ç•¥ï¼šä¿æŒç¼ºå¤±å€¼ä¸å˜
        
        return result_df

================
File: uqm-backend/src/steps/pivot_step.py
================
"""
æ•°æ®é€è§†æ­¥éª¤å®ç°
å°†æ•°æ®ä»é•¿æ ¼å¼è½¬æ¢ä¸ºå®½æ ¼å¼
"""

from typing import Any, Dict, List, Optional, Union
import pandas as pd
import numpy as np

from src.steps.base import BaseStep
from src.utils.exceptions import ValidationError, ExecutionError


class PivotStep(BaseStep):
    """é€è§†æ­¥éª¤æ‰§è¡Œå™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–é€è§†æ­¥éª¤
        
        Args:
            config: é€è§†æ­¥éª¤é…ç½®
        """
        super().__init__(config)
        
        # æ”¯æŒçš„èšåˆå‡½æ•°
        self.supported_agg_functions = {
            'sum': np.sum,
            'mean': np.mean,
            'avg': np.mean,
            'count': np.size,
            'min': np.min,
            'max': np.max,
            'std': np.std,
            'var': np.var,
            'first': lambda x: x.iloc[0] if len(x) > 0 else None,
            'last': lambda x: x.iloc[-1] if len(x) > 0 else None
        }
    
    def validate(self) -> None:
        """éªŒè¯é€è§†æ­¥éª¤é…ç½®"""
        required_fields = ["source", "index", "columns", "values"]
        self._validate_required_config(required_fields)
        
        # éªŒè¯sourceå­—æ®µ
        source = self.config.get("source")
        if not isinstance(source, str):
            raise ValidationError("sourceå¿…é¡»æ˜¯å­—ç¬¦ä¸²")
        
        # éªŒè¯indexå­—æ®µ
        index = self.config.get("index")
        if not isinstance(index, (str, list)):
            raise ValidationError("indexå¿…é¡»æ˜¯å­—ç¬¦ä¸²æˆ–æ•°ç»„")
        
        # éªŒè¯columnså­—æ®µ
        columns = self.config.get("columns")
        if not isinstance(columns, (str, list)):
            raise ValidationError("columnså¿…é¡»æ˜¯å­—ç¬¦ä¸²æˆ–æ•°ç»„")
        
        # éªŒè¯valueså­—æ®µ
        values = self.config.get("values")
        if not isinstance(values, (str, list)):
            raise ValidationError("valueså¿…é¡»æ˜¯å­—ç¬¦ä¸²æˆ–æ•°ç»„")
        
        # éªŒè¯èšåˆå‡½æ•°
        agg_func = self.config.get("agg_func", "sum")
        if isinstance(agg_func, str):
            if agg_func.lower() not in self.supported_agg_functions:
                raise ValidationError(f"ä¸æ”¯æŒçš„èšåˆå‡½æ•°: {agg_func}")
        elif isinstance(agg_func, dict):
            for func in agg_func.values():
                if func.lower() not in self.supported_agg_functions:
                    raise ValidationError(f"ä¸æ”¯æŒçš„èšåˆå‡½æ•°: {func}")
    
    async def execute(self, context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œé€è§†æ­¥éª¤
        
        Args:
            context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            é€è§†åçš„æ•°æ®
        """
        try:
            # è·å–æºæ•°æ®
            source_name = self.config["source"]
            source_data = context["get_source_data"](source_name)
            
            if not source_data:
                self.log_warning("æºæ•°æ®ä¸ºç©º")
                return []
            
            # æ‰§è¡Œæ•°æ®é€è§†
            pivoted_data = self._perform_pivot(source_data)
            
            return pivoted_data
            
        except Exception as e:
            self.log_error("é€è§†æ­¥éª¤æ‰§è¡Œå¤±è´¥", error=str(e))
            raise ExecutionError(f"é€è§†æ‰§è¡Œå¤±è´¥: {e}")
    
    def _perform_pivot(self, source_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œæ•°æ®é€è§†
        
        Args:
            source_data: æºæ•°æ®
            
        Returns:
            é€è§†åçš„æ•°æ®
        """
        try:
            # è½¬æ¢ä¸ºDataFrame
            df = pd.DataFrame(source_data)
            
            # è·å–é€è§†å‚æ•°
            index = self.config["index"]
            columns = self.config["columns"]
            values = self.config["values"]
            agg_func = self.config.get("agg_func", "sum")
            fill_value = self.config.get("fill_value", 0)
            
            # éªŒè¯é€è§†åˆ—æ˜¯å¦å­˜åœ¨
            self._validate_pivot_columns(df, index, columns, values)
            
            # å‡†å¤‡é€è§†æ•°æ®
            df_clean = self._prepare_pivot_data(df, index, columns, values)
            
            # æ‰§è¡Œé€è§†
            if isinstance(agg_func, str):
                # å•ä¸€èšåˆå‡½æ•°
                agg_function = self.supported_agg_functions[agg_func.lower()]
                pivot_df = df_clean.pivot_table(
                    index=index,
                    columns=columns,
                    values=values,
                    aggfunc=agg_function,
                    fill_value=fill_value
                )
            else:
                # å¤šä¸ªèšåˆå‡½æ•°
                agg_functions = {
                    val: self.supported_agg_functions[func.lower()]
                    for val, func in agg_func.items()
                }
                pivot_df = df_clean.pivot_table(
                    index=index,
                    columns=columns,
                    values=list(agg_functions.keys()),
                    aggfunc=agg_functions,
                    fill_value=fill_value
                )
            
            # å¤„ç†å¤šçº§åˆ—å
            pivot_df = self._flatten_column_names(pivot_df)
            
            # é‡ç½®ç´¢å¼•
            pivot_df = pivot_df.reset_index()
            
            # è½¬æ¢å›å­—å…¸åˆ—è¡¨
            result = pivot_df.to_dict('records')
            
            self.log_info(
                "é€è§†æ“ä½œå®Œæˆ",
                original_rows=len(df),
                pivoted_rows=len(result),
                pivoted_columns=len(pivot_df.columns)
            )
            
            return result
            
        except Exception as e:
            self.log_error("æ‰§è¡Œé€è§†æ“ä½œå¤±è´¥", error=str(e))
            raise ExecutionError(f"é€è§†æ“ä½œå¤±è´¥: {e}")
    
    def _validate_pivot_columns(self, df: pd.DataFrame, 
                               index: Union[str, List[str]],
                               columns: Union[str, List[str]],
                               values: Union[str, List[str]]) -> None:
        """
        éªŒè¯é€è§†åˆ—æ˜¯å¦å­˜åœ¨
        
        Args:
            df: æ•°æ®DataFrame
            index: ç´¢å¼•åˆ—
            columns: é€è§†åˆ—
            values: å€¼åˆ—
        """
        # æ”¶é›†æ‰€æœ‰éœ€è¦çš„åˆ—
        required_columns = []
        
        if isinstance(index, str):
            required_columns.append(index)
        else:
            required_columns.extend(index)
        
        if isinstance(columns, str):
            required_columns.append(columns)
        else:
            required_columns.extend(columns)
        
        if isinstance(values, str):
            required_columns.append(values)
        else:
            required_columns.extend(values)
        
        # éªŒè¯åˆ—æ˜¯å¦å­˜åœ¨
        missing_columns = []
        for col in required_columns:
            if col not in df.columns:
                missing_columns.append(col)
        
        if missing_columns:
            raise ValidationError(f"æ•°æ®ä¸­ç¼ºå°‘ä»¥ä¸‹åˆ—: {missing_columns}")
    
    def _prepare_pivot_data(self, df: pd.DataFrame,
                           index: Union[str, List[str]],
                           columns: Union[str, List[str]],
                           values: Union[str, List[str]]) -> pd.DataFrame:
        """
        å‡†å¤‡é€è§†æ•°æ®
        
        Args:
            df: åŸå§‹DataFrame
            index: ç´¢å¼•åˆ—
            columns: é€è§†åˆ—
            values: å€¼åˆ—
            
        Returns:
            å‡†å¤‡å¥½çš„DataFrame
        """
        # é€‰æ‹©éœ€è¦çš„åˆ—
        required_columns = []
        
        if isinstance(index, str):
            required_columns.append(index)
        else:
            required_columns.extend(index)
        
        if isinstance(columns, str):
            required_columns.append(columns)
        else:
            required_columns.extend(columns)
        
        if isinstance(values, str):
            required_columns.append(values)
        else:
            required_columns.extend(values)
        
        # å»é‡åˆ—å
        required_columns = list(set(required_columns))
        
        # é€‰æ‹©åˆ—
        df_clean = df[required_columns].copy()
        
        # å¤„ç†ç¼ºå¤±å€¼
        missing_strategy = self.config.get("missing_strategy", "drop")
        if missing_strategy == "drop":
            df_clean = df_clean.dropna()
        elif missing_strategy == "fill":
            fill_value = self.config.get("missing_fill_value", 0)
            df_clean = df_clean.fillna(fill_value)
        
        # å¤„ç†æ•°æ®ç±»å‹
        if isinstance(values, str):
            value_columns = [values]
        else:
            value_columns = values
        
        for col in value_columns:
            if col in df_clean.columns:
                # å°è¯•è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
                df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')
        
        return df_clean
    
    def _flatten_column_names(self, pivot_df: pd.DataFrame) -> pd.DataFrame:
        """
        å±•å¹³å¤šçº§åˆ—å
        
        Args:
            pivot_df: é€è§†åçš„DataFrame
            
        Returns:
            åˆ—åå±•å¹³åçš„DataFrame
        """
        if isinstance(pivot_df.columns, pd.MultiIndex):
            # å¤„ç†å¤šçº§åˆ—å
            new_columns = []
            for col in pivot_df.columns:
                if isinstance(col, tuple):
                    # ç»„åˆåˆ—å
                    col_name = "_".join([str(c) for c in col if str(c) != ""])
                    new_columns.append(col_name)
                else:
                    new_columns.append(str(col))
            
            pivot_df.columns = new_columns
        
        return pivot_df
    
    def _handle_null_values(self, pivot_df: pd.DataFrame) -> pd.DataFrame:
        """
        å¤„ç†é€è§†åçš„ç©ºå€¼
        
        Args:
            pivot_df: é€è§†åçš„DataFrame
            
        Returns:
            å¤„ç†ç©ºå€¼åçš„DataFrame
        """
        null_strategy = self.config.get("null_strategy", "keep")
        
        if null_strategy == "drop":
            # åˆ é™¤åŒ…å«ç©ºå€¼çš„è¡Œ
            pivot_df = pivot_df.dropna()
        elif null_strategy == "fill":
            # ç”¨æŒ‡å®šå€¼å¡«å……ç©ºå€¼
            fill_value = self.config.get("null_fill_value", 0)
            pivot_df = pivot_df.fillna(fill_value)
        elif null_strategy == "zero":
            # ç”¨0å¡«å……ç©ºå€¼
            pivot_df = pivot_df.fillna(0)
        # "keep"ç­–ç•¥ï¼šä¿æŒç©ºå€¼ä¸å˜
        
        return pivot_df
    
    def _format_pivot_result(self, pivot_df: pd.DataFrame) -> pd.DataFrame:
        """
        æ ¼å¼åŒ–é€è§†ç»“æœ
        
        Args:
            pivot_df: é€è§†åçš„DataFrame
            
        Returns:
            æ ¼å¼åŒ–åçš„DataFrame
        """
        # å¤„ç†åˆ—å
        column_prefix = self.config.get("column_prefix", "")
        column_suffix = self.config.get("column_suffix", "")
        
        if column_prefix or column_suffix:
            new_columns = {}
            for col in pivot_df.columns:
                if col not in self.config.get("index", []):
                    new_col = f"{column_prefix}{col}{column_suffix}"
                    new_columns[col] = new_col
            
            if new_columns:
                pivot_df = pivot_df.rename(columns=new_columns)
        
        # æ’åº
        sort_by = self.config.get("sort_by")
        if sort_by:
            if isinstance(sort_by, str):
                sort_columns = [sort_by]
            else:
                sort_columns = sort_by
            
            # éªŒè¯æ’åºåˆ—æ˜¯å¦å­˜åœ¨
            valid_sort_columns = [col for col in sort_columns if col in pivot_df.columns]
            if valid_sort_columns:
                sort_ascending = self.config.get("sort_ascending", True)
                pivot_df = pivot_df.sort_values(valid_sort_columns, ascending=sort_ascending)
        
        return pivot_df

================
File: uqm-backend/src/steps/query_step.py
================
"""
æŸ¥è¯¢æ­¥éª¤å®ç°
è´Ÿè´£æ‰§è¡ŒSQLæŸ¥è¯¢å¹¶è¿”å›ç»“æœ
"""

from typing import Any, Dict, List, Optional, Union

from src.steps.base import BaseStep
from src.utils.sql_builder import SQLBuilder
from src.utils.exceptions import ValidationError, ExecutionError


class QueryStep(BaseStep):
    """æŸ¥è¯¢æ­¥éª¤æ‰§è¡Œå™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–æŸ¥è¯¢æ­¥éª¤
        
        Args:
            config: æŸ¥è¯¢æ­¥éª¤é…ç½®
        """
        super().__init__(config)
        self.sql_builder = SQLBuilder()
    
    def validate(self) -> None:
        """éªŒè¯æŸ¥è¯¢æ­¥éª¤é…ç½®"""
        required_fields = ["data_source"]
        self._validate_required_config(required_fields)
        
        # éªŒè¯æ•°æ®æº
        data_source = self.config.get("data_source")
        if not isinstance(data_source, str):
            raise ValidationError("data_sourceå¿…é¡»æ˜¯å­—ç¬¦ä¸²")
        
        # éªŒè¯ç»´åº¦å­—æ®µ
        dimensions = self.config.get("dimensions", [])
        if not isinstance(dimensions, list):
            raise ValidationError("dimensionså¿…é¡»æ˜¯æ•°ç»„")
        
        # éªŒè¯æŒ‡æ ‡å­—æ®µ
        metrics = self.config.get("metrics", [])
        if not isinstance(metrics, list):
            raise ValidationError("metricså¿…é¡»æ˜¯æ•°ç»„")
        
        # è‡³å°‘éœ€è¦æœ‰ç»´åº¦æˆ–æŒ‡æ ‡
        if not dimensions and not metrics:
            raise ValidationError("è‡³å°‘éœ€è¦æŒ‡å®šdimensionsæˆ–metrics")
    
    async def execute(self, context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        æ‰§è¡ŒæŸ¥è¯¢æ­¥éª¤
        
        Args:
            context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            æŸ¥è¯¢ç»“æœ
        """
        try:
            # è·å–è¿æ¥å™¨ç®¡ç†å™¨
            connector_manager = context["connector_manager"]
            
            # æ„å»ºSQLæŸ¥è¯¢
            query = self.build_query()
            
            # è·å–é»˜è®¤è¿æ¥å™¨ï¼ˆæˆ–æ ¹æ®é…ç½®é€‰æ‹©ç‰¹å®šè¿æ¥å™¨ï¼‰
            connector = await connector_manager.get_default_connector()
            
            # æ‰§è¡ŒæŸ¥è¯¢
            result = await connector.execute_query(query)
            
            return result
            
        except Exception as e:
            self.log_error("æŸ¥è¯¢æ­¥éª¤æ‰§è¡Œå¤±è´¥", error=str(e))
            raise ExecutionError(f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}")
    
    def build_query(self) -> str:
        """
        æ„å»ºSQLæŸ¥è¯¢
        
        Returns:
            SQLæŸ¥è¯¢è¯­å¥
        """
        try:
            data_source = self.config["data_source"]
            dimensions = self.config.get("dimensions", [])
            metrics = self.config.get("metrics", [])
            filters = self.config.get("filters", [])
            joins = self.config.get("joins", [])
            group_by = self.config.get("group_by", [])
            having = self.config.get("having", [])
            order_by = self.config.get("order_by", [])
            limit = self.config.get("limit")
            offset = self.config.get("offset")
            
            # æ„å»ºSELECTå­å¥
            select_fields = []
            
            # æ·»åŠ ç»´åº¦å­—æ®µ
            for dim in dimensions:
                if isinstance(dim, str):
                    select_fields.append(dim)
                elif isinstance(dim, dict):
                    field_expr = self._build_field_expression(dim)
                    select_fields.append(field_expr)
            
            # æ·»åŠ æŒ‡æ ‡å­—æ®µ
            for metric in metrics:
                if isinstance(metric, str):
                    select_fields.append(metric)
                elif isinstance(metric, dict):
                    metric_expr = self._build_metric_expression(metric)
                    select_fields.append(metric_expr)
            
            # æ·»åŠ è®¡ç®—å­—æ®µ
            calculated_fields = self.config.get("calculated_fields", [])
            for calc_field in calculated_fields:
                calc_expr = self._build_calculated_field(calc_field)
                select_fields.append(calc_expr)
            
            if not select_fields:
                select_fields = ["*"]
            
            # ä½¿ç”¨SQLæ„å»ºå™¨æ„å»ºæŸ¥è¯¢
            query = self.sql_builder.build_select_query(
                select_fields=select_fields,
                from_table=data_source,
                joins=joins,
                where_conditions=filters,
                group_by=group_by,
                having=having,
                order_by=order_by,
                limit=limit,
                offset=offset
            )
            
            self.log_debug("æ„å»ºçš„SQLæŸ¥è¯¢", query=query)
            return query
            
        except Exception as e:
            self.log_error("æ„å»ºSQLæŸ¥è¯¢å¤±è´¥", error=str(e))
            raise ValidationError(f"æ„å»ºæŸ¥è¯¢å¤±è´¥: {e}")
    
    def _build_field_expression(self, field_config: Dict[str, Any]) -> str:
        """
        æ„å»ºå­—æ®µè¡¨è¾¾å¼
        
        Args:
            field_config: å­—æ®µé…ç½®
            
        Returns:
            å­—æ®µè¡¨è¾¾å¼
        """
        name = field_config.get("name")
        alias = field_config.get("alias")
        expression = field_config.get("expression")
        
        if expression:
            # ä½¿ç”¨è‡ªå®šä¹‰è¡¨è¾¾å¼
            result = expression
        elif name:
            # ä½¿ç”¨å­—æ®µå
            result = name
        else:
            raise ValidationError("å­—æ®µé…ç½®å¿…é¡»åŒ…å«nameæˆ–expression")
        
        # æ·»åŠ åˆ«å
        if alias:
            result = f"{result} AS {alias}"
        
        return result
    
    def _build_metric_expression(self, metric_config: Dict[str, Any]) -> str:
        """
        æ„å»ºæŒ‡æ ‡è¡¨è¾¾å¼
        
        Args:
            metric_config: æŒ‡æ ‡é…ç½®
            
        Returns:
            æŒ‡æ ‡è¡¨è¾¾å¼
        """
        name = metric_config.get("name")
        alias = metric_config.get("alias", name)
        agg_function = metric_config.get("aggregation", "SUM")
        expression = metric_config.get("expression")
        
        if expression:
            # ä½¿ç”¨è‡ªå®šä¹‰è¡¨è¾¾å¼
            result = expression
        elif name:
            # ä½¿ç”¨èšåˆå‡½æ•°
            result = f"{agg_function}({name})"
        else:
            raise ValidationError("æŒ‡æ ‡é…ç½®å¿…é¡»åŒ…å«nameæˆ–expression")
        
        # æ·»åŠ åˆ«å
        if alias:
            result = f"{result} AS {alias}"
        
        return result
    
    def _build_calculated_field(self, calc_config: Dict[str, Any]) -> str:
        """
        æ„å»ºè®¡ç®—å­—æ®µè¡¨è¾¾å¼
        
        Args:
            calc_config: è®¡ç®—å­—æ®µé…ç½®
            
        Returns:
            è®¡ç®—å­—æ®µè¡¨è¾¾å¼
        """
        alias = calc_config.get("alias")
        expression = calc_config.get("expression")
        
        if not expression:
            raise ValidationError("è®¡ç®—å­—æ®µå¿…é¡»åŒ…å«expression")
        
        if not alias:
            raise ValidationError("è®¡ç®—å­—æ®µå¿…é¡»åŒ…å«alias")
        
        return f"{expression} AS {alias}"
    
    def _build_window_function(self, window_config: Dict[str, Any]) -> str:
        """
        æ„å»ºçª—å£å‡½æ•°è¡¨è¾¾å¼
        
        Args:
            window_config: çª—å£å‡½æ•°é…ç½®
            
        Returns:
            çª—å£å‡½æ•°è¡¨è¾¾å¼
        """
        function = window_config.get("function")
        partition_by = window_config.get("partition_by", [])
        order_by = window_config.get("order_by", [])
        alias = window_config.get("alias")
        
        if not function:
            raise ValidationError("çª—å£å‡½æ•°å¿…é¡»æŒ‡å®šfunction")
        
        # æ„å»ºçª—å£å‡½æ•°è¡¨è¾¾å¼
        window_expr = function
        
        # æ·»åŠ OVERå­å¥
        over_parts = []
        
        if partition_by:
            partition_clause = "PARTITION BY " + ", ".join(partition_by)
            over_parts.append(partition_clause)
        
        if order_by:
            if isinstance(order_by, list):
                order_clause = "ORDER BY " + ", ".join(order_by)
            else:
                order_clause = f"ORDER BY {order_by}"
            over_parts.append(order_clause)
        
        if over_parts:
            over_clause = " ".join(over_parts)
        else:
            over_clause = ""
        
        result = f"{window_expr} OVER ({over_clause})"
        
        # æ·»åŠ åˆ«å
        if alias:
            result = f"{result} AS {alias}"
        
        return result

================
File: uqm-backend/src/steps/union_step.py
================
"""
é›†åˆåˆå¹¶æ­¥éª¤å®ç°
è´Ÿè´£åˆå¹¶å¤šä¸ªæ•°æ®é›†
"""

from typing import Any, Dict, List, Optional, Union
import pandas as pd

from src.steps.base import BaseStep
from src.utils.exceptions import ValidationError, ExecutionError


class UnionStep(BaseStep):
    """åˆå¹¶æ­¥éª¤æ‰§è¡Œå™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–åˆå¹¶æ­¥éª¤
        
        Args:
            config: åˆå¹¶æ­¥éª¤é…ç½®
        """
        super().__init__(config)
    
    def validate(self) -> None:
        """éªŒè¯åˆå¹¶æ­¥éª¤é…ç½®"""
        required_fields = ["sources"]
        self._validate_required_config(required_fields)
        
        # éªŒè¯sourceså­—æ®µ
        sources = self.config.get("sources")
        if not isinstance(sources, list) or len(sources) < 2:
            raise ValidationError("sourceså¿…é¡»æ˜¯åŒ…å«è‡³å°‘2ä¸ªå…ƒç´ çš„æ•°ç»„")
        
        # éªŒè¯æ¨¡å¼
        mode = self.config.get("mode", "union")
        if mode not in ["union", "union_all", "intersect", "except"]:
            raise ValidationError("modeå¿…é¡»æ˜¯unionã€union_allã€intersectæˆ–exceptä¹‹ä¸€")
    
    async def execute(self, context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œåˆå¹¶æ­¥éª¤
        
        Args:
            context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            åˆå¹¶åçš„æ•°æ®
        """
        try:
            # è·å–æºæ•°æ®
            sources = self.config["sources"]
            source_datasets = context["get_source_data"](sources)
            
            # æ‰§è¡Œæ•°æ®åˆå¹¶
            result = self._perform_union(source_datasets)
            
            return result
            
        except Exception as e:
            self.log_error("åˆå¹¶æ­¥éª¤æ‰§è¡Œå¤±è´¥", error=str(e))
            raise ExecutionError(f"åˆå¹¶æ‰§è¡Œå¤±è´¥: {e}")
    
    def _perform_union(self, source_datasets: Dict[str, List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œæ•°æ®åˆå¹¶
        
        Args:
            source_datasets: æºæ•°æ®é›†å­—å…¸
            
        Returns:
            åˆå¹¶åçš„æ•°æ®
        """
        try:
            mode = self.config.get("mode", "union")
            
            if mode == "union":
                return self._union_distinct(source_datasets)
            elif mode == "union_all":
                return self._union_all(source_datasets)
            elif mode == "intersect":
                return self._intersect(source_datasets)
            elif mode == "except":
                return self._except(source_datasets)
            else:
                raise ValidationError(f"ä¸æ”¯æŒçš„åˆå¹¶æ¨¡å¼: {mode}")
                
        except Exception as e:
            self.log_error("æ‰§è¡Œæ•°æ®åˆå¹¶å¤±è´¥", error=str(e))
            raise ExecutionError(f"æ•°æ®åˆå¹¶å¤±è´¥: {e}")
    
    def _union_all(self, source_datasets: Dict[str, List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
        """æ‰§è¡ŒUNION ALLæ“ä½œï¼ˆä¿ç•™é‡å¤ï¼‰"""
        all_data = []
        
        # éªŒè¯å¹¶å¯¹é½åˆ—ç»“æ„
        aligned_datasets = self._align_columns(source_datasets)
        
        for source_name, data in aligned_datasets.items():
            # ä¸ºæ¯æ¡è®°å½•æ·»åŠ æ¥æºæ ‡è¯†ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
            if self.config.get("add_source_column", False):
                source_column = self.config.get("source_column", "_source")
                for record in data:
                    record[source_column] = source_name
            
            all_data.extend(data)
        
        return all_data
    
    def _union_distinct(self, source_datasets: Dict[str, List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
        """æ‰§è¡ŒUNIONæ“ä½œï¼ˆå»é™¤é‡å¤ï¼‰"""
        # å…ˆæ‰§è¡ŒUNION ALL
        all_data = self._union_all(source_datasets)
        
        # å»é™¤é‡å¤æ•°æ®
        return self._remove_duplicates(all_data)
    
    def _intersect(self, source_datasets: Dict[str, List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
        """æ‰§è¡ŒINTERSECTæ“ä½œï¼ˆäº¤é›†ï¼‰"""
        if len(source_datasets) < 2:
            return []
        
        # è·å–ç¬¬ä¸€ä¸ªæ•°æ®é›†ä½œä¸ºåŸºå‡†
        datasets = list(source_datasets.values())
        result_set = set()
        
        # å°†ç¬¬ä¸€ä¸ªæ•°æ®é›†è½¬æ¢ä¸ºé›†åˆ
        for record in datasets[0]:
            record_tuple = self._record_to_tuple(record)
            result_set.add(record_tuple)
        
        # ä¸å…¶ä»–æ•°æ®é›†æ±‚äº¤é›†
        for dataset in datasets[1:]:
            dataset_set = set()
            for record in dataset:
                record_tuple = self._record_to_tuple(record)
                dataset_set.add(record_tuple)
            
            result_set = result_set.intersection(dataset_set)
        
        # è½¬æ¢å›å­—å…¸åˆ—è¡¨
        return [self._tuple_to_record(t, datasets[0][0].keys()) for t in result_set]
    
    def _except(self, source_datasets: Dict[str, List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
        """æ‰§è¡ŒEXCEPTæ“ä½œï¼ˆå·®é›†ï¼‰"""
        if len(source_datasets) < 2:
            return list(source_datasets.values())[0] if source_datasets else []
        
        datasets = list(source_datasets.values())
        
        # è·å–ç¬¬ä¸€ä¸ªæ•°æ®é›†
        result_set = set()
        for record in datasets[0]:
            record_tuple = self._record_to_tuple(record)
            result_set.add(record_tuple)
        
        # ä»ç»“æœä¸­ç§»é™¤å…¶ä»–æ•°æ®é›†çš„è®°å½•
        for dataset in datasets[1:]:
            for record in dataset:
                record_tuple = self._record_to_tuple(record)
                result_set.discard(record_tuple)
        
        # è½¬æ¢å›å­—å…¸åˆ—è¡¨
        return [self._tuple_to_record(t, datasets[0][0].keys()) for t in result_set]
    
    def _align_columns(self, source_datasets: Dict[str, List[Dict[str, Any]]]) -> Dict[str, List[Dict[str, Any]]]:
        """å¯¹é½åˆ—ç»“æ„"""
        if not source_datasets:
            return source_datasets
        
        # æ”¶é›†æ‰€æœ‰åˆ—å
        all_columns = set()
        for data in source_datasets.values():
            if data:
                all_columns.update(data[0].keys())
        
        # å¯¹é½æ¯ä¸ªæ•°æ®é›†çš„åˆ—
        aligned_datasets = {}
        for source_name, data in source_datasets.items():
            aligned_data = []
            for record in data:
                aligned_record = {}
                for col in all_columns:
                    aligned_record[col] = record.get(col, None)
                aligned_data.append(aligned_record)
            aligned_datasets[source_name] = aligned_data
        
        return aligned_datasets
    
    def _remove_duplicates(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ç§»é™¤é‡å¤æ•°æ®"""
        seen = set()
        unique_data = []
        
        for record in data:
            record_tuple = self._record_to_tuple(record)
            if record_tuple not in seen:
                seen.add(record_tuple)
                unique_data.append(record)
        
        return unique_data
    
    def _record_to_tuple(self, record: Dict[str, Any]) -> tuple:
        """å°†è®°å½•è½¬æ¢ä¸ºå…ƒç»„ï¼ˆç”¨äºé›†åˆæ“ä½œï¼‰"""
        return tuple(sorted(record.items()))
    
    def _tuple_to_record(self, record_tuple: tuple, columns: List[str]) -> Dict[str, Any]:
        """å°†å…ƒç»„è½¬æ¢å›è®°å½•"""
        return dict(record_tuple)
    
    def _validate_schema_compatibility(self, source_datasets: Dict[str, List[Dict[str, Any]]]) -> None:
        """éªŒè¯æ¨¡å¼å…¼å®¹æ€§"""
        if not source_datasets:
            return
        
        # è·å–ç¬¬ä¸€ä¸ªæ•°æ®é›†çš„åˆ—ç»“æ„ä½œä¸ºåŸºå‡†
        first_dataset = list(source_datasets.values())[0]
        if not first_dataset:
            return
        
        base_columns = set(first_dataset[0].keys())
        
        # æ£€æŸ¥å…¶ä»–æ•°æ®é›†çš„åˆ—ç»“æ„
        for source_name, data in source_datasets.items():
            if data:
                current_columns = set(data[0].keys())
                
                # æ ¹æ®é…ç½®å†³å®šå¦‚ä½•å¤„ç†åˆ—å·®å¼‚
                strict_mode = self.config.get("strict_schema", False)
                if strict_mode and current_columns != base_columns:
                    missing_in_current = base_columns - current_columns
                    extra_in_current = current_columns - base_columns
                    
                    error_msg = f"æ•°æ®é›† {source_name} çš„åˆ—ç»“æ„ä¸åŒ¹é…"
                    if missing_in_current:
                        error_msg += f"ï¼Œç¼ºå°‘åˆ—: {missing_in_current}"
                    if extra_in_current:
                        error_msg += f"ï¼Œå¤šä½™åˆ—: {extra_in_current}"
                    
                    raise ValidationError(error_msg)
    
    def _optimize_union_strategy(self, source_datasets: Dict[str, List[Dict[str, Any]]]) -> str:
        """ä¼˜åŒ–åˆå¹¶ç­–ç•¥"""
        total_records = sum(len(data) for data in source_datasets.values())
        dataset_count = len(source_datasets)
        
        if total_records < 10000 and dataset_count <= 5:
            return "memory_union"
        elif total_records < 100000:
            return "streaming_union"
        else:
            return "chunked_union"

================
File: uqm-backend/src/steps/unpivot_step.py
================
"""
æ•°æ®é€†é€è§†æ­¥éª¤å®ç°
å°†æ•°æ®ä»å®½æ ¼å¼è½¬æ¢ä¸ºé•¿æ ¼å¼
"""

from typing import Any, Dict, List, Optional, Union
import pandas as pd

from src.steps.base import BaseStep
from src.utils.exceptions import ValidationError, ExecutionError


class UnpivotStep(BaseStep):
    """é€†é€è§†æ­¥éª¤æ‰§è¡Œå™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        """åˆå§‹åŒ–é€†é€è§†æ­¥éª¤"""
        super().__init__(config)
    
    def validate(self) -> None:
        """éªŒè¯é€†é€è§†æ­¥éª¤é…ç½®"""
        required_fields = ["source", "id_vars", "value_vars"]
        self._validate_required_config(required_fields)
    
    async def execute(self, context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æ‰§è¡Œé€†é€è§†æ­¥éª¤"""
        try:
            # è·å–æºæ•°æ®
            source_name = self.config["source"]
            source_data = context["get_source_data"](source_name)
            
            if not source_data:
                return []
            
            # æ‰§è¡Œé€†é€è§†
            result = self._perform_unpivot(source_data)
            return result
            
        except Exception as e:
            self.log_error("é€†é€è§†æ­¥éª¤æ‰§è¡Œå¤±è´¥", error=str(e))
            raise ExecutionError(f"é€†é€è§†æ‰§è¡Œå¤±è´¥: {e}")
    
    def _perform_unpivot(self, source_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ‰§è¡Œé€†é€è§†æ“ä½œ"""
        df = pd.DataFrame(source_data)
        
        id_vars = self.config["id_vars"]
        value_vars = self.config["value_vars"]
        var_name = self.config.get("var_name", "variable")
        value_name = self.config.get("value_name", "value")
        
        # æ‰§è¡Œmeltæ“ä½œ
        melted_df = df.melt(
            id_vars=id_vars,
            value_vars=value_vars,
            var_name=var_name,
            value_name=value_name
        )
        
        return melted_df.to_dict('records')


class UnionStep(BaseStep):
    """åˆå¹¶æ­¥éª¤æ‰§è¡Œå™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        """åˆå§‹åŒ–åˆå¹¶æ­¥éª¤"""
        super().__init__(config)
    
    def validate(self) -> None:
        """éªŒè¯åˆå¹¶æ­¥éª¤é…ç½®"""
        required_fields = ["sources"]
        self._validate_required_config(required_fields)
    
    async def execute(self, context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æ‰§è¡Œåˆå¹¶æ­¥éª¤"""
        try:
            sources = self.config["sources"]
            source_datasets = context["get_source_data"](sources)
            
            # æ‰§è¡Œæ•°æ®åˆå¹¶
            result = self._perform_union(source_datasets)
            return result
            
        except Exception as e:
            self.log_error("åˆå¹¶æ­¥éª¤æ‰§è¡Œå¤±è´¥", error=str(e))
            raise ExecutionError(f"åˆå¹¶æ‰§è¡Œå¤±è´¥: {e}")
    
    def _perform_union(self, source_datasets: Dict[str, List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
        """æ‰§è¡Œæ•°æ®åˆå¹¶"""
        all_data = []
        
        for source_name, data in source_datasets.items():
            # ä¸ºæ¯æ¡è®°å½•æ·»åŠ æ¥æºæ ‡è¯†
            if self.config.get("add_source_column", False):
                source_column = self.config.get("source_column", "source")
                for record in data:
                    record[source_column] = source_name
            
            all_data.extend(data)
        
        # å¤„ç†é‡å¤æ•°æ®
        if self.config.get("remove_duplicates", False):
            # ç®€å•å»é‡ï¼ˆè½¬æ¢ä¸ºå­—ç¬¦ä¸²æ¯”è¾ƒï¼‰
            seen = set()
            unique_data = []
            for record in all_data:
                record_str = str(sorted(record.items()))
                if record_str not in seen:
                    seen.add(record_str)
                    unique_data.append(record)
            return unique_data
        
        return all_data


class AssertStep(BaseStep):
    """æ–­è¨€æ­¥éª¤æ‰§è¡Œå™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        """åˆå§‹åŒ–æ–­è¨€æ­¥éª¤"""
        super().__init__(config)
    
    def validate(self) -> None:
        """éªŒè¯æ–­è¨€æ­¥éª¤é…ç½®"""
        required_fields = ["source", "assertions"]
        self._validate_required_config(required_fields)
    
    async def execute(self, context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æ‰§è¡Œæ–­è¨€æ­¥éª¤"""
        try:
            # è·å–æºæ•°æ®
            source_name = self.config["source"]
            source_data = context["get_source_data"](source_name)
            
            # æ‰§è¡Œæ–­è¨€æ£€æŸ¥
            self._perform_assertions(source_data)
            
            # æ–­è¨€é€šè¿‡ï¼Œè¿”å›åŸå§‹æ•°æ®
            return source_data
            
        except Exception as e:
            self.log_error("æ–­è¨€æ­¥éª¤æ‰§è¡Œå¤±è´¥", error=str(e))
            raise ExecutionError(f"æ–­è¨€æ‰§è¡Œå¤±è´¥: {e}")
    
    def _perform_assertions(self, source_data: List[Dict[str, Any]]) -> None:
        """æ‰§è¡Œæ–­è¨€æ£€æŸ¥"""
        assertions = self.config["assertions"]
        
        for assertion in assertions:
            assertion_type = assertion.get("type")
            
            if assertion_type == "row_count":
                self._assert_row_count(source_data, assertion)
            elif assertion_type == "not_null":
                self._assert_not_null(source_data, assertion)
            elif assertion_type == "unique":
                self._assert_unique(source_data, assertion)
            elif assertion_type == "range":
                self._assert_range(source_data, assertion)
            else:
                self.log_warning(f"æœªçŸ¥çš„æ–­è¨€ç±»å‹: {assertion_type}")
    
    def _assert_row_count(self, data: List[Dict[str, Any]], assertion: Dict[str, Any]) -> None:
        """æ–­è¨€è¡Œæ•°"""
        expected_count = assertion.get("expected")
        min_count = assertion.get("min")
        max_count = assertion.get("max")
        
        actual_count = len(data)
        
        if expected_count is not None and actual_count != expected_count:
            raise ExecutionError(f"è¡Œæ•°æ–­è¨€å¤±è´¥: æœŸæœ›{expected_count}ï¼Œå®é™…{actual_count}")
        
        if min_count is not None and actual_count < min_count:
            raise ExecutionError(f"æœ€å°è¡Œæ•°æ–­è¨€å¤±è´¥: æœŸæœ›è‡³å°‘{min_count}ï¼Œå®é™…{actual_count}")
        
        if max_count is not None and actual_count > max_count:
            raise ExecutionError(f"æœ€å¤§è¡Œæ•°æ–­è¨€å¤±è´¥: æœŸæœ›æœ€å¤š{max_count}ï¼Œå®é™…{actual_count}")
    
    def _assert_not_null(self, data: List[Dict[str, Any]], assertion: Dict[str, Any]) -> None:
        """æ–­è¨€éç©º"""
        columns = assertion.get("columns", [])
        
        for record in data:
            for column in columns:
                if column in record and (record[column] is None or record[column] == ""):
                    raise ExecutionError(f"éç©ºæ–­è¨€å¤±è´¥: åˆ— {column} åŒ…å«ç©ºå€¼")
    
    def _assert_unique(self, data: List[Dict[str, Any]], assertion: Dict[str, Any]) -> None:
        """æ–­è¨€å”¯ä¸€æ€§"""
        columns = assertion.get("columns", [])
        
        seen_values = set()
        for record in data:
            key_values = tuple(record.get(col) for col in columns)
            if key_values in seen_values:
                raise ExecutionError(f"å”¯ä¸€æ€§æ–­è¨€å¤±è´¥: åˆ— {columns} å­˜åœ¨é‡å¤å€¼")
            seen_values.add(key_values)
    
    def _assert_range(self, data: List[Dict[str, Any]], assertion: Dict[str, Any]) -> None:
        """æ–­è¨€å€¼èŒƒå›´"""
        column = assertion.get("column")
        min_value = assertion.get("min")
        max_value = assertion.get("max")
        
        for record in data:
            if column in record:
                value = record[column]
                if isinstance(value, (int, float)):
                    if min_value is not None and value < min_value:
                        raise ExecutionError(f"èŒƒå›´æ–­è¨€å¤±è´¥: åˆ— {column} å€¼ {value} å°äºæœ€å°å€¼ {min_value}")
                    if max_value is not None and value > max_value:
                        raise ExecutionError(f"èŒƒå›´æ–­è¨€å¤±è´¥: åˆ— {column} å€¼ {value} å¤§äºæœ€å¤§å€¼ {max_value}")

================
File: uqm-backend/src/utils/__init__.py
================
"""å·¥å…·æ¨¡å—åŒ…åˆå§‹åŒ–æ–‡ä»¶"""

================
File: uqm-backend/src/utils/exceptions.py
================
"""
å¼‚å¸¸å¤„ç†æ¨¡å—
å®šä¹‰è‡ªå®šä¹‰å¼‚å¸¸ç±»å’Œå¼‚å¸¸å¤„ç†å™¨
"""

from typing import Any, Dict
from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import JSONResponse
from fastapi.exceptions import RequestValidationError
from starlette.exceptions import HTTPException as StarletteHTTPException

from src.utils.logging import get_logger

logger = get_logger(__name__)


class UQMBaseException(Exception):
    """UQMåŸºç¡€å¼‚å¸¸ç±»"""
    
    def __init__(self, message: str, error_code: str = None, details: Dict[str, Any] = None):
        self.message = message
        self.error_code = error_code or self.__class__.__name__
        self.details = details or {}
        super().__init__(self.message)


class ValidationError(UQMBaseException):
    """æ•°æ®éªŒè¯å¼‚å¸¸"""
    pass


class ExecutionError(UQMBaseException):
    """æ‰§è¡Œå¼‚å¸¸"""
    pass


class ConnectionError(UQMBaseException):
    """è¿æ¥å¼‚å¸¸"""
    pass


class CacheError(UQMBaseException):
    """ç¼“å­˜å¼‚å¸¸"""
    pass


class ParseError(UQMBaseException):
    """è§£æå¼‚å¸¸"""
    pass


class TimeoutError(UQMBaseException):
    """è¶…æ—¶å¼‚å¸¸"""
    pass


def setup_exception_handlers(app: FastAPI) -> None:
    """è®¾ç½®å…¨å±€å¼‚å¸¸å¤„ç†å™¨"""
    
    @app.exception_handler(UQMBaseException)
    async def uqm_exception_handler(request: Request, exc: UQMBaseException) -> JSONResponse:
        """UQMè‡ªå®šä¹‰å¼‚å¸¸å¤„ç†å™¨"""
        logger.error(
            "UQMå¼‚å¸¸",
            error_code=exc.error_code,
            message=exc.message,
            details=exc.details,
            path=request.url.path,
            method=request.method
        )
        
        return JSONResponse(
            status_code=400,
            content={
                "error": {
                    "code": exc.error_code,
                    "message": exc.message,
                    "details": exc.details
                }
            }
        )
    
    @app.exception_handler(RequestValidationError)
    async def validation_exception_handler(request: Request, exc: RequestValidationError) -> JSONResponse:
        """è¯·æ±‚éªŒè¯å¼‚å¸¸å¤„ç†å™¨"""
        logger.error(
            "è¯·æ±‚éªŒè¯å¼‚å¸¸",
            errors=exc.errors(),
            path=request.url.path,
            method=request.method
        )
        
        return JSONResponse(
            status_code=422,
            content={
                "error": {
                    "code": "VALIDATION_ERROR",
                    "message": "è¯·æ±‚æ•°æ®éªŒè¯å¤±è´¥",
                    "details": exc.errors()
                }
            }
        )
    
    @app.exception_handler(StarletteHTTPException)
    async def http_exception_handler(request: Request, exc: StarletteHTTPException) -> JSONResponse:
        """HTTPå¼‚å¸¸å¤„ç†å™¨"""
        logger.error(
            "HTTPå¼‚å¸¸",
            status_code=exc.status_code,
            detail=exc.detail,
            path=request.url.path,
            method=request.method
        )
        
        return JSONResponse(
            status_code=exc.status_code,
            content={
                "error": {
                    "code": f"HTTP_{exc.status_code}",
                    "message": exc.detail,
                    "details": {}
                }
            }
        )
    
    @app.exception_handler(Exception)
    async def general_exception_handler(request: Request, exc: Exception) -> JSONResponse:
        """é€šç”¨å¼‚å¸¸å¤„ç†å™¨"""
        logger.error(
            "æœªå¤„ç†å¼‚å¸¸",
            exception_type=type(exc).__name__,
            exception_message=str(exc),
            path=request.url.path,
            method=request.method,
            exc_info=True
        )
        
        return JSONResponse(
            status_code=500,
            content={
                "error": {
                    "code": "INTERNAL_SERVER_ERROR",
                    "message": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯",
                    "details": {}
                }
            }
        )

================
File: uqm-backend/src/utils/expression_parser.py
================
"""
è¡¨è¾¾å¼è§£æå™¨æ¨¡å—

æä¾›å®‰å…¨çš„è¡¨è¾¾å¼è§£æå’Œæ‰§è¡ŒåŠŸèƒ½ï¼Œæ”¯æŒæ•°æ®è½¬æ¢ã€è®¡ç®—ã€æ¡ä»¶åˆ¤æ–­ç­‰æ“ä½œã€‚
"""

import ast
import re
import operator
import math
import pandas as pd
from typing import Any, Dict, List, Optional, Union, Callable, Tuple
from datetime import datetime, date, timedelta
from decimal import Decimal
import logging

from ..utils.exceptions import ExpressionError, ValidationError

logger = logging.getLogger(__name__)


class SafeExpressionEvaluator(ast.NodeVisitor):
    """å®‰å…¨çš„è¡¨è¾¾å¼æ±‚å€¼å™¨"""
    
    # å…è®¸çš„æ“ä½œç¬¦
    ALLOWED_OPERATORS = {
        # ç®—æœ¯è¿ç®—ç¬¦
        ast.Add: operator.add,
        ast.Sub: operator.sub,
        ast.Mult: operator.mul,
        ast.Div: operator.truediv,
        ast.FloorDiv: operator.floordiv,
        ast.Mod: operator.mod,
        ast.Pow: operator.pow,
        
        # æ¯”è¾ƒè¿ç®—ç¬¦
        ast.Eq: operator.eq,
        ast.NotEq: operator.ne,
        ast.Lt: operator.lt,
        ast.LtE: operator.le,
        ast.Gt: operator.gt,
        ast.GtE: operator.ge,
        
        # é€»è¾‘è¿ç®—ç¬¦
        ast.And: lambda x, y: x and y,
        ast.Or: lambda x, y: x or y,
        
        # ä½è¿ç®—ç¬¦
        ast.BitAnd: operator.and_,
        ast.BitOr: operator.or_,
        ast.BitXor: operator.xor,
        
        # ä¸€å…ƒè¿ç®—ç¬¦
        ast.UAdd: operator.pos,
        ast.USub: operator.neg,
        ast.Not: operator.not_,
        ast.Invert: operator.inv,
    }
    
    # å…è®¸çš„å†…ç½®å‡½æ•°
    ALLOWED_BUILTINS = {
        'abs': abs,
        'round': round,
        'max': max,
        'min': min,
        'sum': sum,
        'len': len,
        'int': int,
        'float': float,
        'str': str,
        'bool': bool,
        'list': list,
        'dict': dict,
        'tuple': tuple,
        'set': set,
    }
    
    # å…è®¸çš„æ•°å­¦å‡½æ•°
    ALLOWED_MATH_FUNCTIONS = {
        'sqrt': math.sqrt,
        'ceil': math.ceil,
        'floor': math.floor,
        'sin': math.sin,
        'cos': math.cos,
        'tan': math.tan,
        'log': math.log,
        'log10': math.log10,
        'exp': math.exp,
        'pow': math.pow,
        'pi': math.pi,
        'e': math.e,
    }
    
    def __init__(self, context: Dict[str, Any] = None):
        self.context = context or {}
        self.allowed_names = set(self.context.keys())
        self.allowed_names.update(self.ALLOWED_BUILTINS.keys())
        self.allowed_names.update(self.ALLOWED_MATH_FUNCTIONS.keys())
    
    def evaluate(self, expression: str) -> Any:
        """å®‰å…¨åœ°æ‰§è¡Œè¡¨è¾¾å¼"""
        try:
            # è§£æè¡¨è¾¾å¼ä¸º AST
            tree = ast.parse(expression, mode='eval')
            # è®¿é—®å¹¶æ‰§è¡Œ AST
            return self.visit(tree.body)
        except SyntaxError as e:
            raise ExpressionError(f"è¡¨è¾¾å¼è¯­æ³•é”™è¯¯: {str(e)}")
        except Exception as e:
            raise ExpressionError(f"è¡¨è¾¾å¼æ‰§è¡Œé”™è¯¯: {str(e)}")
    
    def visit_Expression(self, node):
        """è®¿é—®è¡¨è¾¾å¼èŠ‚ç‚¹"""
        return self.visit(node.body)
    
    def visit_Constant(self, node):
        """è®¿é—®å¸¸é‡èŠ‚ç‚¹"""
        return node.value
    
    def visit_Num(self, node):
        """è®¿é—®æ•°å­—èŠ‚ç‚¹ï¼ˆPython < 3.8ï¼‰"""
        return node.n
    
    def visit_Str(self, node):
        """è®¿é—®å­—ç¬¦ä¸²èŠ‚ç‚¹ï¼ˆPython < 3.8ï¼‰"""
        return node.s
    
    def visit_Name(self, node):
        """è®¿é—®åç§°èŠ‚ç‚¹"""
        name = node.id
        
        # æ£€æŸ¥æ˜¯å¦åœ¨å…è®¸çš„åç§°åˆ—è¡¨ä¸­
        if name not in self.allowed_names:
            raise ExpressionError(f"ä¸å…è®¸è®¿é—®å˜é‡æˆ–å‡½æ•°: {name}")
        
        # è¿”å›ç›¸åº”çš„å€¼
        if name in self.context:
            return self.context[name]
        elif name in self.ALLOWED_BUILTINS:
            return self.ALLOWED_BUILTINS[name]
        elif name in self.ALLOWED_MATH_FUNCTIONS:
            return self.ALLOWED_MATH_FUNCTIONS[name]
        else:
            raise ExpressionError(f"æœªå®šä¹‰çš„å˜é‡: {name}")
    
    def visit_BinOp(self, node):
        """è®¿é—®äºŒå…ƒæ“ä½œèŠ‚ç‚¹"""
        op_type = type(node.op)
        
        if op_type not in self.ALLOWED_OPERATORS:
            raise ExpressionError(f"ä¸å…è®¸çš„æ“ä½œç¬¦: {op_type.__name__}")
        
        left = self.visit(node.left)
        right = self.visit(node.right)
        
        try:
            return self.ALLOWED_OPERATORS[op_type](left, right)
        except ZeroDivisionError:
            raise ExpressionError("é™¤é›¶é”™è¯¯")
        except Exception as e:
            raise ExpressionError(f"æ“ä½œæ‰§è¡Œé”™è¯¯: {str(e)}")
    
    def visit_UnaryOp(self, node):
        """è®¿é—®ä¸€å…ƒæ“ä½œèŠ‚ç‚¹"""
        op_type = type(node.op)
        
        if op_type not in self.ALLOWED_OPERATORS:
            raise ExpressionError(f"ä¸å…è®¸çš„ä¸€å…ƒæ“ä½œç¬¦: {op_type.__name__}")
        
        operand = self.visit(node.operand)
        
        try:
            return self.ALLOWED_OPERATORS[op_type](operand)
        except Exception as e:
            raise ExpressionError(f"ä¸€å…ƒæ“ä½œæ‰§è¡Œé”™è¯¯: {str(e)}")
    
    def visit_Compare(self, node):
        """è®¿é—®æ¯”è¾ƒæ“ä½œèŠ‚ç‚¹"""
        left = self.visit(node.left)
        
        for op, comparator in zip(node.ops, node.comparators):
            op_type = type(op)
            
            if op_type not in self.ALLOWED_OPERATORS:
                raise ExpressionError(f"ä¸å…è®¸çš„æ¯”è¾ƒæ“ä½œç¬¦: {op_type.__name__}")
            
            right = self.visit(comparator)
            
            try:
                result = self.ALLOWED_OPERATORS[op_type](left, right)
                if not result:
                    return False
                left = right
            except Exception as e:
                raise ExpressionError(f"æ¯”è¾ƒæ“ä½œæ‰§è¡Œé”™è¯¯: {str(e)}")
        
        return True
    
    def visit_BoolOp(self, node):
        """è®¿é—®å¸ƒå°”æ“ä½œèŠ‚ç‚¹"""
        op_type = type(node.op)
        
        if op_type == ast.And:
            for value in node.values:
                result = self.visit(value)
                if not result:
                    return False
            return True
        elif op_type == ast.Or:
            for value in node.values:
                result = self.visit(value)
                if result:
                    return True
            return False
        else:
            raise ExpressionError(f"ä¸å…è®¸çš„å¸ƒå°”æ“ä½œç¬¦: {op_type.__name__}")
    
    def visit_Call(self, node):
        """è®¿é—®å‡½æ•°è°ƒç”¨èŠ‚ç‚¹"""
        func_name = None
        
        if isinstance(node.func, ast.Name):
            func_name = node.func.id
        else:
            raise ExpressionError("ä¸æ”¯æŒå¤æ‚çš„å‡½æ•°è°ƒç”¨")
        
        # æ£€æŸ¥å‡½æ•°æ˜¯å¦è¢«å…è®¸
        if func_name not in self.allowed_names:
            raise ExpressionError(f"ä¸å…è®¸è°ƒç”¨å‡½æ•°: {func_name}")
        
        # è·å–å‡½æ•°å¯¹è±¡
        if func_name in self.ALLOWED_BUILTINS:
            func = self.ALLOWED_BUILTINS[func_name]
        elif func_name in self.ALLOWED_MATH_FUNCTIONS:
            func = self.ALLOWED_MATH_FUNCTIONS[func_name]
        elif func_name in self.context and callable(self.context[func_name]):
            func = self.context[func_name]
        else:
            raise ExpressionError(f"å‡½æ•°ä¸å¯è°ƒç”¨: {func_name}")
        
        # è®¡ç®—å‚æ•°
        args = [self.visit(arg) for arg in node.args]
        kwargs = {kw.arg: self.visit(kw.value) for kw in node.keywords}
        
        try:
            return func(*args, **kwargs)
        except Exception as e:
            raise ExpressionError(f"å‡½æ•°è°ƒç”¨é”™è¯¯ {func_name}: {str(e)}")
    
    def visit_List(self, node):
        """è®¿é—®åˆ—è¡¨èŠ‚ç‚¹"""
        return [self.visit(item) for item in node.elts]
    
    def visit_Tuple(self, node):
        """è®¿é—®å…ƒç»„èŠ‚ç‚¹"""
        return tuple(self.visit(item) for item in node.elts)
    
    def visit_Dict(self, node):
        """è®¿é—®å­—å…¸èŠ‚ç‚¹"""
        return {
            self.visit(key): self.visit(value)
            for key, value in zip(node.keys, node.values)
        }
    
    def visit_Subscript(self, node):
        """è®¿é—®ä¸‹æ ‡èŠ‚ç‚¹"""
        value = self.visit(node.value)
        index = self.visit(node.slice)
        
        try:
            return value[index]
        except Exception as e:
            raise ExpressionError(f"ä¸‹æ ‡è®¿é—®é”™è¯¯: {str(e)}")
    
    def visit_Slice(self, node):
        """è®¿é—®åˆ‡ç‰‡èŠ‚ç‚¹"""
        lower = self.visit(node.lower) if node.lower else None
        upper = self.visit(node.upper) if node.upper else None
        step = self.visit(node.step) if node.step else None
        
        return slice(lower, upper, step)
    
    def visit_Index(self, node):
        """è®¿é—®ç´¢å¼•èŠ‚ç‚¹ï¼ˆPython < 3.9ï¼‰"""
        return self.visit(node.value)
    
    def generic_visit(self, node):
        """è®¿é—®æœªçŸ¥èŠ‚ç‚¹ç±»å‹"""
        raise ExpressionError(f"ä¸æ”¯æŒçš„è¡¨è¾¾å¼èŠ‚ç‚¹ç±»å‹: {type(node).__name__}")


class ExpressionParser:
    """è¡¨è¾¾å¼è§£æå™¨"""
    
    def __init__(self):
        self.functions = {}
        self.variables = {}
        self._register_default_functions()
    
    def _register_default_functions(self):
        """æ³¨å†Œé»˜è®¤å‡½æ•°"""
        # å­—ç¬¦ä¸²å‡½æ•°
        self.functions.update({
            'upper': lambda x: str(x).upper(),
            'lower': lambda x: str(x).lower(),
            'strip': lambda x: str(x).strip(),
            'split': lambda x, sep=' ': str(x).split(sep),
            'replace': lambda x, old, new: str(x).replace(old, new),
            'startswith': lambda x, prefix: str(x).startswith(prefix),
            'endswith': lambda x, suffix: str(x).endswith(suffix),
            'contains': lambda x, substr: substr in str(x),
            'length': lambda x: len(str(x)),
        })
        
        # æ—¥æœŸæ—¶é—´å‡½æ•°
        self.functions.update({
            'now': lambda: datetime.now(),
            'today': lambda: date.today(),
            'year': lambda x: x.year if isinstance(x, (date, datetime)) else None,
            'month': lambda x: x.month if isinstance(x, (date, datetime)) else None,
            'day': lambda x: x.day if isinstance(x, (date, datetime)) else None,
            'weekday': lambda x: x.weekday() if isinstance(x, (date, datetime)) else None,
            'strftime': lambda x, fmt: x.strftime(fmt) if isinstance(x, (date, datetime)) else None,
        })
        
        # ç±»å‹è½¬æ¢å‡½æ•°
        self.functions.update({
            'to_int': lambda x: int(x) if x is not None else None,
            'to_float': lambda x: float(x) if x is not None else None,
            'to_str': lambda x: str(x) if x is not None else None,
            'to_bool': lambda x: bool(x) if x is not None else None,
        })
        
        # æ¡ä»¶å‡½æ•°
        self.functions.update({
            'if_null': lambda x, default: default if x is None else x,
            'if_empty': lambda x, default: default if not x else x,
            'coalesce': lambda *args: next((arg for arg in args if arg is not None), None),
        })
        
        # æ•°ç»„å‡½æ•°
        self.functions.update({
            'first': lambda arr: arr[0] if arr else None,
            'last': lambda arr: arr[-1] if arr else None,
            'join': lambda arr, sep=',': sep.join(str(x) for x in arr),
            'sort': lambda arr: sorted(arr),
            'unique': lambda arr: list(set(arr)),
        })
    
    def register_function(self, name: str, func: Callable):
        """æ³¨å†Œè‡ªå®šä¹‰å‡½æ•°"""
        if not callable(func):
            raise ValueError(f"å‡½æ•° {name} å¿…é¡»æ˜¯å¯è°ƒç”¨å¯¹è±¡")
        
        self.functions[name] = func
        logger.debug(f"æ³¨å†Œè‡ªå®šä¹‰å‡½æ•°: {name}")
    
    def set_variable(self, name: str, value: Any):
        """è®¾ç½®å˜é‡å€¼"""
        self.variables[name] = value
    
    def set_variables(self, variables: Dict[str, Any]):
        """æ‰¹é‡è®¾ç½®å˜é‡"""
        self.variables.update(variables)
    
    def parse_and_evaluate(self, expression: str, context: Dict[str, Any] = None) -> Any:
        """è§£æå¹¶æ‰§è¡Œè¡¨è¾¾å¼"""
        # åˆå¹¶ä¸Šä¸‹æ–‡
        full_context = self.variables.copy()
        full_context.update(self.functions)
        if context:
            full_context.update(context)
        
        # åˆ›å»ºæ±‚å€¼å™¨
        evaluator = SafeExpressionEvaluator(full_context)
        
        # æ‰§è¡Œè¡¨è¾¾å¼
        return evaluator.evaluate(expression)
    
    def validate_expression(self, expression: str) -> Tuple[bool, str]:
        """éªŒè¯è¡¨è¾¾å¼è¯­æ³•"""
        try:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å±é™©æ¨¡å¼
            dangerous_patterns = [
                r'__\w+__',  # åŒä¸‹åˆ’çº¿å±æ€§
                r'import\s+',  # import è¯­å¥
                r'exec\s*\(',  # exec å‡½æ•°
                r'eval\s*\(',  # eval å‡½æ•°
                r'open\s*\(',  # open å‡½æ•°
                r'file\s*\(',  # file å‡½æ•°
            ]
            
            for pattern in dangerous_patterns:
                if re.search(pattern, expression, re.IGNORECASE):
                    return False, f"è¡¨è¾¾å¼åŒ…å«å±é™©æ¨¡å¼: {pattern}"
            
            # è§£æè¯­æ³•
            ast.parse(expression, mode='eval')
            return True, ""
            
        except SyntaxError as e:
            return False, f"è¯­æ³•é”™è¯¯: {str(e)}"
        except Exception as e:
            return False, f"éªŒè¯å¤±è´¥: {str(e)}"


class DataFrameExpressionParser(ExpressionParser):
    """DataFrame è¡¨è¾¾å¼è§£æå™¨"""
    
    def __init__(self):
        super().__init__()
        self._register_dataframe_functions()
    
    def _register_dataframe_functions(self):
        """æ³¨å†Œ DataFrame ç›¸å…³å‡½æ•°"""
        # èšåˆå‡½æ•°
        self.functions.update({
            'sum_col': lambda df, col: df[col].sum() if col in df.columns else 0,
            'mean_col': lambda df, col: df[col].mean() if col in df.columns else 0,
            'count_col': lambda df, col: df[col].count() if col in df.columns else 0,
            'max_col': lambda df, col: df[col].max() if col in df.columns else None,
            'min_col': lambda df, col: df[col].min() if col in df.columns else None,
        })
        
        # æ¡ä»¶ç­›é€‰å‡½æ•°
        self.functions.update({
            'filter_rows': lambda df, condition: df.query(condition),
            'select_cols': lambda df, cols: df[cols] if isinstance(cols, list) else df[[cols]],
            'drop_cols': lambda df, cols: df.drop(columns=cols),
        })
    
    def apply_to_dataframe(self, df: pd.DataFrame, expression: str, 
                          column_name: str = None) -> Union[pd.DataFrame, pd.Series, Any]:
        """å°†è¡¨è¾¾å¼åº”ç”¨åˆ° DataFrame"""
        try:
            # è®¾ç½® DataFrame ä½œä¸ºä¸Šä¸‹æ–‡å˜é‡
            context = {'df': df}
            
            # æ·»åŠ åˆ—ä½œä¸ºå˜é‡
            for col in df.columns:
                context[col] = df[col]
            
            # è§£æå¹¶æ‰§è¡Œè¡¨è¾¾å¼
            result = self.parse_and_evaluate(expression, context)
            
            # å¦‚æœç»“æœæ˜¯æ ‡é‡ä¸”æŒ‡å®šäº†åˆ—åï¼Œåˆ›å»ºæ–°åˆ—
            if column_name and not isinstance(result, (pd.DataFrame, pd.Series)):
                if isinstance(result, (list, tuple)):
                    if len(result) == len(df):
                        df[column_name] = result
                        return df
                    else:
                        raise ExpressionError(f"ç»“æœé•¿åº¦ {len(result)} ä¸ DataFrame è¡Œæ•° {len(df)} ä¸åŒ¹é…")
                else:
                    df[column_name] = result
                    return df
            
            return result
            
        except Exception as e:
            raise ExpressionError(f"DataFrame è¡¨è¾¾å¼æ‰§è¡Œå¤±è´¥: {str(e)}")
    
    def create_computed_column(self, df: pd.DataFrame, column_name: str, 
                              expression: str) -> pd.DataFrame:
        """åˆ›å»ºè®¡ç®—åˆ—"""
        # ä¸ºæ¯è¡Œåˆ›å»ºä¸Šä¸‹æ–‡å¹¶æ‰§è¡Œè¡¨è¾¾å¼
        results = []
        
        for index, row in df.iterrows():
            # åˆ›å»ºè¡Œä¸Šä¸‹æ–‡
            context = row.to_dict()
            context['index'] = index
            
            try:
                result = self.parse_and_evaluate(expression, context)
                results.append(result)
            except Exception as e:
                logger.warning(f"ç¬¬ {index} è¡Œè¡¨è¾¾å¼æ‰§è¡Œå¤±è´¥: {str(e)}")
                results.append(None)
        
        # æ·»åŠ æ–°åˆ—
        df_copy = df.copy()
        df_copy[column_name] = results
        
        return df_copy
    
    def filter_dataframe(self, df: pd.DataFrame, condition: str) -> pd.DataFrame:
        """æ ¹æ®æ¡ä»¶ç­›é€‰ DataFrame"""
        try:
            # ä½¿ç”¨ pandas query æ–¹æ³•è¿›è¡Œç­›é€‰
            return df.query(condition)
        except Exception as e:
            # å¦‚æœ query å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨è‡ªå®šä¹‰è§£æ
            try:
                mask_results = []
                
                for index, row in df.iterrows():
                    context = row.to_dict()
                    context['index'] = index
                    
                    result = self.parse_and_evaluate(condition, context)
                    mask_results.append(bool(result))
                
                return df[mask_results]
                
            except Exception as e2:
                raise ExpressionError(f"æ¡ä»¶ç­›é€‰å¤±è´¥: {str(e2)}")


class SQLExpressionParser:
    """SQL è¡¨è¾¾å¼è§£æå™¨"""
    
    def __init__(self):
        self.sql_functions = {
            # å­—ç¬¦ä¸²å‡½æ•°
            'UPPER': 'UPPER',
            'LOWER': 'LOWER',
            'TRIM': 'TRIM',
            'LENGTH': 'LENGTH',
            'SUBSTRING': 'SUBSTRING',
            'CONCAT': 'CONCAT',
            'REPLACE': 'REPLACE',
            
            # æ•°å€¼å‡½æ•°
            'ABS': 'ABS',
            'ROUND': 'ROUND',
            'CEIL': 'CEIL',
            'FLOOR': 'FLOOR',
            'SQRT': 'SQRT',
            'POWER': 'POWER',
            
            # æ—¥æœŸå‡½æ•°
            'NOW': 'NOW',
            'CURRENT_DATE': 'CURRENT_DATE',
            'YEAR': 'YEAR',
            'MONTH': 'MONTH',
            'DAY': 'DAY',
            'DATE_ADD': 'DATE_ADD',
            'DATE_SUB': 'DATE_SUB',
            
            # èšåˆå‡½æ•°
            'SUM': 'SUM',
            'COUNT': 'COUNT',
            'AVG': 'AVG',
            'MAX': 'MAX',
            'MIN': 'MIN',
            'GROUP_CONCAT': 'GROUP_CONCAT',
            
            # æ¡ä»¶å‡½æ•°
            'CASE': 'CASE',
            'IF': 'IF',
            'IFNULL': 'IFNULL',
            'COALESCE': 'COALESCE',
        }
    
    def convert_expression_to_sql(self, expression: str, 
                                 dialect: str = 'standard') -> str:
        """å°†è¡¨è¾¾å¼è½¬æ¢ä¸º SQL"""
        # ç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥æ ¹æ®ä¸åŒæ•°æ®åº“æ–¹è¨€è¿›è¡Œè½¬æ¢
        sql_expression = expression
        
        # æ›¿æ¢ä¸€äº›å¸¸è§çš„ Python è¡¨è¾¾å¼ä¸º SQL
        replacements = {
            ' and ': ' AND ',
            ' or ': ' OR ',
            ' not ': ' NOT ',
            '==': '=',
            '!=': '<>',
            'True': '1',
            'False': '0',
            'None': 'NULL',
        }
        
        for old, new in replacements.items():
            sql_expression = sql_expression.replace(old, new)
        
        return sql_expression
    
    def validate_sql_expression(self, expression: str) -> Tuple[bool, str]:
        """éªŒè¯ SQL è¡¨è¾¾å¼"""
        try:
            # æ£€æŸ¥åŸºæœ¬è¯­æ³•
            if not expression.strip():
                return False, "è¡¨è¾¾å¼ä¸èƒ½ä¸ºç©º"
            
            # æ£€æŸ¥å¹³è¡¡çš„æ‹¬å·
            if expression.count('(') != expression.count(')'):
                return False, "æ‹¬å·ä¸åŒ¹é…"
            
            # æ£€æŸ¥å¼•å·
            single_quotes = expression.count("'")
            double_quotes = expression.count('"')
            
            if single_quotes % 2 != 0:
                return False, "å•å¼•å·ä¸åŒ¹é…"
            
            if double_quotes % 2 != 0:
                return False, "åŒå¼•å·ä¸åŒ¹é…"
            
            return True, ""
            
        except Exception as e:
            return False, f"SQL è¡¨è¾¾å¼éªŒè¯å¤±è´¥: {str(e)}"


# å…¨å±€è§£æå™¨å®ä¾‹
expression_parser = ExpressionParser()
dataframe_expression_parser = DataFrameExpressionParser()
sql_expression_parser = SQLExpressionParser()

================
File: uqm-backend/src/utils/logging.py
================
"""
æ—¥å¿—é…ç½®æ¨¡å—
è´Ÿè´£è®¾ç½®ç»“æ„åŒ–æ—¥å¿—è®°å½•
"""

import sys
import structlog
from typing import Any, Dict


def setup_logging(log_level: str = "INFO") -> None:
    """
    è®¾ç½®ç»“æ„åŒ–æ—¥å¿—è®°å½•
    
    Args:
        log_level: æ—¥å¿—çº§åˆ«
    """
    # é…ç½®structlog
    structlog.configure(
        processors=[
            structlog.stdlib.filter_by_level,
            structlog.stdlib.add_logger_name,
            structlog.stdlib.add_log_level,
            structlog.stdlib.PositionalArgumentsFormatter(),
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.UnicodeDecoder(),
            structlog.processors.JSONRenderer()
        ],
        context_class=structlog.threadlocal.wrap_dict(dict),
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.stdlib.BoundLogger,
        cache_logger_on_first_use=True,
    )


def get_logger(name: str) -> structlog.stdlib.BoundLogger:
    """
    è·å–æ—¥å¿—è®°å½•å™¨å®ä¾‹
    
    Args:
        name: æ—¥å¿—è®°å½•å™¨åç§°
        
    Returns:
        ç»“æ„åŒ–æ—¥å¿—è®°å½•å™¨å®ä¾‹
    """
    return structlog.get_logger(name)


class LoggerMixin:
    """æ—¥å¿—è®°å½•å™¨æ··å…¥ç±»"""
    
    @property
    def logger(self) -> structlog.stdlib.BoundLogger:
        """è·å–ç±»ä¸“ç”¨çš„æ—¥å¿—è®°å½•å™¨"""
        return get_logger(self.__class__.__name__)
    
    def log_info(self, message: str, **kwargs: Any) -> None:
        """è®°å½•ä¿¡æ¯æ—¥å¿—"""
        self.logger.info(message, **kwargs)
    
    def log_error(self, message: str, **kwargs: Any) -> None:
        """è®°å½•é”™è¯¯æ—¥å¿—"""
        self.logger.error(message, **kwargs)
    
    def log_warning(self, message: str, **kwargs: Any) -> None:
        """è®°å½•è­¦å‘Šæ—¥å¿—"""
        self.logger.warning(message, **kwargs)
    
    def log_debug(self, message: str, **kwargs: Any) -> None:
        """è®°å½•è°ƒè¯•æ—¥å¿—"""
        self.logger.debug(message, **kwargs)

================
File: uqm-backend/src/utils/sql_builder.py
================
"""
SQLæŸ¥è¯¢æ„å»ºå·¥å…·
æä¾›æ„å»ºå„ç§SQLæŸ¥è¯¢çš„åŠŸèƒ½
"""

from typing import Any, Dict, List, Optional, Union
from enum import Enum

from src.utils.logging import LoggerMixin
from src.utils.exceptions import ValidationError


class SQLDialect(Enum):
    """SQLæ–¹è¨€æšä¸¾"""
    STANDARD = "standard"
    POSTGRESQL = "postgresql"
    MYSQL = "mysql"
    SQLITE = "sqlite"


class SQLBuilder(LoggerMixin):
    """SQLæŸ¥è¯¢æ„å»ºå™¨"""
    
    def __init__(self, dialect: SQLDialect = SQLDialect.STANDARD):
        """
        åˆå§‹åŒ–SQLæ„å»ºå™¨
        
        Args:
            dialect: SQLæ–¹è¨€
        """
        self.dialect = dialect
    
    def build_select_query(self, 
                          select_fields: List[str],
                          from_table: str,
                          joins: Optional[List[Dict[str, Any]]] = None,
                          where_conditions: Optional[List[Dict[str, Any]]] = None,
                          group_by: Optional[List[str]] = None,
                          having: Optional[List[Dict[str, Any]]] = None,
                          order_by: Optional[List[Union[str, Dict[str, Any]]]] = None,
                          limit: Optional[int] = None,
                          offset: Optional[int] = None) -> str:
        """
        æ„å»ºSELECTæŸ¥è¯¢
        
        Args:
            select_fields: é€‰æ‹©å­—æ®µåˆ—è¡¨
            from_table: æºè¡¨å
            joins: JOINæ¡ä»¶åˆ—è¡¨
            where_conditions: WHEREæ¡ä»¶åˆ—è¡¨
            group_by: GROUP BYå­—æ®µåˆ—è¡¨
            having: HAVINGæ¡ä»¶åˆ—è¡¨
            order_by: ORDER BYå­—æ®µåˆ—è¡¨
            limit: é™åˆ¶è¡Œæ•°
            offset: åç§»é‡
            
        Returns:
            SQLæŸ¥è¯¢è¯­å¥
        """
        try:
            query_parts = []
            
            # SELECTå­å¥
            select_clause = self._build_select_clause(select_fields)
            query_parts.append(select_clause)
            
            # FROMå­å¥
            from_clause = self._build_from_clause(from_table)
            query_parts.append(from_clause)
            
            # JOINå­å¥
            if joins:
                join_clause = self._build_join_clause(joins)
                if join_clause:
                    query_parts.append(join_clause)
            
            # WHEREå­å¥
            if where_conditions:
                where_clause = self._build_where_clause(where_conditions)
                if where_clause:
                    query_parts.append(where_clause)
            
            # GROUP BYå­å¥
            if group_by:
                group_by_clause = self._build_group_by_clause(group_by)
                query_parts.append(group_by_clause)
            
            # HAVINGå­å¥
            if having:
                having_clause = self._build_having_clause(having)
                if having_clause:
                    query_parts.append(having_clause)
            
            # ORDER BYå­å¥
            if order_by:
                order_by_clause = self._build_order_by_clause(order_by)
                if order_by_clause:
                    query_parts.append(order_by_clause)
            
            # LIMITå­å¥
            if limit is not None:
                limit_clause = self._build_limit_clause(limit, offset)
                if limit_clause:
                    query_parts.append(limit_clause)
            
            # ç»„åˆæŸ¥è¯¢
            query = "\n".join(query_parts)
            
            return query
            
        except Exception as e:
            self.log_error("æ„å»ºSELECTæŸ¥è¯¢å¤±è´¥", error=str(e))
            raise ValidationError(f"æ„å»ºSELECTæŸ¥è¯¢å¤±è´¥: {e}")
    
    def _build_select_clause(self, select_fields: List[str]) -> str:
        """æ„å»ºSELECTå­å¥"""
        if not select_fields:
            return "SELECT *"
        
        return f"SELECT {', '.join(select_fields)}"
    
    def _build_from_clause(self, from_table: str) -> str:
        """æ„å»ºFROMå­å¥"""
        return f"FROM {from_table}"
    
    def _build_join_clause(self, joins: List[Dict[str, Any]]) -> str:
        """æ„å»ºJOINå­å¥"""
        join_parts = []
        
        for join in joins:
            join_type = join.get("type", "INNER")
            table = join.get("table")
            condition = join.get("on")
            
            if not table or not condition:
                continue
            
            # æ„å»ºè¿æ¥æ¡ä»¶
            join_condition = self._build_join_condition(condition)
            join_part = f"{join_type} JOIN {table} ON {join_condition}"
            join_parts.append(join_part)
        
        return "\n".join(join_parts)
    
    def _build_where_clause(self, where_conditions: List[Dict[str, Any]]) -> str:
        """æ„å»ºWHEREå­å¥"""
        if not where_conditions:
            return ""
        
        conditions = []
        for condition in where_conditions:
            condition_str = self._build_condition(condition)
            if condition_str:
                conditions.append(condition_str)
        
        if not conditions:
            return ""
        
        return f"WHERE {' AND '.join(conditions)}"
    
    def _build_group_by_clause(self, group_by: List[str]) -> str:
        """æ„å»ºGROUP BYå­å¥"""
        if not group_by:
            return ""
        
        return f"GROUP BY {', '.join(group_by)}"
    
    def _build_having_clause(self, having: List[Dict[str, Any]]) -> str:
        """æ„å»ºHAVINGå­å¥"""
        if not having:
            return ""
        
        conditions = []
        for condition in having:
            condition_str = self._build_condition(condition)
            if condition_str:
                conditions.append(condition_str)
        
        if not conditions:
            return ""
        
        return f"HAVING {' AND '.join(conditions)}"
    
    def _build_order_by_clause(self, order_by: List[Union[str, Dict[str, Any]]]) -> str:
        """æ„å»ºORDER BYå­å¥"""
        if not order_by:
            return ""
        
        order_parts = []
        for order_item in order_by:
            if isinstance(order_item, str):
                order_parts.append(order_item)
            elif isinstance(order_item, dict):
                field = order_item.get("field")
                direction = order_item.get("direction", "ASC")
                if field:
                    order_parts.append(f"{field} {direction}")
        
        if not order_parts:
            return ""
        
        return f"ORDER BY {', '.join(order_parts)}"
    
    def _build_limit_clause(self, limit: int, offset: Optional[int] = None) -> str:
        """æ„å»ºLIMITå­å¥"""
        if self.dialect == SQLDialect.MYSQL:
            if offset is not None:
                return f"LIMIT {offset}, {limit}"
            else:
                return f"LIMIT {limit}"
        
        elif self.dialect == SQLDialect.POSTGRESQL:
            if offset is not None:
                return f"LIMIT {limit} OFFSET {offset}"
            else:
                return f"LIMIT {limit}"
        
        elif self.dialect == SQLDialect.SQLITE:
            if offset is not None:
                return f"LIMIT {limit} OFFSET {offset}"
            else:
                return f"LIMIT {limit}"
        
        else:
            # æ ‡å‡†SQL
            if offset is not None:
                return f"LIMIT {limit} OFFSET {offset}"
            else:
                return f"LIMIT {limit}"
    
    def _build_join_condition(self, condition: Union[str, Dict[str, Any]]) -> str:
        """æ„å»ºè¿æ¥æ¡ä»¶"""
        if isinstance(condition, str):
            return condition
        
        elif isinstance(condition, dict):
            left = condition.get("left")
            right = condition.get("right")
            operator = condition.get("operator", "=")
            
            if left and right:
                return f"{left} {operator} {right}"
        
        return ""
    
    def _build_condition(self, condition: Union[str, Dict[str, Any]]) -> str:
        """æ„å»ºæ¡ä»¶è¡¨è¾¾å¼"""
        if isinstance(condition, str):
            return condition
        
        elif isinstance(condition, dict):
            field = condition.get("field")
            operator = condition.get("operator", "=")
            value = condition.get("value")
            
            if not field:
                return ""
            
            # å¤„ç†ä¸åŒçš„æ“ä½œç¬¦
            if operator.upper() == "IN":
                if isinstance(value, list):
                    value_str = ", ".join([self._format_value(v) for v in value])
                    return f"{field} IN ({value_str})"
                else:
                    return f"{field} IN ({self._format_value(value)})"
            
            elif operator.upper() == "BETWEEN":
                if isinstance(value, list) and len(value) == 2:
                    return f"{field} BETWEEN {self._format_value(value[0])} AND {self._format_value(value[1])}"
            
            elif operator.upper() == "LIKE":
                return f"{field} LIKE {self._format_value(value)}"
            
            elif operator.upper() == "IS NULL":
                return f"{field} IS NULL"
            
            elif operator.upper() == "IS NOT NULL":
                return f"{field} IS NOT NULL"
            
            else:
                return f"{field} {operator} {self._format_value(value)}"
        
        return ""
    
    def _format_value(self, value: Any) -> str:
        """æ ¼å¼åŒ–å€¼"""
        if value is None:
            return "NULL"
        elif isinstance(value, str):
            # è½¬ä¹‰å•å¼•å·
            escaped_value = value.replace("'", "''")
            return f"'{escaped_value}'"
        elif isinstance(value, bool):
            return "TRUE" if value else "FALSE"
        elif isinstance(value, (int, float)):
            return str(value)
        else:
            return f"'{str(value)}'"
    
    def build_insert_query(self, table_name: str, data: Dict[str, Any]) -> str:
        """æ„å»ºINSERTæŸ¥è¯¢"""
        columns = list(data.keys())
        values = [self._format_value(data[col]) for col in columns]
        
        columns_str = ", ".join(columns)
        values_str = ", ".join(values)
        
        return f"INSERT INTO {table_name} ({columns_str}) VALUES ({values_str})"
    
    def build_update_query(self, table_name: str, 
                          updates: Dict[str, Any],
                          where_conditions: List[Dict[str, Any]]) -> str:
        """æ„å»ºUPDATEæŸ¥è¯¢"""
        set_parts = []
        for column, value in updates.items():
            set_parts.append(f"{column} = {self._format_value(value)}")
        
        set_clause = ", ".join(set_parts)
        where_clause = self._build_where_clause(where_conditions)
        
        query = f"UPDATE {table_name} SET {set_clause}"
        if where_clause:
            query += f" {where_clause}"
        
        return query
    
    def build_delete_query(self, table_name: str, 
                          where_conditions: List[Dict[str, Any]]) -> str:
        """æ„å»ºDELETEæŸ¥è¯¢"""
        where_clause = self._build_where_clause(where_conditions)
        
        query = f"DELETE FROM {table_name}"
        if where_clause:
            query += f" {where_clause}"
        
        return query

================
File: uqm-backend/src/utils/validators.py
================
"""
æ•°æ®éªŒè¯å·¥å…·æ¨¡å—

æä¾›å„ç§æ•°æ®éªŒè¯åŠŸèƒ½ï¼ŒåŒ…æ‹¬ UQM é…ç½®éªŒè¯ã€æ•°æ®æ ¼å¼éªŒè¯ã€ä¸šåŠ¡é€»è¾‘éªŒè¯ç­‰ã€‚
"""

import re
import json
import pandas as pd
from typing import Any, Dict, List, Optional, Union, Tuple
from datetime import datetime, date
from decimal import Decimal, InvalidOperation
import logging

from ..utils.exceptions import ValidationError

logger = logging.getLogger(__name__)


class DataValidator:
    """æ•°æ®éªŒè¯å™¨åŸºç±»"""
    
    def __init__(self):
        self.errors = []
    
    def add_error(self, field: str, message: str, value: Any = None):
        """æ·»åŠ éªŒè¯é”™è¯¯"""
        self.errors.append({
            'field': field,
            'message': message,
            'value': value,
            'timestamp': datetime.now().isoformat()
        })
    
    def clear_errors(self):
        """æ¸…ç©ºé”™è¯¯åˆ—è¡¨"""
        self.errors = []
    
    def has_errors(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰éªŒè¯é”™è¯¯"""
        return len(self.errors) > 0
    
    def get_errors(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰éªŒè¯é”™è¯¯"""
        return self.errors.copy()
    
    def raise_if_errors(self):
        """å¦‚æœæœ‰é”™è¯¯åˆ™æŠ›å‡ºå¼‚å¸¸"""
        if self.has_errors():
            raise ValidationError(f"éªŒè¯å¤±è´¥: {len(self.errors)} ä¸ªé”™è¯¯", details=self.errors)


class UQMValidator(DataValidator):
    """UQM é…ç½®éªŒè¯å™¨"""
    
    # æ”¯æŒçš„æ­¥éª¤ç±»å‹
    SUPPORTED_STEP_TYPES = {
        'query', 'enrich', 'pivot', 'unpivot', 'union', 'assert',
        'filter', 'sort', 'group', 'aggregate', 'join', 'transform'
    }
    
    # æ”¯æŒçš„æ•°æ®æºç±»å‹
    SUPPORTED_DATASOURCE_TYPES = {
        'postgres', 'mysql', 'sqlite', 'oracle', 'sqlserver',
        'mongodb', 'redis', 'elasticsearch', 'api', 'file'
    }
    
    def validate_uqm_config(self, config: Dict[str, Any]) -> bool:
        """éªŒè¯å®Œæ•´çš„ UQM é…ç½®"""
        self.clear_errors()
        
        # éªŒè¯æ ¹çº§å­—æ®µ
        self._validate_root_fields(config)
        
        # éªŒè¯æ•°æ®æºé…ç½®
        if 'datasources' in config:
            self._validate_datasources(config['datasources'])
        
        # éªŒè¯æ­¥éª¤é…ç½®
        if 'steps' in config:
            self._validate_steps(config['steps'])
        
        # éªŒè¯è¾“å‡ºé…ç½®
        if 'output' in config:
            self._validate_output(config['output'])
        
        return not self.has_errors()
    
    def _validate_root_fields(self, config: Dict[str, Any]):
        """éªŒè¯æ ¹çº§å¿…éœ€å­—æ®µ"""
        required_fields = ['name', 'version', 'steps']
        
        for field in required_fields:
            if field not in config:
                self.add_error(field, f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
        
        # éªŒè¯ç‰ˆæœ¬æ ¼å¼
        if 'version' in config:
            version = config['version']
            if not isinstance(version, str) or not re.match(r'^\d+\.\d+(\.\d+)?$', version):
                self.add_error('version', f"ç‰ˆæœ¬æ ¼å¼æ— æ•ˆ: {version}")
        
        # éªŒè¯åç§°
        if 'name' in config:
            name = config['name']
            if not isinstance(name, str) or len(name.strip()) == 0:
                self.add_error('name', f"åç§°æ— æ•ˆ: {name}")
    
    def _validate_datasources(self, datasources: Dict[str, Any]):
        """éªŒè¯æ•°æ®æºé…ç½®"""
        if not isinstance(datasources, dict):
            self.add_error('datasources', "æ•°æ®æºé…ç½®å¿…é¡»æ˜¯å­—å…¸ç±»å‹")
            return
        
        for ds_name, ds_config in datasources.items():
            self._validate_single_datasource(ds_name, ds_config)
    
    def _validate_single_datasource(self, name: str, config: Dict[str, Any]):
        """éªŒè¯å•ä¸ªæ•°æ®æºé…ç½®"""
        if not isinstance(config, dict):
            self.add_error(f'datasources.{name}', "æ•°æ®æºé…ç½®å¿…é¡»æ˜¯å­—å…¸ç±»å‹")
            return
        
        # éªŒè¯ç±»å‹
        if 'type' not in config:
            self.add_error(f'datasources.{name}.type', "ç¼ºå°‘æ•°æ®æºç±»å‹")
        elif config['type'] not in self.SUPPORTED_DATASOURCE_TYPES:
            self.add_error(f'datasources.{name}.type', 
                          f"ä¸æ”¯æŒçš„æ•°æ®æºç±»å‹: {config['type']}")
        
        # éªŒè¯è¿æ¥é…ç½®
        if 'connection' not in config:
            self.add_error(f'datasources.{name}.connection', "ç¼ºå°‘è¿æ¥é…ç½®")
        elif not isinstance(config['connection'], dict):
            self.add_error(f'datasources.{name}.connection', "è¿æ¥é…ç½®å¿…é¡»æ˜¯å­—å…¸ç±»å‹")
    
    def _validate_steps(self, steps: List[Dict[str, Any]]):
        """éªŒè¯æ­¥éª¤é…ç½®"""
        if not isinstance(steps, list):
            self.add_error('steps', "æ­¥éª¤é…ç½®å¿…é¡»æ˜¯åˆ—è¡¨ç±»å‹")
            return
        
        if len(steps) == 0:
            self.add_error('steps', "è‡³å°‘éœ€è¦ä¸€ä¸ªæ­¥éª¤")
            return
        
        step_names = set()
        for i, step in enumerate(steps):
            self._validate_single_step(i, step, step_names)
    
    def _validate_single_step(self, index: int, step: Dict[str, Any], step_names: set):
        """éªŒè¯å•ä¸ªæ­¥éª¤é…ç½®"""
        if not isinstance(step, dict):
            self.add_error(f'steps[{index}]', "æ­¥éª¤é…ç½®å¿…é¡»æ˜¯å­—å…¸ç±»å‹")
            return
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        required_fields = ['name', 'type']
        for field in required_fields:
            if field not in step:
                self.add_error(f'steps[{index}].{field}', f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
        
        # éªŒè¯æ­¥éª¤åç§°å”¯ä¸€æ€§
        if 'name' in step:
            name = step['name']
            if name in step_names:
                self.add_error(f'steps[{index}].name', f"æ­¥éª¤åç§°é‡å¤: {name}")
            else:
                step_names.add(name)
        
        # éªŒè¯æ­¥éª¤ç±»å‹
        if 'type' in step:
            step_type = step['type']
            if step_type not in self.SUPPORTED_STEP_TYPES:
                self.add_error(f'steps[{index}].type', 
                              f"ä¸æ”¯æŒçš„æ­¥éª¤ç±»å‹: {step_type}")
        
        # éªŒè¯ä¾èµ–å…³ç³»
        if 'depends_on' in step:
            self._validate_dependencies(index, step['depends_on'], step_names)
    
    def _validate_dependencies(self, step_index: int, dependencies: List[str], 
                             available_steps: set):
        """éªŒè¯æ­¥éª¤ä¾èµ–å…³ç³»"""
        if not isinstance(dependencies, list):
            self.add_error(f'steps[{step_index}].depends_on', 
                          "ä¾èµ–é…ç½®å¿…é¡»æ˜¯åˆ—è¡¨ç±»å‹")
            return
        
        for dep in dependencies:
            if not isinstance(dep, str):
                self.add_error(f'steps[{step_index}].depends_on', 
                              f"ä¾èµ–åç§°å¿…é¡»æ˜¯å­—ç¬¦ä¸²: {dep}")
            elif dep not in available_steps:
                self.add_error(f'steps[{step_index}].depends_on', 
                              f"å¼•ç”¨äº†ä¸å­˜åœ¨çš„æ­¥éª¤: {dep}")
    
    def _validate_output(self, output: Dict[str, Any]):
        """éªŒè¯è¾“å‡ºé…ç½®"""
        if not isinstance(output, dict):
            self.add_error('output', "è¾“å‡ºé…ç½®å¿…é¡»æ˜¯å­—å…¸ç±»å‹")
            return
        
        # éªŒè¯è¾“å‡ºæ ¼å¼
        if 'format' in output:
            supported_formats = {'json', 'csv', 'excel', 'parquet', 'sql'}
            if output['format'] not in supported_formats:
                self.add_error('output.format', 
                              f"ä¸æ”¯æŒçš„è¾“å‡ºæ ¼å¼: {output['format']}")


class DataTypeValidator(DataValidator):
    """æ•°æ®ç±»å‹éªŒè¯å™¨"""
    
    def validate_dataframe(self, df: pd.DataFrame, schema: Dict[str, Any]) -> bool:
        """éªŒè¯ DataFrame ç»“æ„å’Œæ•°æ®"""
        self.clear_errors()
        
        if not isinstance(df, pd.DataFrame):
            self.add_error('dataframe', "è¾“å…¥ä¸æ˜¯æœ‰æ•ˆçš„ DataFrame")
            return False
        
        # éªŒè¯åˆ—ç»“æ„
        if 'columns' in schema:
            self._validate_columns(df, schema['columns'])
        
        # éªŒè¯æ•°æ®çº¦æŸ
        if 'constraints' in schema:
            self._validate_constraints(df, schema['constraints'])
        
        return not self.has_errors()
    
    def _validate_columns(self, df: pd.DataFrame, column_schema: Dict[str, Any]):
        """éªŒè¯åˆ—ç»“æ„"""
        # æ£€æŸ¥å¿…éœ€åˆ—
        if 'required' in column_schema:
            for col in column_schema['required']:
                if col not in df.columns:
                    self.add_error('columns', f"ç¼ºå°‘å¿…éœ€åˆ—: {col}")
        
        # æ£€æŸ¥åˆ—æ•°æ®ç±»å‹
        if 'types' in column_schema:
            for col, expected_type in column_schema['types'].items():
                if col in df.columns:
                    self._validate_column_type(df, col, expected_type)
    
    def _validate_column_type(self, df: pd.DataFrame, column: str, expected_type: str):
        """éªŒè¯åˆ—æ•°æ®ç±»å‹"""
        actual_type = str(df[column].dtype)
        
        # ç±»å‹æ˜ å°„
        type_mapping = {
            'int': ['int64', 'int32', 'int16', 'int8'],
            'float': ['float64', 'float32'],
            'string': ['object', 'string'],
            'datetime': ['datetime64[ns]', 'datetime64'],
            'bool': ['bool']
        }
        
        if expected_type in type_mapping:
            if actual_type not in type_mapping[expected_type]:
                self.add_error(f'columns.{column}', 
                              f"åˆ—ç±»å‹ä¸åŒ¹é…ï¼ŒæœŸæœ›: {expected_type}, å®é™…: {actual_type}")
    
    def _validate_constraints(self, df: pd.DataFrame, constraints: Dict[str, Any]):
        """éªŒè¯æ•°æ®çº¦æŸ"""
        # æ£€æŸ¥éç©ºçº¦æŸ
        if 'not_null' in constraints:
            for col in constraints['not_null']:
                if col in df.columns and df[col].isnull().any():
                    null_count = df[col].isnull().sum()
                    self.add_error(f'constraints.not_null.{col}', 
                                  f"åˆ—åŒ…å« {null_count} ä¸ªç©ºå€¼")
        
        # æ£€æŸ¥å”¯ä¸€æ€§çº¦æŸ
        if 'unique' in constraints:
            for col in constraints['unique']:
                if col in df.columns and df[col].duplicated().any():
                    dup_count = df[col].duplicated().sum()
                    self.add_error(f'constraints.unique.{col}', 
                                  f"åˆ—åŒ…å« {dup_count} ä¸ªé‡å¤å€¼")
        
        # æ£€æŸ¥å€¼èŒƒå›´çº¦æŸ
        if 'range' in constraints:
            for col, range_config in constraints['range'].items():
                if col in df.columns:
                    self._validate_value_range(df, col, range_config)
    
    def _validate_value_range(self, df: pd.DataFrame, column: str, 
                            range_config: Dict[str, Any]):
        """éªŒè¯å€¼èŒƒå›´"""
        if 'min' in range_config:
            min_val = range_config['min']
            if (df[column] < min_val).any():
                violation_count = (df[column] < min_val).sum()
                self.add_error(f'constraints.range.{column}.min', 
                              f"{violation_count} ä¸ªå€¼å°äºæœ€å°å€¼ {min_val}")
        
        if 'max' in range_config:
            max_val = range_config['max']
            if (df[column] > max_val).any():
                violation_count = (df[column] > max_val).sum()
                self.add_error(f'constraints.range.{column}.max', 
                              f"{violation_count} ä¸ªå€¼å¤§äºæœ€å¤§å€¼ {max_val}")


class SchemaValidator(DataValidator):
    """Schema éªŒè¯å™¨"""
    
    def validate_json_schema(self, data: Any, schema: Dict[str, Any]) -> bool:
        """éªŒè¯ JSON Schema"""
        self.clear_errors()
        
        try:
            # è¿™é‡Œåº”è¯¥ä½¿ç”¨ jsonschema åº“è¿›è¡ŒéªŒè¯
            # ç”±äºç®€åŒ–å®ç°ï¼Œè¿™é‡ŒåªåšåŸºæœ¬éªŒè¯
            self._validate_basic_schema(data, schema)
        except Exception as e:
            self.add_error('schema', f"Schema éªŒè¯å¤±è´¥: {str(e)}")
        
        return not self.has_errors()
    
    def _validate_basic_schema(self, data: Any, schema: Dict[str, Any]):
        """åŸºæœ¬ Schema éªŒè¯"""
        # éªŒè¯ç±»å‹
        if 'type' in schema:
            expected_type = schema['type']
            if not self._check_type(data, expected_type):
                self.add_error('type', f"ç±»å‹ä¸åŒ¹é…ï¼ŒæœŸæœ›: {expected_type}")
        
        # éªŒè¯å¿…éœ€å±æ€§ï¼ˆå¯¹è±¡ç±»å‹ï¼‰
        if isinstance(data, dict) and 'required' in schema:
            for field in schema['required']:
                if field not in data:
                    self.add_error('required', f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
        
        # éªŒè¯å±æ€§ï¼ˆå¯¹è±¡ç±»å‹ï¼‰
        if isinstance(data, dict) and 'properties' in schema:
            for key, value in data.items():
                if key in schema['properties']:
                    # é€’å½’éªŒè¯å­å±æ€§
                    sub_validator = SchemaValidator()
                    if not sub_validator.validate_json_schema(value, schema['properties'][key]):
                        self.errors.extend(sub_validator.get_errors())
    
    def _check_type(self, data: Any, expected_type: str) -> bool:
        """æ£€æŸ¥æ•°æ®ç±»å‹"""
        type_mapping = {
            'string': str,
            'number': (int, float, Decimal),
            'integer': int,
            'boolean': bool,
            'array': list,
            'object': dict,
            'null': type(None)
        }
        
        if expected_type in type_mapping:
            return isinstance(data, type_mapping[expected_type])
        
        return True


def validate_sql_injection(sql: str) -> Tuple[bool, List[str]]:
    """éªŒè¯ SQL æ³¨å…¥é£é™©"""
    dangerous_patterns = [
        r';\s*(drop|delete|truncate|update)\s+',
        r'union\s+select',
        r'exec\s*\(',
        r'xp_cmdshell',
        r'sp_executesql',
        r'--|#',
        r'/\*.*\*/',
        r'@@version',
        r'information_schema',
        r'sys\.',
    ]
    
    risks = []
    sql_lower = sql.lower()
    
    for pattern in dangerous_patterns:
        if re.search(pattern, sql_lower, re.IGNORECASE):
            risks.append(f"æ£€æµ‹åˆ°æ½œåœ¨çš„ SQL æ³¨å…¥æ¨¡å¼: {pattern}")
    
    return len(risks) == 0, risks


def validate_column_name(name: str) -> Tuple[bool, str]:
    """éªŒè¯åˆ—åæ ¼å¼"""
    if not isinstance(name, str):
        return False, "åˆ—åå¿…é¡»æ˜¯å­—ç¬¦ä¸²"
    
    if len(name.strip()) == 0:
        return False, "åˆ—åä¸èƒ½ä¸ºç©º"
    
    # æ£€æŸ¥ SQL ä¿ç•™å­—
    sql_keywords = {
        'select', 'from', 'where', 'group', 'order', 'by', 'having',
        'insert', 'update', 'delete', 'drop', 'create', 'alter',
        'table', 'database', 'index', 'view', 'procedure'
    }
    
    if name.lower() in sql_keywords:
        return False, f"åˆ—åä¸èƒ½ä½¿ç”¨ SQL ä¿ç•™å­—: {name}"
    
    # æ£€æŸ¥å­—ç¬¦æ ¼å¼
    if not re.match(r'^[a-zA-Z][a-zA-Z0-9_]*$', name):
        return False, "åˆ—ååªèƒ½åŒ…å«å­—æ¯ã€æ•°å­—å’Œä¸‹åˆ’çº¿ï¼Œä¸”å¿…é¡»ä»¥å­—æ¯å¼€å¤´"
    
    return True, ""


def validate_expression(expression: str) -> Tuple[bool, str]:
    """éªŒè¯è¡¨è¾¾å¼å®‰å…¨æ€§"""
    if not isinstance(expression, str):
        return False, "è¡¨è¾¾å¼å¿…é¡»æ˜¯å­—ç¬¦ä¸²"
    
    # æ£€æŸ¥å±é™©å‡½æ•°è°ƒç”¨
    dangerous_functions = [
        'eval', 'exec', 'compile', '__import__',
        'open', 'file', 'input', 'raw_input',
        'reload', 'vars', 'globals', 'locals'
    ]
    
    for func in dangerous_functions:
        if func in expression:
            return False, f"è¡¨è¾¾å¼åŒ…å«å±é™©å‡½æ•°: {func}"
    
    # æ£€æŸ¥å±é™©å±æ€§è®¿é—®
    if '__' in expression and ('__class__' in expression or '__base__' in expression):
        return False, "è¡¨è¾¾å¼åŒ…å«å±é™©çš„å±æ€§è®¿é—®"
    
    return True, ""


class BusinessValidator(DataValidator):
    """ä¸šåŠ¡é€»è¾‘éªŒè¯å™¨"""
    
    def validate_pivot_config(self, config: Dict[str, Any]) -> bool:
        """éªŒè¯é€è§†é…ç½®"""
        self.clear_errors()
        
        required_fields = ['index_columns', 'pivot_column', 'value_columns']
        for field in required_fields:
            if field not in config:
                self.add_error(field, f"é€è§†é…ç½®ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
        
        # éªŒè¯åˆ—ååˆ—è¡¨
        for field in ['index_columns', 'value_columns']:
            if field in config:
                if not isinstance(config[field], list):
                    self.add_error(field, f"{field} å¿…é¡»æ˜¯åˆ—è¡¨ç±»å‹")
                elif len(config[field]) == 0:
                    self.add_error(field, f"{field} ä¸èƒ½ä¸ºç©º")
        
        return not self.has_errors()
    
    def validate_join_config(self, config: Dict[str, Any]) -> bool:
        """éªŒè¯è¿æ¥é…ç½®"""
        self.clear_errors()
        
        required_fields = ['left_on', 'right_on', 'how']
        for field in required_fields:
            if field not in config:
                self.add_error(field, f"è¿æ¥é…ç½®ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
        
        # éªŒè¯è¿æ¥ç±»å‹
        if 'how' in config:
            valid_joins = {'inner', 'left', 'right', 'outer', 'cross'}
            if config['how'] not in valid_joins:
                self.add_error('how', f"ä¸æ”¯æŒçš„è¿æ¥ç±»å‹: {config['how']}")
        
        return not self.has_errors()
    
    def validate_aggregation_config(self, config: Dict[str, Any]) -> bool:
        """éªŒè¯èšåˆé…ç½®"""
        self.clear_errors()
        
        if 'group_by' not in config and 'agg_functions' not in config:
            self.add_error('config', "èšåˆé…ç½®å¿…é¡»åŒ…å« group_by æˆ– agg_functions")
        
        # éªŒè¯èšåˆå‡½æ•°
        if 'agg_functions' in config:
            valid_functions = {
                'sum', 'count', 'avg', 'min', 'max', 'std', 'var', 'median'
            }
            
            for col, func in config['agg_functions'].items():
                if isinstance(func, str):
                    if func not in valid_functions:
                        self.add_error('agg_functions', 
                                      f"ä¸æ”¯æŒçš„èšåˆå‡½æ•°: {func}")
                elif isinstance(func, list):
                    for f in func:
                        if f not in valid_functions:
                            self.add_error('agg_functions', 
                                          f"ä¸æ”¯æŒçš„èšåˆå‡½æ•°: {f}")
        
        return not self.has_errors()


# å…¨å±€éªŒè¯å™¨å®ä¾‹
uqm_validator = UQMValidator()
data_type_validator = DataTypeValidator()
schema_validator = SchemaValidator()
business_validator = BusinessValidator()

================
File: uqm-backend/tests/conftest.py
================
"""
æµ‹è¯•é…ç½®å’Œå…¬å…±å·¥å…·
"""

import os
import sys
import pytest
import asyncio
import tempfile
import shutil
from pathlib import Path
from typing import Dict, Any
from unittest.mock import Mock, patch

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.config.settings import Settings
from src.utils.logging import setup_logging


# æµ‹è¯•æ•°æ®åº“é…ç½®
TEST_DATABASE_URL = "sqlite:///:memory:"
TEST_REDIS_URL = "redis://localhost:6379/15"

# æµ‹è¯•ç¯å¢ƒå˜é‡
TEST_ENV_VARS = {
    "DATABASE_URL": TEST_DATABASE_URL,
    "REDIS_URL": TEST_REDIS_URL,
    "LOG_LEVEL": "DEBUG",
    "TESTING": "true",
    "CACHE_BACKEND": "memory",
}


@pytest.fixture(scope="session")
def event_loop():
    """åˆ›å»ºäº‹ä»¶å¾ªç¯ä¾›æµ‹è¯•ä½¿ç”¨"""
    loop = asyncio.new_event_loop()
    yield loop
    loop.close()


@pytest.fixture(scope="session")
def test_settings():
    """æµ‹è¯•è®¾ç½®"""
    with patch.dict(os.environ, TEST_ENV_VARS):
        settings = Settings()
        yield settings


@pytest.fixture
def temp_dir():
    """ä¸´æ—¶ç›®å½•"""
    temp_dir = tempfile.mkdtemp()
    yield temp_dir
    shutil.rmtree(temp_dir)


@pytest.fixture
def sample_uqm_config():
    """ç¤ºä¾‹ UQM é…ç½®"""
    return {
        "name": "test_uqm",
        "version": "1.0.0",
        "description": "æµ‹è¯•ç”¨ UQM é…ç½®",
        "datasources": {
            "test_db": {
                "type": "sqlite",
                "connection": {
                    "database": ":memory:"
                }
            }
        },
        "steps": [
            {
                "name": "query_step",
                "type": "query",
                "datasource": "test_db",
                "config": {
                    "sql": "SELECT 1 as test_column"
                }
            },
            {
                "name": "enrich_step",
                "type": "enrich",
                "depends_on": ["query_step"],
                "config": {
                    "enrichments": [
                        {
                            "column": "computed_column",
                            "expression": "test_column * 2"
                        }
                    ]
                }
            }
        ],
        "output": {
            "format": "json"
        }
    }


@pytest.fixture
def sample_dataframe():
    """ç¤ºä¾‹ DataFrame"""
    import pandas as pd
    
    return pd.DataFrame({
        'id': [1, 2, 3, 4, 5],
        'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],
        'age': [25, 30, 35, 28, 32],
        'salary': [50000, 60000, 70000, 55000, 65000],
        'department': ['IT', 'HR', 'IT', 'Finance', 'HR']
    })


@pytest.fixture
def mock_database():
    """æ¨¡æ‹Ÿæ•°æ®åº“è¿æ¥"""
    db_mock = Mock()
    db_mock.execute.return_value = Mock()
    db_mock.fetchall.return_value = [
        {'id': 1, 'name': 'Test User', 'email': 'test@example.com'}
    ]
    return db_mock


@pytest.fixture
def mock_redis():
    """æ¨¡æ‹Ÿ Redis è¿æ¥"""
    redis_mock = Mock()
    redis_mock.get.return_value = None
    redis_mock.set.return_value = True
    redis_mock.delete.return_value = 1
    redis_mock.exists.return_value = False
    return redis_mock


class MockDataConnector:
    """æ¨¡æ‹Ÿæ•°æ®è¿æ¥å™¨"""
    
    def __init__(self, data=None):
        self.data = data or []
        self.connected = False
    
    async def connect(self):
        self.connected = True
    
    async def disconnect(self):
        self.connected = False
    
    async def execute_query(self, sql: str, params: Dict[str, Any] = None):
        return self.data
    
    def get_connection_info(self):
        return {
            "type": "mock",
            "connected": self.connected
        }


class AsyncMock:
    """å¼‚æ­¥æ–¹æ³•çš„æ¨¡æ‹Ÿå¯¹è±¡"""
    
    def __init__(self, return_value=None):
        self._return_value = return_value
        self.call_count = 0
        self.call_args_list = []
    
    async def __call__(self, *args, **kwargs):
        self.call_count += 1
        self.call_args_list.append((args, kwargs))
        return self._return_value


def create_test_file(content: str, file_path: str = None, temp_dir: str = None) -> str:
    """åˆ›å»ºæµ‹è¯•æ–‡ä»¶"""
    if file_path is None:
        if temp_dir is None:
            temp_dir = tempfile.mkdtemp()
        file_path = os.path.join(temp_dir, "test_file.txt")
    
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    return file_path


def assert_dataframe_equal(df1, df2, check_dtype=True, check_index=True):
    """æ–­è¨€ä¸¤ä¸ª DataFrame ç›¸ç­‰"""
    import pandas as pd
    import pandas.testing as pdt
    
    pdt.assert_frame_equal(df1, df2, check_dtype=check_dtype, check_index=check_index)


def assert_dict_subset(subset: Dict[str, Any], superset: Dict[str, Any]):
    """æ–­è¨€å­—å…¸æ˜¯å¦ä¸€ä¸ªå­—å…¸çš„å­é›†"""
    for key, value in subset.items():
        assert key in superset, f"é”® '{key}' ä¸åœ¨ç›®æ ‡å­—å…¸ä¸­"
        assert superset[key] == value, f"é”® '{key}' çš„å€¼ä¸åŒ¹é…: æœŸæœ› {value}, å®é™… {superset[key]}"


class TestDataHelper:
    """æµ‹è¯•æ•°æ®è¾…åŠ©ç±»"""
    
    @staticmethod
    def create_sample_csv(file_path: str, rows: int = 100):
        """åˆ›å»ºç¤ºä¾‹ CSV æ–‡ä»¶"""
        import pandas as pd
        import random
        
        data = {
            'id': range(1, rows + 1),
            'name': [f'User_{i}' for i in range(1, rows + 1)],
            'age': [random.randint(18, 80) for _ in range(rows)],
            'salary': [random.randint(30000, 100000) for _ in range(rows)],
            'department': [random.choice(['IT', 'HR', 'Finance', 'Marketing']) for _ in range(rows)]
        }
        
        df = pd.DataFrame(data)
        df.to_csv(file_path, index=False)
        return file_path
    
    @staticmethod
    def create_sample_json(file_path: str, records: int = 50):
        """åˆ›å»ºç¤ºä¾‹ JSON æ–‡ä»¶"""
        import json
        import random
        
        data = []
        for i in range(1, records + 1):
            record = {
                'id': i,
                'name': f'Product_{i}',
                'price': round(random.uniform(10.0, 1000.0), 2),
                'category': random.choice(['Electronics', 'Clothing', 'Books', 'Home']),
                'in_stock': random.choice([True, False])
            }
            data.append(record)
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2)
        
        return file_path


# å…¨å±€æµ‹è¯•é…ç½®
def setup_test_logging():
    """è®¾ç½®æµ‹è¯•æ—¥å¿—"""
    setup_logging(level="DEBUG", format_type="simple")


# è‡ªåŠ¨è®¾ç½®æµ‹è¯•ç¯å¢ƒ
def pytest_configure(config):
    """pytest é…ç½®é’©å­"""
    # è®¾ç½®æµ‹è¯•ç¯å¢ƒå˜é‡
    for key, value in TEST_ENV_VARS.items():
        os.environ.setdefault(key, value)
    
    # è®¾ç½®æµ‹è¯•æ—¥å¿—
    setup_test_logging()


def pytest_sessionstart(session):
    """æµ‹è¯•ä¼šè¯å¼€å§‹æ—¶çš„é’©å­"""
    print("å¼€å§‹è¿è¡Œ UQM Backend æµ‹è¯•å¥—ä»¶...")


def pytest_sessionfinish(session, exitstatus):
    """æµ‹è¯•ä¼šè¯ç»“æŸæ—¶çš„é’©å­"""
    print(f"UQM Backend æµ‹è¯•å¥—ä»¶å®Œæˆï¼Œé€€å‡ºçŠ¶æ€: {exitstatus}")


# æ ‡è®°å®šä¹‰
pytest_plugins = []

# è‡ªå®šä¹‰æ ‡è®°
pytest.mark.unit = pytest.mark.unit
pytest.mark.integration = pytest.mark.integration
pytest.mark.slow = pytest.mark.slow
pytest.mark.async_test = pytest.mark.asyncio

================
File: uqm-backend/tests/integration/test_core_engine.py
================
"""
æ ¸å¿ƒå¼•æ“é›†æˆæµ‹è¯•
"""

import pytest
import asyncio
import pandas as pd
from unittest.mock import Mock, patch, AsyncMock

from src.core.engine import UQMEngine
from src.core.parser import UQMParser  
from src.core.cache import CacheManager
from src.connectors.base import DataConnectorManager
from src.utils.exceptions import UQMError, ValidationError, ExecutionError


class TestUQMEngineIntegration:
    """UQM å¼•æ“é›†æˆæµ‹è¯•"""
    
    @pytest.fixture
    async def engine(self, test_settings):
        """åˆ›å»ºæµ‹è¯•å¼•æ“å®ä¾‹"""
        engine = UQMEngine(test_settings)
        await engine.initialize()
        yield engine
        await engine.cleanup()
    
    @pytest.fixture
    def sample_simple_config(self):
        """ç®€å•çš„æµ‹è¯•é…ç½®"""
        return {
            "name": "integration_test",
            "version": "1.0.0",
            "datasources": {
                "test_db": {
                    "type": "sqlite",
                    "connection": {
                        "database": ":memory:"
                    }
                }
            },
            "steps": [
                {
                    "name": "simple_query",
                    "type": "query",
                    "datasource": "test_db",
                    "config": {
                        "sql": "SELECT 1 as id, 'test' as name"
                    }
                }
            ]
        }
    
    @pytest.fixture
    def sample_complex_config(self):
        """å¤æ‚çš„æµ‹è¯•é…ç½®"""
        return {
            "name": "complex_integration_test",
            "version": "1.0.0",
            "datasources": {
                "source_db": {
                    "type": "sqlite",
                    "connection": {
                        "database": ":memory:"
                    }
                }
            },
            "steps": [
                {
                    "name": "extract_data",
                    "type": "query",
                    "datasource": "source_db",
                    "config": {
                        "sql": """
                        SELECT 
                            1 as id, 'Alice' as name, 25 as age, 'IT' as department, 50000 as salary
                        UNION ALL
                        SELECT 
                            2 as id, 'Bob' as name, 30 as age, 'HR' as department, 45000 as salary
                        UNION ALL
                        SELECT 
                            3 as id, 'Charlie' as name, 35 as age, 'IT' as department, 60000 as salary
                        """
                    }
                },
                {
                    "name": "enrich_data",
                    "type": "enrich",
                    "depends_on": ["extract_data"],
                    "config": {
                        "enrichments": [
                            {
                                "column": "salary_grade",
                                "expression": "'High' if salary > 50000 else 'Low'"
                            },
                            {
                                "column": "full_info",
                                "expression": "name + ' (' + str(age) + ')'"
                            }
                        ]
                    }
                },
                {
                    "name": "filter_data",
                    "type": "filter",
                    "depends_on": ["enrich_data"],
                    "config": {
                        "condition": "department == 'IT'"
                    }
                },
                {
                    "name": "assert_data",
                    "type": "assert",
                    "depends_on": ["filter_data"],
                    "config": {
                        "assertions": [
                            {
                                "name": "check_record_count",
                                "condition": "len(df) >= 1",
                                "message": "åº”è¯¥è‡³å°‘æœ‰ä¸€æ¡ IT éƒ¨é—¨çš„è®°å½•"
                            },
                            {
                                "name": "check_department",
                                "condition": "all(df['department'] == 'IT')",
                                "message": "æ‰€æœ‰è®°å½•éƒ½åº”è¯¥æ˜¯ IT éƒ¨é—¨"
                            }
                        ]
                    }
                }
            ]
        }
    
    @pytest.mark.async_test
    async def test_simple_execution(self, engine, sample_simple_config):
        """æµ‹è¯•ç®€å•é…ç½®çš„æ‰§è¡Œ"""
        result = await engine.execute(sample_simple_config)
        
        assert result is not None
        assert result['status'] == 'success'
        assert 'execution_id' in result
        assert 'results' in result
        assert 'simple_query' in result['results']
        
        # éªŒè¯æŸ¥è¯¢ç»“æœ
        query_result = result['results']['simple_query']
        assert len(query_result) == 1
        assert query_result[0]['id'] == 1
        assert query_result[0]['name'] == 'test'
    
    @pytest.mark.async_test
    async def test_complex_workflow_execution(self, engine, sample_complex_config):
        """æµ‹è¯•å¤æ‚å·¥ä½œæµçš„æ‰§è¡Œ"""
        result = await engine.execute(sample_complex_config)
        
        assert result is not None
        assert result['status'] == 'success'
        assert 'execution_id' in result
        assert 'results' in result
        
        # éªŒè¯æ‰€æœ‰æ­¥éª¤éƒ½è¢«æ‰§è¡Œ
        expected_steps = ['extract_data', 'enrich_data', 'filter_data', 'assert_data']
        for step in expected_steps:
            assert step in result['results']
        
        # éªŒè¯æœ€ç»ˆç­›é€‰ç»“æœ
        final_result = result['results']['filter_data']
        assert len(final_result) == 2  # åº”è¯¥æœ‰ä¸¤æ¡ IT éƒ¨é—¨çš„è®°å½•
        assert all(record['department'] == 'IT' for record in final_result)
        
        # éªŒè¯ä¸°å¯ŒåŒ–å­—æ®µ
        for record in final_result:
            assert 'salary_grade' in record
            assert 'full_info' in record
            assert record['full_info'].endswith(')')
    
    @pytest.mark.async_test
    async def test_execution_with_dependencies(self, engine):
        """æµ‹è¯•å¸¦ä¾èµ–å…³ç³»çš„æ‰§è¡Œ"""
        config = {
            "name": "dependency_test",
            "version": "1.0.0",
            "datasources": {
                "test_db": {
                    "type": "sqlite",
                    "connection": {"database": ":memory:"}
                }
            },
            "steps": [
                {
                    "name": "step_a",
                    "type": "query",
                    "datasource": "test_db",
                    "config": {"sql": "SELECT 'A' as step"}
                },
                {
                    "name": "step_b",
                    "type": "query", 
                    "datasource": "test_db",
                    "depends_on": ["step_a"],
                    "config": {"sql": "SELECT 'B' as step"}
                },
                {
                    "name": "step_c",
                    "type": "query",
                    "datasource": "test_db", 
                    "depends_on": ["step_a", "step_b"],
                    "config": {"sql": "SELECT 'C' as step"}
                }
            ]
        }
        
        result = await engine.execute(config)
        
        assert result['status'] == 'success'
        
        # éªŒè¯æ‰§è¡Œé¡ºåºè®°å½•
        execution_order = result.get('execution_order', [])
        assert len(execution_order) == 3
        
        # step_a åº”è¯¥æœ€å…ˆæ‰§è¡Œ
        assert execution_order[0] == 'step_a'
        
        # step_b åº”è¯¥åœ¨ step_a ä¹‹åæ‰§è¡Œ
        step_a_index = execution_order.index('step_a')
        step_b_index = execution_order.index('step_b')
        assert step_b_index > step_a_index
        
        # step_c åº”è¯¥æœ€åæ‰§è¡Œ
        step_c_index = execution_order.index('step_c')
        assert step_c_index > step_a_index
        assert step_c_index > step_b_index
    
    @pytest.mark.async_test
    async def test_execution_with_invalid_config(self, engine):
        """æµ‹è¯•æ— æ•ˆé…ç½®çš„æ‰§è¡Œ"""
        invalid_config = {
            "name": "invalid_test",
            # ç¼ºå°‘ version å’Œ steps
        }
        
        with pytest.raises(ValidationError):
            await engine.execute(invalid_config)
    
    @pytest.mark.async_test
    async def test_execution_with_step_failure(self, engine):
        """æµ‹è¯•æ­¥éª¤å¤±è´¥çš„å¤„ç†"""
        config = {
            "name": "failure_test",
            "version": "1.0.0",
            "datasources": {
                "test_db": {
                    "type": "sqlite",
                    "connection": {"database": ":memory:"}
                }
            },
            "steps": [
                {
                    "name": "good_step",
                    "type": "query",
                    "datasource": "test_db",
                    "config": {"sql": "SELECT 1 as value"}
                },
                {
                    "name": "bad_step",
                    "type": "query",
                    "datasource": "test_db",
                    "depends_on": ["good_step"],
                    "config": {"sql": "SELECT * FROM nonexistent_table"}
                }
            ]
        }
        
        with pytest.raises(ExecutionError):
            await engine.execute(config)
    
    @pytest.mark.async_test 
    async def test_execution_with_caching(self, engine):
        """æµ‹è¯•å¸¦ç¼“å­˜çš„æ‰§è¡Œ"""
        config = {
            "name": "cache_test",
            "version": "1.0.0",
            "datasources": {
                "test_db": {
                    "type": "sqlite",
                    "connection": {"database": ":memory:"}
                }
            },
            "steps": [
                {
                    "name": "cached_step",
                    "type": "query",
                    "datasource": "test_db",
                    "config": {"sql": "SELECT datetime('now') as timestamp"},
                    "cache": {
                        "enabled": True,
                        "ttl": 3600
                    }
                }
            ]
        }
        
        # ç¬¬ä¸€æ¬¡æ‰§è¡Œ
        result1 = await engine.execute(config)
        first_timestamp = result1['results']['cached_step'][0]['timestamp']
        
        # ç¬¬äºŒæ¬¡æ‰§è¡Œï¼ˆåº”è¯¥ä½¿ç”¨ç¼“å­˜ï¼‰
        result2 = await engine.execute(config)
        second_timestamp = result2['results']['cached_step'][0]['timestamp']
        
        # ç”±äºä½¿ç”¨äº†ç¼“å­˜ï¼Œæ—¶é—´æˆ³åº”è¯¥ç›¸åŒ
        assert first_timestamp == second_timestamp
    
    @pytest.mark.async_test
    async def test_parallel_execution(self, engine):
        """æµ‹è¯•å¹¶è¡Œæ‰§è¡Œ"""
        config = {
            "name": "parallel_test",
            "version": "1.0.0",
            "datasources": {
                "test_db": {
                    "type": "sqlite",
                    "connection": {"database": ":memory:"}
                }
            },
            "steps": [
                {
                    "name": "parallel_step_1",
                    "type": "query",
                    "datasource": "test_db",
                    "config": {"sql": "SELECT 1 as value"}
                },
                {
                    "name": "parallel_step_2", 
                    "type": "query",
                    "datasource": "test_db",
                    "config": {"sql": "SELECT 2 as value"}
                },
                {
                    "name": "parallel_step_3",
                    "type": "query",
                    "datasource": "test_db", 
                    "config": {"sql": "SELECT 3 as value"}
                },
                {
                    "name": "dependent_step",
                    "type": "union",
                    "depends_on": ["parallel_step_1", "parallel_step_2", "parallel_step_3"],
                    "config": {
                        "datasets": ["parallel_step_1", "parallel_step_2", "parallel_step_3"]
                    }
                }
            ],
            "options": {
                "parallel": True,
                "max_workers": 3
            }
        }
        
        result = await engine.execute(config)
        
        assert result['status'] == 'success'
        assert 'dependent_step' in result['results']
        
        # éªŒè¯åˆå¹¶ç»“æœ
        union_result = result['results']['dependent_step']
        assert len(union_result) == 3
        values = [record['value'] for record in union_result]
        assert set(values) == {1, 2, 3}
    
    @pytest.mark.async_test
    async def test_memory_management(self, engine):
        """æµ‹è¯•å†…å­˜ç®¡ç†"""
        # åˆ›å»ºä¸€ä¸ªä¼šäº§ç”Ÿè¾ƒå¤§æ•°æ®é›†çš„é…ç½®
        config = {
            "name": "memory_test",
            "version": "1.0.0",
            "datasources": {
                "test_db": {
                    "type": "sqlite",
                    "connection": {"database": ":memory:"}
                }
            },
            "steps": [
                {
                    "name": "large_dataset",
                    "type": "query",
                    "datasource": "test_db",
                    "config": {
                        "sql": """
                        WITH RECURSIVE numbers(x) AS (
                            SELECT 1
                            UNION ALL
                            SELECT x+1 FROM numbers WHERE x < 1000
                        )
                        SELECT x as id, 'data_' || x as value FROM numbers
                        """
                    }
                }
            ]
        }
        
        result = await engine.execute(config)
        
        assert result['status'] == 'success'
        assert len(result['results']['large_dataset']) == 1000
    
    @pytest.mark.async_test
    async def test_timeout_handling(self, engine):
        """æµ‹è¯•è¶…æ—¶å¤„ç†"""
        config = {
            "name": "timeout_test",
            "version": "1.0.0",
            "datasources": {
                "test_db": {
                    "type": "sqlite",
                    "connection": {"database": ":memory:"}
                }
            },
            "steps": [
                {
                    "name": "timeout_step",
                    "type": "query",
                    "datasource": "test_db",
                    "config": {"sql": "SELECT 1 as value"},
                    "timeout": 1  # 1ç§’è¶…æ—¶
                }
            ],
            "options": {
                "timeout": 5  # æ€»è¶…æ—¶5ç§’
            }
        }
        
        # æ­£å¸¸æƒ…å†µä¸‹åº”è¯¥æˆåŠŸæ‰§è¡Œ
        result = await engine.execute(config)
        assert result['status'] == 'success'


class TestUQMParserIntegration:
    """UQM è§£æå™¨é›†æˆæµ‹è¯•"""
    
    def test_parse_and_validate_complete_config(self, sample_uqm_config):
        """æµ‹è¯•è§£æå’ŒéªŒè¯å®Œæ•´é…ç½®"""
        parser = UQMParser()
        
        parsed_config = parser.parse(sample_uqm_config)
        
        assert parsed_config is not None
        assert parsed_config.name == sample_uqm_config['name']
        assert parsed_config.version == sample_uqm_config['version']
        assert len(parsed_config.steps) == len(sample_uqm_config['steps'])
        assert len(parsed_config.datasources) == len(sample_uqm_config['datasources'])
    
    def test_parse_with_validation_errors(self):
        """æµ‹è¯•è§£ææ—¶çš„éªŒè¯é”™è¯¯"""
        parser = UQMParser()
        
        invalid_config = {
            "name": "",  # æ— æ•ˆåç§°
            "version": "invalid",  # æ— æ•ˆç‰ˆæœ¬
            "steps": []  # ç©ºæ­¥éª¤
        }
        
        with pytest.raises(ValidationError):
            parser.parse(invalid_config)
    
    def test_parse_complex_expressions(self):
        """æµ‹è¯•å¤æ‚è¡¨è¾¾å¼çš„è§£æ"""
        parser = UQMParser()
        
        config = {
            "name": "expression_test",
            "version": "1.0.0",
            "steps": [
                {
                    "name": "enrich_step",
                    "type": "enrich",
                    "config": {
                        "enrichments": [
                            {
                                "column": "complex_calc",
                                "expression": "round((salary * 0.1) + (age * 100), 2)"
                            },
                            {
                                "column": "conditional",
                                "expression": "'Senior' if age > 30 else 'Junior'"
                            }
                        ]
                    }
                }
            ]
        }
        
        parsed_config = parser.parse(config)
        
        assert parsed_config is not None
        enrichments = parsed_config.steps[0].config['enrichments']
        assert len(enrichments) == 2
        
        # éªŒè¯è¡¨è¾¾å¼è¢«æ­£ç¡®è§£æ
        assert enrichments[0]['expression'] == "round((salary * 0.1) + (age * 100), 2)"
        assert enrichments[1]['expression'] == "'Senior' if age > 30 else 'Junior'"


class TestCacheIntegration:
    """ç¼“å­˜é›†æˆæµ‹è¯•"""
    
    @pytest.fixture
    async def cache_manager(self, test_settings):
        """åˆ›å»ºç¼“å­˜ç®¡ç†å™¨"""
        cache_manager = CacheManager(test_settings)
        await cache_manager.initialize()
        yield cache_manager
        await cache_manager.cleanup()
    
    @pytest.mark.async_test
    async def test_cache_step_results(self, cache_manager):
        """æµ‹è¯•ç¼“å­˜æ­¥éª¤ç»“æœ"""
        step_name = "test_step"
        test_data = [{"id": 1, "name": "test"}]
        cache_key = f"step:{step_name}"
        
        # ç¼“å­˜æ•°æ®
        await cache_manager.set(cache_key, test_data, ttl=3600)
        
        # è·å–ç¼“å­˜æ•°æ®
        cached_data = await cache_manager.get(cache_key)
        
        assert cached_data == test_data
    
    @pytest.mark.async_test
    async def test_cache_invalidation(self, cache_manager):
        """æµ‹è¯•ç¼“å­˜å¤±æ•ˆ"""
        cache_key = "test_key"
        test_data = {"value": "test"}
        
        # è®¾ç½®çŸ­TTLçš„ç¼“å­˜
        await cache_manager.set(cache_key, test_data, ttl=1)
        
        # ç«‹å³è·å–åº”è¯¥æœ‰æ•°æ®
        cached_data = await cache_manager.get(cache_key)
        assert cached_data == test_data
        
        # ç­‰å¾…ç¼“å­˜è¿‡æœŸ
        await asyncio.sleep(1.1)
        
        # ç°åœ¨åº”è¯¥æ²¡æœ‰æ•°æ®
        expired_data = await cache_manager.get(cache_key)
        assert expired_data is None


class TestConnectorIntegration:
    """è¿æ¥å™¨é›†æˆæµ‹è¯•"""
    
    @pytest.fixture
    async def connector_manager(self, test_settings):
        """åˆ›å»ºè¿æ¥å™¨ç®¡ç†å™¨"""
        manager = DataConnectorManager(test_settings)
        await manager.initialize()
        yield manager
        await manager.cleanup()
    
    @pytest.mark.async_test
    async def test_sqlite_connector_integration(self, connector_manager):
        """æµ‹è¯• SQLite è¿æ¥å™¨é›†æˆ"""
        # æ³¨å†Œ SQLite æ•°æ®æº
        datasource_config = {
            "type": "sqlite",
            "connection": {
                "database": ":memory:"
            }
        }
        
        connector = await connector_manager.get_connector("test_sqlite", datasource_config)
        
        # åˆ›å»ºæµ‹è¯•è¡¨å¹¶æ’å…¥æ•°æ®
        await connector.execute_query("""
            CREATE TABLE users (
                id INTEGER PRIMARY KEY,
                name TEXT NOT NULL,
                age INTEGER
            )
        """)
        
        await connector.execute_query("""
            INSERT INTO users (name, age) VALUES 
            ('Alice', 25),
            ('Bob', 30),
            ('Charlie', 35)
        """)
        
        # æŸ¥è¯¢æ•°æ®
        result = await connector.execute_query("SELECT * FROM users ORDER BY id")
        
        assert len(result) == 3
        assert result[0]['name'] == 'Alice'
        assert result[1]['name'] == 'Bob' 
        assert result[2]['name'] == 'Charlie'
    
    @pytest.mark.async_test
    async def test_connector_pool_management(self, connector_manager):
        """æµ‹è¯•è¿æ¥å™¨æ± ç®¡ç†"""
        datasource_config = {
            "type": "sqlite",
            "connection": {
                "database": ":memory:"
            }
        }
        
        # è·å–å¤šä¸ªè¿æ¥å™¨å®ä¾‹
        connector1 = await connector_manager.get_connector("test_db", datasource_config)
        connector2 = await connector_manager.get_connector("test_db", datasource_config)
        
        # åº”è¯¥è¿”å›ç›¸åŒçš„è¿æ¥å™¨å®ä¾‹ï¼ˆæ± åŒ–ï¼‰
        assert connector1 is connector2
        
        # éªŒè¯è¿æ¥å™¨çŠ¶æ€
        info1 = connector1.get_connection_info()
        info2 = connector2.get_connection_info()
        
        assert info1 == info2
        assert info1['type'] == 'sqlite'


class TestEndToEndWorkflows:
    """ç«¯åˆ°ç«¯å·¥ä½œæµæµ‹è¯•"""
    
    @pytest.mark.async_test
    async def test_data_pipeline_workflow(self, test_settings):
        """æµ‹è¯•å®Œæ•´æ•°æ®ç®¡é“å·¥ä½œæµ"""
        engine = UQMEngine(test_settings)
        await engine.initialize()
        
        try:
            # å®šä¹‰æ•°æ®ç®¡é“é…ç½®
            pipeline_config = {
                "name": "data_pipeline",
                "version": "1.0.0",
                "description": "å®Œæ•´çš„æ•°æ®å¤„ç†ç®¡é“",
                "datasources": {
                    "source_db": {
                        "type": "sqlite",
                        "connection": {"database": ":memory:"}
                    }
                },
                "steps": [
                    # æ­¥éª¤1: æ•°æ®æå–
                    {
                        "name": "extract_raw_data",
                        "type": "query",
                        "datasource": "source_db",
                        "config": {
                            "sql": """
                            SELECT 
                                'sales' as type, 'Q1' as quarter, 1000 as amount, 'Product A' as product
                            UNION ALL
                            SELECT 
                                'sales' as type, 'Q2' as quarter, 1200 as amount, 'Product A' as product
                            UNION ALL
                            SELECT 
                                'sales' as type, 'Q1' as quarter, 800 as amount, 'Product B' as product
                            UNION ALL
                            SELECT 
                                'sales' as type, 'Q2' as quarter, 900 as amount, 'Product B' as product
                            """
                        }
                    },
                    
                    # æ­¥éª¤2: æ•°æ®æ¸…æ´—å’Œä¸°å¯ŒåŒ–
                    {
                        "name": "clean_and_enrich",
                        "type": "enrich",
                        "depends_on": ["extract_raw_data"],
                        "config": {
                            "enrichments": [
                                {
                                    "column": "amount_category",
                                    "expression": "'High' if amount > 1000 else 'Low'"
                                },
                                {
                                    "column": "product_code",
                                    "expression": "'PA' if product == 'Product A' else 'PB'"
                                }
                            ]
                        }
                    },
                    
                    # æ­¥éª¤3: æ•°æ®é€è§†
                    {
                        "name": "pivot_by_quarter",
                        "type": "pivot",
                        "depends_on": ["clean_and_enrich"],
                        "config": {
                            "index_columns": ["product"],
                            "pivot_column": "quarter",
                            "value_columns": ["amount"],
                            "aggregation": "sum"
                        }
                    },
                    
                    # æ­¥éª¤4: æ•°æ®éªŒè¯
                    {
                        "name": "validate_results",
                        "type": "assert",
                        "depends_on": ["pivot_by_quarter"],
                        "config": {
                            "assertions": [
                                {
                                    "name": "check_product_count",
                                    "condition": "len(df) == 2",
                                    "message": "åº”è¯¥æœ‰ä¸¤ä¸ªäº§å“"
                                },
                                {
                                    "name": "check_columns",
                                    "condition": "'Q1' in df.columns and 'Q2' in df.columns",
                                    "message": "åº”è¯¥åŒ…å«Q1å’ŒQ2åˆ—"
                                }
                            ]
                        }
                    }
                ],
                "output": {
                    "format": "json"
                },
                "options": {
                    "parallel": False,
                    "validate_data": True,
                    "cache_enabled": True
                }
            }
            
            # æ‰§è¡Œç®¡é“
            result = await engine.execute(pipeline_config)
            
            # éªŒè¯æ‰§è¡Œç»“æœ
            assert result['status'] == 'success'
            assert 'execution_id' in result
            assert len(result['results']) == 4
            
            # éªŒè¯æœ€ç»ˆé€è§†ç»“æœ
            pivot_result = result['results']['pivot_by_quarter']
            assert len(pivot_result) == 2  # ä¸¤ä¸ªäº§å“
            
            # æ£€æŸ¥åˆ—ç»“æ„
            if len(pivot_result) > 0:
                columns = set(pivot_result[0].keys())
                assert 'product' in columns
                assert any('Q1' in col or 'amount_Q1' in col for col in columns)
                assert any('Q2' in col or 'amount_Q2' in col for col in columns)
            
        finally:
            await engine.cleanup()
    
    @pytest.mark.async_test
    async def test_error_recovery_workflow(self, test_settings):
        """æµ‹è¯•é”™è¯¯æ¢å¤å·¥ä½œæµ"""
        engine = UQMEngine(test_settings)
        await engine.initialize()
        
        try:
            # åŒ…å«æ•…æ„é”™è¯¯çš„é…ç½®
            config_with_error = {
                "name": "error_recovery_test",
                "version": "1.0.0",
                "datasources": {
                    "test_db": {
                        "type": "sqlite",
                        "connection": {"database": ":memory:"}
                    }
                },
                "steps": [
                    {
                        "name": "good_step",
                        "type": "query",
                        "datasource": "test_db",
                        "config": {"sql": "SELECT 1 as value"}
                    },
                    {
                        "name": "error_step",
                        "type": "query",
                        "datasource": "test_db",
                        "config": {"sql": "SELECT * FROM non_existent_table"},
                        "retry": {
                            "enabled": True,
                            "max_attempts": 2,
                            "delay": 0.1
                        }
                    }
                ],
                "options": {
                    "fail_fast": False
                }
            }
            
            # æ‰§è¡Œåº”è¯¥å¤±è´¥ä½†è®°å½•é”™è¯¯ä¿¡æ¯
            with pytest.raises(ExecutionError) as exc_info:
                await engine.execute(config_with_error)
            
            # éªŒè¯é”™è¯¯ä¿¡æ¯åŒ…å«é‡è¯•ä¿¡æ¯
            error_details = str(exc_info.value)
            assert "error_step" in error_details
            
        finally:
            await engine.cleanup()

================
File: uqm-backend/tests/unit/test_expression_parser.py
================
"""
è¡¨è¾¾å¼è§£æå™¨å•å…ƒæµ‹è¯•
"""

import pytest
import pandas as pd
from datetime import datetime, date

from src.utils.expression_parser import (
    SafeExpressionEvaluator,
    ExpressionParser,
    DataFrameExpressionParser,
    SQLExpressionParser,
    expression_parser,
    dataframe_expression_parser,
    sql_expression_parser
)
from src.utils.exceptions import ExpressionError


class TestSafeExpressionEvaluator:
    """å®‰å…¨è¡¨è¾¾å¼æ±‚å€¼å™¨æµ‹è¯•"""
    
    def test_basic_arithmetic(self):
        """æµ‹è¯•åŸºæœ¬ç®—æœ¯è¿ç®—"""
        evaluator = SafeExpressionEvaluator()
        
        assert evaluator.evaluate("1 + 2") == 3
        assert evaluator.evaluate("10 - 5") == 5
        assert evaluator.evaluate("3 * 4") == 12
        assert evaluator.evaluate("15 / 3") == 5
        assert evaluator.evaluate("2 ** 3") == 8
        assert evaluator.evaluate("10 % 3") == 1
    
    def test_comparison_operators(self):
        """æµ‹è¯•æ¯”è¾ƒè¿ç®—ç¬¦"""
        evaluator = SafeExpressionEvaluator()
        
        assert evaluator.evaluate("5 > 3") is True
        assert evaluator.evaluate("3 < 5") is True
        assert evaluator.evaluate("5 >= 5") is True
        assert evaluator.evaluate("3 <= 5") is True
        assert evaluator.evaluate("5 == 5") is True
        assert evaluator.evaluate("5 != 3") is True
    
    def test_logical_operators(self):
        """æµ‹è¯•é€»è¾‘è¿ç®—ç¬¦"""
        evaluator = SafeExpressionEvaluator()
        
        assert evaluator.evaluate("True and True") is True
        assert evaluator.evaluate("True and False") is False
        assert evaluator.evaluate("True or False") is True
        assert evaluator.evaluate("False or False") is False
        assert evaluator.evaluate("not True") is False
        assert evaluator.evaluate("not False") is True
    
    def test_context_variables(self):
        """æµ‹è¯•ä¸Šä¸‹æ–‡å˜é‡"""
        context = {'x': 10, 'y': 20, 'name': 'test'}
        evaluator = SafeExpressionEvaluator(context)
        
        assert evaluator.evaluate("x + y") == 30
        assert evaluator.evaluate("x * 2") == 20
        assert evaluator.evaluate("name") == 'test'
    
    def test_builtin_functions(self):
        """æµ‹è¯•å†…ç½®å‡½æ•°"""
        evaluator = SafeExpressionEvaluator()
        
        assert evaluator.evaluate("abs(-5)") == 5
        assert evaluator.evaluate("max(1, 5, 3)") == 5
        assert evaluator.evaluate("min(1, 5, 3)") == 1
        assert evaluator.evaluate("round(3.14159, 2)") == 3.14
        assert evaluator.evaluate("len([1, 2, 3])") == 3
    
    def test_math_functions(self):
        """æµ‹è¯•æ•°å­¦å‡½æ•°"""
        evaluator = SafeExpressionEvaluator()
        
        assert evaluator.evaluate("sqrt(16)") == 4
        assert evaluator.evaluate("ceil(3.2)") == 4
        assert evaluator.evaluate("floor(3.8)") == 3
        assert abs(evaluator.evaluate("sin(0)") - 0) < 1e-10
    
    def test_list_operations(self):
        """æµ‹è¯•åˆ—è¡¨æ“ä½œ"""
        evaluator = SafeExpressionEvaluator()
        
        assert evaluator.evaluate("[1, 2, 3]") == [1, 2, 3]
        assert evaluator.evaluate("[1, 2, 3][0]") == 1
        assert evaluator.evaluate("[1, 2, 3][1:3]") == [2, 3]
    
    def test_string_operations(self):
        """æµ‹è¯•å­—ç¬¦ä¸²æ“ä½œ"""
        evaluator = SafeExpressionEvaluator()
        
        assert evaluator.evaluate("'hello' + ' world'") == 'hello world'
        assert evaluator.evaluate("'test'[0]") == 't'
        assert evaluator.evaluate("'hello'[1:4]") == 'ell'
    
    def test_forbidden_operations(self):
        """æµ‹è¯•ç¦æ­¢çš„æ“ä½œ"""
        evaluator = SafeExpressionEvaluator()
        
        # æµ‹è¯•ç¦æ­¢çš„å‡½æ•°
        with pytest.raises(ExpressionError):
            evaluator.evaluate("eval('1+1')")
        
        with pytest.raises(ExpressionError):
            evaluator.evaluate("exec('print(1)')")
        
        with pytest.raises(ExpressionError):
            evaluator.evaluate("open('file.txt')")
    
    def test_syntax_errors(self):
        """æµ‹è¯•è¯­æ³•é”™è¯¯"""
        evaluator = SafeExpressionEvaluator()
        
        with pytest.raises(ExpressionError):
            evaluator.evaluate("1 +")
        
        with pytest.raises(ExpressionError):
            evaluator.evaluate("((1 + 2)")
    
    def test_division_by_zero(self):
        """æµ‹è¯•é™¤é›¶é”™è¯¯"""
        evaluator = SafeExpressionEvaluator()
        
        with pytest.raises(ExpressionError):
            evaluator.evaluate("1 / 0")


class TestExpressionParser:
    """è¡¨è¾¾å¼è§£æå™¨æµ‹è¯•"""
    
    def test_register_function(self):
        """æµ‹è¯•æ³¨å†Œè‡ªå®šä¹‰å‡½æ•°"""
        parser = ExpressionParser()
        
        def custom_add(a, b):
            return a + b + 1
        
        parser.register_function("custom_add", custom_add)
        result = parser.parse_and_evaluate("custom_add(2, 3)")
        assert result == 6
    
    def test_set_variables(self):
        """æµ‹è¯•è®¾ç½®å˜é‡"""
        parser = ExpressionParser()
        
        parser.set_variable("x", 10)
        parser.set_variables({"y": 20, "z": 30})
        
        assert parser.parse_and_evaluate("x + y + z") == 60
    
    def test_string_functions(self):
        """æµ‹è¯•å­—ç¬¦ä¸²å‡½æ•°"""
        parser = ExpressionParser()
        
        assert parser.parse_and_evaluate("upper('hello')") == "HELLO"
        assert parser.parse_and_evaluate("lower('WORLD')") == "world"
        assert parser.parse_and_evaluate("strip('  test  ')") == "test"
        assert parser.parse_and_evaluate("length('hello')") == 5
    
    def test_conditional_functions(self):
        """æµ‹è¯•æ¡ä»¶å‡½æ•°"""
        parser = ExpressionParser()
        
        assert parser.parse_and_evaluate("if_null(None, 'default')") == "default"
        assert parser.parse_and_evaluate("if_null('value', 'default')") == "value"
        assert parser.parse_and_evaluate("coalesce(None, None, 'first')") == "first"
    
    def test_array_functions(self):
        """æµ‹è¯•æ•°ç»„å‡½æ•°"""
        parser = ExpressionParser()
        
        assert parser.parse_and_evaluate("first([1, 2, 3])") == 1
        assert parser.parse_and_evaluate("last([1, 2, 3])") == 3
        assert parser.parse_and_evaluate("join([1, 2, 3], '-')") == "1-2-3"
    
    def test_validate_expression(self):
        """æµ‹è¯•è¡¨è¾¾å¼éªŒè¯"""
        parser = ExpressionParser()
        
        # æœ‰æ•ˆè¡¨è¾¾å¼
        valid, msg = parser.validate_expression("1 + 2")
        assert valid is True
        assert msg == ""
        
        # æ— æ•ˆè¯­æ³•
        valid, msg = parser.validate_expression("1 +")
        assert valid is False
        assert "è¯­æ³•é”™è¯¯" in msg
        
        # å±é™©æ¨¡å¼
        valid, msg = parser.validate_expression("__import__('os')")
        assert valid is False
        assert "å±é™©æ¨¡å¼" in msg


class TestDataFrameExpressionParser:
    """DataFrame è¡¨è¾¾å¼è§£æå™¨æµ‹è¯•"""
    
    def test_apply_to_dataframe(self, sample_dataframe):
        """æµ‹è¯•åº”ç”¨åˆ° DataFrame"""
        parser = DataFrameExpressionParser()
        
        # è®¡ç®—åˆ—çš„æ€»å’Œ
        result = parser.apply_to_dataframe(sample_dataframe, "sum_col(df, 'age')")
        assert result == sample_dataframe['age'].sum()
        
        # è·å–åˆ—çš„æœ€å¤§å€¼
        result = parser.apply_to_dataframe(sample_dataframe, "max_col(df, 'salary')")
        assert result == sample_dataframe['salary'].max()
    
    def test_create_computed_column(self, sample_dataframe):
        """æµ‹è¯•åˆ›å»ºè®¡ç®—åˆ—"""
        parser = DataFrameExpressionParser()
        
        # åˆ›å»ºå¹´é¾„ * 1000 çš„åˆ—
        result_df = parser.create_computed_column(
            sample_dataframe, 
            "age_times_1000", 
            "age * 1000"
        )
        
        assert "age_times_1000" in result_df.columns
        expected_values = sample_dataframe['age'] * 1000
        pd.testing.assert_series_equal(
            result_df['age_times_1000'], 
            expected_values, 
            check_names=False
        )
    
    def test_filter_dataframe(self, sample_dataframe):
        """æµ‹è¯•ç­›é€‰ DataFrame"""
        parser = DataFrameExpressionParser()
        
        # ç­›é€‰å¹´é¾„å¤§äº 30 çš„è®°å½•
        result_df = parser.filter_dataframe(sample_dataframe, "age > 30")
        
        expected_df = sample_dataframe[sample_dataframe['age'] > 30]
        pd.testing.assert_frame_equal(result_df.reset_index(drop=True), 
                                    expected_df.reset_index(drop=True))
    
    def test_complex_expression(self, sample_dataframe):
        """æµ‹è¯•å¤æ‚è¡¨è¾¾å¼"""
        parser = DataFrameExpressionParser()
        
        # åˆ›å»ºåŒ…å«æ¡ä»¶é€»è¾‘çš„åˆ—
        result_df = parser.create_computed_column(
            sample_dataframe,
            "salary_category",
            "if_null('High' if salary > 60000 else 'Low', 'Unknown')"
        )
        
        assert "salary_category" in result_df.columns
        # éªŒè¯éƒ¨åˆ†ç»“æœ
        high_salary_mask = sample_dataframe['salary'] > 60000
        assert all(result_df.loc[high_salary_mask, 'salary_category'] == 'High')


class TestSQLExpressionParser:
    """SQL è¡¨è¾¾å¼è§£æå™¨æµ‹è¯•"""
    
    def test_convert_expression_to_sql(self):
        """æµ‹è¯•è¡¨è¾¾å¼è½¬æ¢ä¸º SQL"""
        parser = SQLExpressionParser()
        
        # æµ‹è¯•åŸºæœ¬è½¬æ¢
        sql = parser.convert_expression_to_sql("age > 30 and salary != 0")
        assert "AND" in sql
        assert "<>" in sql or "!=" in sql
        
        # æµ‹è¯•å¸ƒå°”å€¼è½¬æ¢
        sql = parser.convert_expression_to_sql("active == True")
        assert "1" in sql or "TRUE" in sql.upper()
    
    def test_validate_sql_expression(self):
        """æµ‹è¯• SQL è¡¨è¾¾å¼éªŒè¯"""
        parser = SQLExpressionParser()
        
        # æœ‰æ•ˆè¡¨è¾¾å¼
        valid, msg = parser.validate_sql_expression("age > 30")
        assert valid is True
        assert msg == ""
        
        # æ‹¬å·ä¸åŒ¹é…
        valid, msg = parser.validate_sql_expression("age > (30")
        assert valid is False
        assert "æ‹¬å·ä¸åŒ¹é…" in msg
        
        # å¼•å·ä¸åŒ¹é…
        valid, msg = parser.validate_sql_expression("name = 'test")
        assert valid is False
        assert "å¼•å·ä¸åŒ¹é…" in msg
    
    def test_empty_expression(self):
        """æµ‹è¯•ç©ºè¡¨è¾¾å¼"""
        parser = SQLExpressionParser()
        
        valid, msg = parser.validate_sql_expression("")
        assert valid is False
        assert "ä¸èƒ½ä¸ºç©º" in msg


class TestGlobalParsers:
    """æµ‹è¯•å…¨å±€è§£æå™¨å®ä¾‹"""
    
    def test_expression_parser_instance(self):
        """æµ‹è¯•å…¨å±€è¡¨è¾¾å¼è§£æå™¨å®ä¾‹"""
        result = expression_parser.parse_and_evaluate("1 + 2")
        assert result == 3
    
    def test_dataframe_expression_parser_instance(self, sample_dataframe):
        """æµ‹è¯•å…¨å±€ DataFrame è¡¨è¾¾å¼è§£æå™¨å®ä¾‹"""
        result = dataframe_expression_parser.apply_to_dataframe(
            sample_dataframe, 
            "len(df)"
        )
        assert result == len(sample_dataframe)
    
    def test_sql_expression_parser_instance(self):
        """æµ‹è¯•å…¨å±€ SQL è¡¨è¾¾å¼è§£æå™¨å®ä¾‹"""
        valid, msg = sql_expression_parser.validate_sql_expression("SELECT * FROM table")
        assert valid is True
        assert msg == ""


class TestExpressionIntegration:
    """è¡¨è¾¾å¼è§£æå™¨é›†æˆæµ‹è¯•"""
    
    def test_complex_dataframe_workflow(self, sample_dataframe):
        """æµ‹è¯•å¤æ‚çš„ DataFrame å·¥ä½œæµ"""
        parser = DataFrameExpressionParser()
        
        # æ­¥éª¤ 1: æ·»åŠ è®¡ç®—åˆ—
        df = parser.create_computed_column(
            sample_dataframe,
            "salary_per_age",
            "salary / age"
        )
        
        # æ­¥éª¤ 2: ç­›é€‰æ•°æ®
        df = parser.filter_dataframe(df, "department == 'IT'")
        
        # æ­¥éª¤ 3: å†æ·»åŠ ä¸€åˆ—
        df = parser.create_computed_column(
            df,
            "is_senior",
            "age > 30"
        )
        
        # éªŒè¯ç»“æœ
        assert "salary_per_age" in df.columns
        assert "is_senior" in df.columns
        assert all(df['department'] == 'IT')
    
    def test_error_handling_in_computed_column(self, sample_dataframe):
        """æµ‹è¯•è®¡ç®—åˆ—ä¸­çš„é”™è¯¯å¤„ç†"""
        parser = DataFrameExpressionParser()
        
        # åŒ…å«å¯èƒ½å‡ºé”™çš„è¡¨è¾¾å¼
        result_df = parser.create_computed_column(
            sample_dataframe,
            "error_prone",
            "salary / (age - age)"  # ä¼šå¯¼è‡´é™¤é›¶é”™è¯¯
        )
        
        # åº”è¯¥åŒ…å« None å€¼ï¼ˆé”™è¯¯å¤„ç†ç»“æœï¼‰
        assert "error_prone" in result_df.columns
        assert result_df['error_prone'].isnull().any()
    
    def test_expression_with_custom_context(self):
        """æµ‹è¯•è‡ªå®šä¹‰ä¸Šä¸‹æ–‡çš„è¡¨è¾¾å¼"""
        parser = ExpressionParser()
        
        # æ³¨å†Œè‡ªå®šä¹‰å‡½æ•°
        def calculate_tax(salary, rate=0.1):
            return salary * rate
        
        parser.register_function("calculate_tax", calculate_tax)
        
        # è®¾ç½®å˜é‡
        context = {"base_salary": 50000}
        
        # æ‰§è¡Œè¡¨è¾¾å¼
        result = parser.parse_and_evaluate("calculate_tax(base_salary, 0.15)", context)
        assert result == 7500

================
File: uqm-backend/tests/unit/test_validators.py
================
"""
æ•°æ®éªŒè¯å™¨å•å…ƒæµ‹è¯•
"""

import pytest
import pandas as pd
from datetime import datetime

from src.utils.validators import (
    DataValidator,
    UQMValidator,
    DataTypeValidator,
    SchemaValidator,
    BusinessValidator,
    validate_sql_injection,
    validate_column_name,
    validate_expression,
    uqm_validator,
    data_type_validator,
    schema_validator,
    business_validator
)
from src.utils.exceptions import ValidationError


class TestDataValidator:
    """æ•°æ®éªŒè¯å™¨åŸºç±»æµ‹è¯•"""
    
    def test_add_error(self):
        """æµ‹è¯•æ·»åŠ é”™è¯¯"""
        validator = DataValidator()
        
        validator.add_error("field1", "é”™è¯¯ä¿¡æ¯1", "value1")
        validator.add_error("field2", "é”™è¯¯ä¿¡æ¯2")
        
        assert validator.has_errors() is True
        assert len(validator.get_errors()) == 2
        
        errors = validator.get_errors()
        assert errors[0]['field'] == "field1"
        assert errors[0]['message'] == "é”™è¯¯ä¿¡æ¯1"
        assert errors[0]['value'] == "value1"
    
    def test_clear_errors(self):
        """æµ‹è¯•æ¸…ç©ºé”™è¯¯"""
        validator = DataValidator()
        
        validator.add_error("field1", "é”™è¯¯ä¿¡æ¯1")
        assert validator.has_errors() is True
        
        validator.clear_errors()
        assert validator.has_errors() is False
        assert len(validator.get_errors()) == 0
    
    def test_raise_if_errors(self):
        """æµ‹è¯•æœ‰é”™è¯¯æ—¶æŠ›å‡ºå¼‚å¸¸"""
        validator = DataValidator()
        
        # æ²¡æœ‰é”™è¯¯æ—¶ä¸åº”æŠ›å‡ºå¼‚å¸¸
        validator.raise_if_errors()
        
        # æœ‰é”™è¯¯æ—¶åº”æŠ›å‡ºå¼‚å¸¸
        validator.add_error("field1", "é”™è¯¯ä¿¡æ¯1")
        with pytest.raises(ValidationError):
            validator.raise_if_errors()


class TestUQMValidator:
    """UQM é…ç½®éªŒè¯å™¨æµ‹è¯•"""
    
    def test_validate_valid_config(self, sample_uqm_config):
        """æµ‹è¯•éªŒè¯æœ‰æ•ˆé…ç½®"""
        validator = UQMValidator()
        
        result = validator.validate_uqm_config(sample_uqm_config)
        assert result is True
        assert not validator.has_errors()
    
    def test_validate_missing_required_fields(self):
        """æµ‹è¯•ç¼ºå°‘å¿…éœ€å­—æ®µ"""
        validator = UQMValidator()
        config = {"name": "test"}  # ç¼ºå°‘ version å’Œ steps
        
        result = validator.validate_uqm_config(config)
        assert result is False
        assert validator.has_errors()
        
        errors = validator.get_errors()
        error_fields = [e['field'] for e in errors]
        assert 'version' in error_fields
        assert 'steps' in error_fields
    
    def test_validate_invalid_version_format(self):
        """æµ‹è¯•æ— æ•ˆçš„ç‰ˆæœ¬æ ¼å¼"""
        validator = UQMValidator()
        config = {
            "name": "test",
            "version": "invalid_version",
            "steps": []
        }
        
        result = validator.validate_uqm_config(config)
        assert result is False
        assert any("ç‰ˆæœ¬æ ¼å¼æ— æ•ˆ" in e['message'] for e in validator.get_errors())
    
    def test_validate_empty_name(self):
        """æµ‹è¯•ç©ºåç§°"""
        validator = UQMValidator()
        config = {
            "name": "",
            "version": "1.0.0",
            "steps": []
        }
        
        result = validator.validate_uqm_config(config)
        assert result is False
        assert any("åç§°æ— æ•ˆ" in e['message'] for e in validator.get_errors())
    
    def test_validate_datasources(self):
        """æµ‹è¯•æ•°æ®æºéªŒè¯"""
        validator = UQMValidator()
        config = {
            "name": "test",
            "version": "1.0.0",
            "datasources": {
                "db1": {
                    "type": "postgres",
                    "connection": {"host": "localhost"}
                },
                "db2": {
                    "type": "invalid_type",
                    "connection": {"host": "localhost"}
                }
            },
            "steps": [{"name": "step1", "type": "query"}]
        }
        
        result = validator.validate_uqm_config(config)
        assert result is False
        assert any("ä¸æ”¯æŒçš„æ•°æ®æºç±»å‹" in e['message'] for e in validator.get_errors())
    
    def test_validate_steps_structure(self):
        """æµ‹è¯•æ­¥éª¤ç»“æ„éªŒè¯"""
        validator = UQMValidator()
        config = {
            "name": "test",
            "version": "1.0.0",
            "steps": [
                {"name": "step1", "type": "query"},
                {"name": "step1", "type": "enrich"},  # é‡å¤åç§°
                {"type": "filter"},  # ç¼ºå°‘åç§°
                {"name": "step3", "type": "invalid_type"}  # æ— æ•ˆç±»å‹
            ]
        }
        
        result = validator.validate_uqm_config(config)
        assert result is False
        
        errors = validator.get_errors()
        error_messages = [e['message'] for e in errors]
        assert any("æ­¥éª¤åç§°é‡å¤" in msg for msg in error_messages)
        assert any("ç¼ºå°‘å¿…éœ€å­—æ®µ" in msg for msg in error_messages)
        assert any("ä¸æ”¯æŒçš„æ­¥éª¤ç±»å‹" in msg for msg in error_messages)
    
    def test_validate_step_dependencies(self):
        """æµ‹è¯•æ­¥éª¤ä¾èµ–éªŒè¯"""
        validator = UQMValidator()
        config = {
            "name": "test",
            "version": "1.0.0",
            "steps": [
                {"name": "step1", "type": "query"},
                {"name": "step2", "type": "enrich", "depends_on": ["nonexistent_step"]}
            ]
        }
        
        result = validator.validate_uqm_config(config)
        assert result is False
        assert any("å¼•ç”¨äº†ä¸å­˜åœ¨çš„æ­¥éª¤" in e['message'] for e in validator.get_errors())
    
    def test_validate_output_config(self):
        """æµ‹è¯•è¾“å‡ºé…ç½®éªŒè¯"""
        validator = UQMValidator()
        config = {
            "name": "test",
            "version": "1.0.0",
            "steps": [{"name": "step1", "type": "query"}],
            "output": {
                "format": "invalid_format"
            }
        }
        
        result = validator.validate_uqm_config(config)
        assert result is False
        assert any("ä¸æ”¯æŒçš„è¾“å‡ºæ ¼å¼" in e['message'] for e in validator.get_errors())


class TestDataTypeValidator:
    """æ•°æ®ç±»å‹éªŒè¯å™¨æµ‹è¯•"""
    
    def test_validate_dataframe_structure(self, sample_dataframe):
        """æµ‹è¯• DataFrame ç»“æ„éªŒè¯"""
        validator = DataTypeValidator()
        schema = {
            "columns": {
                "required": ["id", "name", "age"],
                "types": {
                    "id": "int",
                    "name": "string",
                    "age": "int"
                }
            }
        }
        
        result = validator.validate_dataframe(sample_dataframe, schema)
        assert result is True
        assert not validator.has_errors()
    
    def test_validate_missing_required_columns(self, sample_dataframe):
        """æµ‹è¯•ç¼ºå°‘å¿…éœ€åˆ—"""
        validator = DataTypeValidator()
        schema = {
            "columns": {
                "required": ["id", "name", "missing_column"]
            }
        }
        
        result = validator.validate_dataframe(sample_dataframe, schema)
        assert result is False
        assert any("ç¼ºå°‘å¿…éœ€åˆ—" in e['message'] for e in validator.get_errors())
    
    def test_validate_column_types(self, sample_dataframe):
        """æµ‹è¯•åˆ—ç±»å‹éªŒè¯"""
        validator = DataTypeValidator()
        schema = {
            "columns": {
                "types": {
                    "id": "string",  # å®é™…æ˜¯ intï¼Œç±»å‹ä¸åŒ¹é…
                    "name": "string"
                }
            }
        }
        
        result = validator.validate_dataframe(sample_dataframe, schema)
        assert result is False
        assert any("åˆ—ç±»å‹ä¸åŒ¹é…" in e['message'] for e in validator.get_errors())
    
    def test_validate_constraints(self):
        """æµ‹è¯•æ•°æ®çº¦æŸéªŒè¯"""
        validator = DataTypeValidator()
        
        # åˆ›å»ºåŒ…å«é—®é¢˜çš„ DataFrame
        df = pd.DataFrame({
            'id': [1, 2, 2, 4],  # åŒ…å«é‡å¤å€¼
            'name': ['Alice', None, 'Charlie', 'David'],  # åŒ…å«ç©ºå€¼
            'score': [85, 95, 45, 105]  # åŒ…å«è¶…å‡ºèŒƒå›´çš„å€¼
        })
        
        schema = {
            "constraints": {
                "not_null": ["name"],
                "unique": ["id"],
                "range": {
                    "score": {"min": 0, "max": 100}
                }
            }
        }
        
        result = validator.validate_dataframe(df, schema)
        assert result is False
        
        errors = validator.get_errors()
        error_messages = [e['message'] for e in errors]
        assert any("åŒ…å«" in msg and "ç©ºå€¼" in msg for msg in error_messages)
        assert any("åŒ…å«" in msg and "é‡å¤å€¼" in msg for msg in error_messages)
        assert any("å¤§äºæœ€å¤§å€¼" in msg for msg in error_messages)


class TestSchemaValidator:
    """Schema éªŒè¯å™¨æµ‹è¯•"""
    
    def test_validate_basic_types(self):
        """æµ‹è¯•åŸºæœ¬ç±»å‹éªŒè¯"""
        validator = SchemaValidator()
        
        # å­—ç¬¦ä¸²ç±»å‹
        schema = {"type": "string"}
        assert validator.validate_json_schema("test", schema) is True
        assert validator.validate_json_schema(123, schema) is False
        
        # æ•°å­—ç±»å‹
        schema = {"type": "number"}
        validator.clear_errors()
        assert validator.validate_json_schema(123, schema) is True
        validator.clear_errors()
        assert validator.validate_json_schema("test", schema) is False
    
    def test_validate_object_schema(self):
        """æµ‹è¯•å¯¹è±¡ Schema éªŒè¯"""
        validator = SchemaValidator()
        
        schema = {
            "type": "object",
            "required": ["name", "age"],
            "properties": {
                "name": {"type": "string"},
                "age": {"type": "number"}
            }
        }
        
        # æœ‰æ•ˆå¯¹è±¡
        data = {"name": "Alice", "age": 25}
        assert validator.validate_json_schema(data, schema) is True
        
        # ç¼ºå°‘å¿…éœ€å­—æ®µ
        validator.clear_errors()
        data = {"name": "Alice"}
        assert validator.validate_json_schema(data, schema) is False
        assert any("ç¼ºå°‘å¿…éœ€å­—æ®µ" in e['message'] for e in validator.get_errors())


class TestBusinessValidator:
    """ä¸šåŠ¡é€»è¾‘éªŒè¯å™¨æµ‹è¯•"""
    
    def test_validate_pivot_config(self):
        """æµ‹è¯•é€è§†é…ç½®éªŒè¯"""
        validator = BusinessValidator()
        
        # æœ‰æ•ˆé…ç½®
        config = {
            "index_columns": ["date", "category"],
            "pivot_column": "metric",
            "value_columns": ["value"]
        }
        assert validator.validate_pivot_config(config) is True
        
        # ç¼ºå°‘å¿…éœ€å­—æ®µ
        config = {"index_columns": ["date"]}
        validator.clear_errors()
        assert validator.validate_pivot_config(config) is False
        assert validator.has_errors()
    
    def test_validate_join_config(self):
        """æµ‹è¯•è¿æ¥é…ç½®éªŒè¯"""
        validator = BusinessValidator()
        
        # æœ‰æ•ˆé…ç½®
        config = {
            "left_on": "id",
            "right_on": "user_id",
            "how": "inner"
        }
        assert validator.validate_join_config(config) is True
        
        # æ— æ•ˆè¿æ¥ç±»å‹
        config["how"] = "invalid_join"
        validator.clear_errors()
        assert validator.validate_join_config(config) is False
        assert any("ä¸æ”¯æŒçš„è¿æ¥ç±»å‹" in e['message'] for e in validator.get_errors())
    
    def test_validate_aggregation_config(self):
        """æµ‹è¯•èšåˆé…ç½®éªŒè¯"""
        validator = BusinessValidator()
        
        # æœ‰æ•ˆé…ç½®
        config = {
            "group_by": ["category"],
            "agg_functions": {
                "sales": "sum",
                "quantity": ["count", "avg"]
            }
        }
        assert validator.validate_aggregation_config(config) is True
        
        # æ— æ•ˆèšåˆå‡½æ•°
        config["agg_functions"]["sales"] = "invalid_function"
        validator.clear_errors()
        assert validator.validate_aggregation_config(config) is False
        assert any("ä¸æ”¯æŒçš„èšåˆå‡½æ•°" in e['message'] for e in validator.get_errors())


class TestUtilityFunctions:
    """å·¥å…·å‡½æ•°æµ‹è¯•"""
    
    def test_validate_sql_injection(self):
        """æµ‹è¯• SQL æ³¨å…¥éªŒè¯"""
        # å®‰å…¨çš„ SQL
        safe_sql = "SELECT * FROM users WHERE age > 25"
        is_safe, risks = validate_sql_injection(safe_sql)
        assert is_safe is True
        assert len(risks) == 0
        
        # åŒ…å«å±é™©æ¨¡å¼çš„ SQL
        dangerous_sql = "SELECT * FROM users; DROP TABLE users;"
        is_safe, risks = validate_sql_injection(dangerous_sql)
        assert is_safe is False
        assert len(risks) > 0
        
        # åŒ…å«æ³¨é‡Šçš„ SQL
        comment_sql = "SELECT * FROM users -- WHERE age > 25"
        is_safe, risks = validate_sql_injection(comment_sql)
        assert is_safe is False
        assert len(risks) > 0
    
    def test_validate_column_name(self):
        """æµ‹è¯•åˆ—åéªŒè¯"""
        # æœ‰æ•ˆåˆ—å
        valid, msg = validate_column_name("user_id")
        assert valid is True
        assert msg == ""
        
        valid, msg = validate_column_name("columnA")
        assert valid is True
        
        # æ— æ•ˆåˆ—å
        valid, msg = validate_column_name("123column")
        assert valid is False
        assert "å¿…é¡»ä»¥å­—æ¯å¼€å¤´" in msg
        
        valid, msg = validate_column_name("select")
        assert valid is False
        assert "SQL ä¿ç•™å­—" in msg
        
        valid, msg = validate_column_name("")
        assert valid is False
        assert "ä¸èƒ½ä¸ºç©º" in msg
    
    def test_validate_expression(self):
        """æµ‹è¯•è¡¨è¾¾å¼éªŒè¯"""
        # å®‰å…¨è¡¨è¾¾å¼
        safe_expr = "x + y * 2"
        valid, msg = validate_expression(safe_expr)
        assert valid is True
        assert msg == ""
        
        # åŒ…å«å±é™©å‡½æ•°çš„è¡¨è¾¾å¼
        dangerous_expr = "eval('x + y')"
        valid, msg = validate_expression(dangerous_expr)
        assert valid is False
        assert "å±é™©å‡½æ•°" in msg
        
        # åŒ…å«å±é™©å±æ€§è®¿é—®çš„è¡¨è¾¾å¼
        dangerous_expr = "x.__class__.__bases__"
        valid, msg = validate_expression(dangerous_expr)
        assert valid is False
        assert "å±é™©çš„å±æ€§è®¿é—®" in msg


class TestGlobalValidators:
    """æµ‹è¯•å…¨å±€éªŒè¯å™¨å®ä¾‹"""
    
    def test_uqm_validator_instance(self, sample_uqm_config):
        """æµ‹è¯•å…¨å±€ UQM éªŒè¯å™¨å®ä¾‹"""
        result = uqm_validator.validate_uqm_config(sample_uqm_config)
        assert result is True
    
    def test_data_type_validator_instance(self, sample_dataframe):
        """æµ‹è¯•å…¨å±€æ•°æ®ç±»å‹éªŒè¯å™¨å®ä¾‹"""
        schema = {"columns": {"required": ["id", "name"]}}
        result = data_type_validator.validate_dataframe(sample_dataframe, schema)
        assert result is True
    
    def test_schema_validator_instance(self):
        """æµ‹è¯•å…¨å±€ Schema éªŒè¯å™¨å®ä¾‹"""
        schema = {"type": "string"}
        result = schema_validator.validate_json_schema("test", schema)
        assert result is True
    
    def test_business_validator_instance(self):
        """æµ‹è¯•å…¨å±€ä¸šåŠ¡éªŒè¯å™¨å®ä¾‹"""
        config = {
            "index_columns": ["date"],
            "pivot_column": "metric",
            "value_columns": ["value"]
        }
        result = business_validator.validate_pivot_config(config)
        assert result is True


class TestValidationIntegration:
    """éªŒè¯å™¨é›†æˆæµ‹è¯•"""
    
    def test_comprehensive_uqm_validation(self):
        """æµ‹è¯•ç»¼åˆ UQM éªŒè¯"""
        validator = UQMValidator()
        
        # å¤æ‚çš„ UQM é…ç½®
        config = {
            "name": "complex_uqm",
            "version": "2.1.0",
            "description": "å¤æ‚çš„ UQM é…ç½®ç¤ºä¾‹",
            "datasources": {
                "main_db": {
                    "type": "postgres",
                    "connection": {
                        "host": "localhost",
                        "port": 5432,
                        "database": "testdb"
                    }
                },
                "cache_db": {
                    "type": "redis",
                    "connection": {
                        "host": "localhost",
                        "port": 6379
                    }
                }
            },
            "steps": [
                {
                    "name": "extract_users",
                    "type": "query",
                    "datasource": "main_db",
                    "config": {
                        "sql": "SELECT * FROM users"
                    }
                },
                {
                    "name": "enrich_users",
                    "type": "enrich",
                    "depends_on": ["extract_users"],
                    "config": {
                        "enrichments": [
                            {"column": "full_name", "expression": "first_name + ' ' + last_name"}
                        ]
                    }
                },
                {
                    "name": "pivot_data",
                    "type": "pivot",
                    "depends_on": ["enrich_users"],
                    "config": {
                        "index_columns": ["department"],
                        "pivot_column": "role",
                        "value_columns": ["salary"]
                    }
                }
            ],
            "output": {
                "format": "json",
                "file_path": "output.json"
            }
        }
        
        result = validator.validate_uqm_config(config)
        assert result is True
        assert not validator.has_errors()
    
    def test_validation_error_accumulation(self):
        """æµ‹è¯•éªŒè¯é”™è¯¯ç´¯ç§¯"""
        validator = UQMValidator()
        
        # åŒ…å«å¤šç§é”™è¯¯çš„é…ç½®
        config = {
            "name": "",  # ç©ºåç§°
            "version": "invalid",  # æ— æ•ˆç‰ˆæœ¬
            "datasources": {
                "db1": {
                    "type": "invalid_type",  # æ— æ•ˆç±»å‹
                    "connection": "not_a_dict"  # é”™è¯¯çš„è¿æ¥é…ç½®ç±»å‹
                }
            },
            "steps": [
                {"type": "query"},  # ç¼ºå°‘åç§°
                {"name": "step1", "type": "invalid_step_type"},  # æ— æ•ˆæ­¥éª¤ç±»å‹
                {"name": "step2", "type": "enrich", "depends_on": ["nonexistent"]}  # æ— æ•ˆä¾èµ–
            ]
        }
        
        result = validator.validate_uqm_config(config)
        assert result is False
        assert len(validator.get_errors()) >= 5  # åº”è¯¥æœ‰å¤šä¸ªé”™è¯¯




================================================================
End of Codebase
================================================================
